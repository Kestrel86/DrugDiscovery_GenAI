{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Dl_LFACtFxLV",
        "outputId": "b26c0723-4667-41f8-c3f1-7415439ba8dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit-pypi\n",
            "  Downloading rdkit_pypi-2022.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit-pypi) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit-pypi) (11.2.1)\n",
            "Downloading rdkit_pypi-2022.9.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit-pypi\n",
            "Successfully installed rdkit-pypi-2022.9.5\n",
            "Cloning into 'moses'...\n",
            "remote: Enumerating objects: 1957, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 1957 (delta 0), reused 2 (delta 0), pack-reused 1953 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1957/1957), 164.05 MiB | 17.26 MiB/s, done.\n",
            "Resolving deltas: 100% (1068/1068), done.\n",
            "Filtering content: 100% (68/68), 323.72 MiB | 58.18 MiB/s, done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-1-b129bda6a2e6>\", line 15, in <cell line: 0>\n",
            "    from rdkit import Chem\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/rdkit/__init__.py\", line 23, in <module>\n",
            "    from rdkit.Chem.Draw import IPythonConsole\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/rdkit/Chem/__init__.py\", line 18, in <module>\n",
            "    from rdkit import DataStructs\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/rdkit/DataStructs/__init__.py\", line 13, in <module>\n",
            "    from rdkit.DataStructs import cDataStructs\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "_ARRAY_API not found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-1-b129bda6a2e6>\", line 15, in <cell line: 0>\n",
            "    from rdkit import Chem\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/rdkit/__init__.py\", line 23, in <module>\n",
            "    from rdkit.Chem.Draw import IPythonConsole\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/rdkit/Chem/__init__.py\", line 20, in <module>\n",
            "    from rdkit.Chem import rdchem\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "_ARRAY_API not found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-1-b129bda6a2e6>\", line 15, in <cell line: 0>\n",
            "    from rdkit import Chem\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/rdkit/__init__.py\", line 23, in <module>\n",
            "    from rdkit.Chem.Draw import IPythonConsole\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/rdkit/Chem/__init__.py\", line 24, in <module>\n",
            "    from rdkit.Chem.rdmolops import *\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "_ARRAY_API not found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-1-b129bda6a2e6>\", line 15, in <cell line: 0>\n",
            "    from rdkit import Chem\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/rdkit/__init__.py\", line 23, in <module>\n",
            "    from rdkit.Chem.Draw import IPythonConsole\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/rdkit/Chem/Draw/__init__.py\", line 20, in <module>\n",
            "    from rdkit.Chem import rdDepictor\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "_ARRAY_API not found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-1-b129bda6a2e6>\", line 15, in <cell line: 0>\n",
            "    from rdkit import Chem\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/rdkit/__init__.py\", line 23, in <module>\n",
            "    from rdkit.Chem.Draw import IPythonConsole\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/rdkit/Chem/Draw/__init__.py\", line 21, in <module>\n",
            "    from rdkit.Chem.Draw import rdMolDraw2D\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "_ARRAY_API not found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
          ]
        }
      ],
      "source": [
        "!pip install rdkit-pypi\n",
        "!git clone https://github.com/molecularsets/moses.git\n",
        "\n",
        "import matplotlib as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from rdkit import Chem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6_cUokLgF8gD"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def load_smiles_from_csv(path, split_type='train'):\n",
        "    '''\n",
        "    Loads SMILES strings from a CSV file.\n",
        "\n",
        "    Args:\n",
        "        path (str): Path to the CSV file\n",
        "        split_type (str): Split type ('train' or 'test')\n",
        "\n",
        "    Returns:\n",
        "        list: List of SMILES strings\n",
        "    '''\n",
        "    smiles = []\n",
        "    with open(path, 'r') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            if row['SPLIT'].strip().lower() == split_type:\n",
        "                smiles.append(row['SMILES'].strip())\n",
        "    return smiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xYSD_yd3F_qy"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Functions are from the RNN model we have but not entirely sure where they would fit in the VAE\n",
        "Currently working on implementation, the process_smiles function may help in creating valid molecules\n",
        "'''\n",
        "\n",
        "# Function to add start and end tokens\n",
        "def process_smiles(smiles_list):\n",
        "    return [\"^\" + s + \"$\" for s in smiles_list]\n",
        "\n",
        "# Create character dictionaries including special tokens\n",
        "def create_vocab(smiles_list):\n",
        "    all_chars = sorted(list(set(''.join(smiles_list))))\n",
        "    char2idx = {ch: i + 1 for i, ch in enumerate(all_chars)}\n",
        "    char2idx[''] = 0  # Padding token\n",
        "    idx2char = {i: ch for ch, i in char2idx.items()}\n",
        "    return char2idx, idx2char, len(char2idx)\n",
        "\n",
        "# Enhanced tokenization\n",
        "def tokenize(smiles, char2idx):\n",
        "    return [char2idx.get(c, 0) for c in smiles]  # Default to 0 if unknown\n",
        "\n",
        "def detokenize(tokens, idx2char):\n",
        "    return ''.join([idx2char.get(t, '') for t in tokens if t != 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1wJys-XBGH93"
      },
      "outputs": [],
      "source": [
        "def extract_unique_chars(smiles_list):\n",
        "    '''\n",
        "    Extracts unique characters from a list of SMILES strings.\n",
        "\n",
        "    Args:\n",
        "        smiles_list (list): List of SMILES strings\n",
        "\n",
        "    Returns:\n",
        "        list: List of unique characters\n",
        "    '''\n",
        "    unique_chars = set()\n",
        "    for smiles in smiles_list:\n",
        "        unique_chars.update(smiles.strip())\n",
        "    return sorted(unique_chars)\n",
        "\n",
        "def clean_smiles(smiles):\n",
        "    '''\n",
        "    Cleans a SMILES string by removing unwanted characters.\n",
        "\n",
        "    Args:\n",
        "        smiles (str): SMILES string\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned SMILES string\n",
        "    '''\n",
        "    # Remove unwanted metadata and special characters\n",
        "    cleaned = smiles.split(',')[0].strip()\n",
        "    cleaned = cleaned.replace('#', '')  # Remove '#' characters\n",
        "    cleaned = cleaned.replace('$', '')  # Remove '$' characters\n",
        "    cleaned = cleaned.replace('^', '')  # Remove '^' characters if present\n",
        "    return cleaned\n",
        "\n",
        "# trying out start and end characters\n",
        "def decode_smiles(one_hot_tensor, idx_to_char):\n",
        "    '''\n",
        "    Decodes a one-hot encoded tensor back to a SMILES string, stopping at the end token.\n",
        "    '''\n",
        "    smiles = ''\n",
        "    one_hot_tensor = one_hot_tensor.view(-1, len(idx_to_char))\n",
        "    for row in one_hot_tensor:\n",
        "        idx = row.argmax().item()\n",
        "        char = idx_to_char[idx]\n",
        "        if char == '$' and len(smiles) > 0:  # End of sequence\n",
        "            break\n",
        "        if char != '^':  # Skip start token\n",
        "            smiles += char\n",
        "    return smiles.strip()\n",
        "\n",
        "# def decode_smiles(one_hot_tensor, idx_to_char):\n",
        "#     '''\n",
        "#     Decodes a one-hot encoded tensor back to SMILES.\n",
        "\n",
        "#     Args:\n",
        "#         one_hot_tensor (torch.Tensor): One-hot encoded tensor\n",
        "#     '''\n",
        "#     smiles = ''\n",
        "#     one_hot_tensor = one_hot_tensor.view(-1, len(idx_to_char))  # unflatten\n",
        "#     for row in one_hot_tensor:\n",
        "#         idx = row.argmax().item()\n",
        "#         smiles += idx_to_char[idx]\n",
        "#     return smiles.strip()\n",
        "\n",
        "def verify_smiles(smiles):\n",
        "  '''\n",
        "  Verifies the validity of a SMILES string using RDKit.\n",
        "\n",
        "  Args:\n",
        "      smiles (str): SMILES string to verify\n",
        "\n",
        "  Returns:\n",
        "      bool: True if valid, False otherwise\n",
        "  '''\n",
        "  mol = Chem.MolFromSmiles(smiles)\n",
        "  return mol is not None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "y7wjyy_pGPj1"
      },
      "outputs": [],
      "source": [
        "# Generate a new molecule from VAE by sampling from the latent space\n",
        "def generate_smiles(model, latent_dim, idx_to_char, max_length, vocab_size, temperature=1.0):\n",
        "    '''\n",
        "    Generates a new SMILES string by sampling from the VAE's latent space.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): VAE model\n",
        "        latent_dim (int): Dimension of the latent space\n",
        "    '''\n",
        "    z = torch.randn(1, latent_dim).to(model.fc1.weight.device)  # Ensure z is on the same device as the model\n",
        "    with torch.no_grad():\n",
        "        generated = model.decode(z)\n",
        "\n",
        "    # Add postprocessing to convert to SMILES\n",
        "    probs = F.softmax(generated.view(max_length, vocab_size) / temperature, dim=-1)\n",
        "\n",
        "    # Sample the next character from the probability distribution\n",
        "    generated_tokens_indices = torch.multinomial(probs, 1).cpu().numpy().flatten()\n",
        "\n",
        "    # Iterate through indices to build the SMILES string\n",
        "    generated_smiles = \"\".join([idx_to_char.get(i, \"\") for i in generated_tokens_indices])\n",
        "    # generated_smiles = generated_smiles.replace('^', '').replace('$', '')\n",
        "    if '$' in generated_smiles:\n",
        "      generated_smiles = generated_smiles.split('$')[0]\n",
        "    generated_smiles = generated_smiles.lstrip('^')\n",
        "\n",
        "    # Verification using rdkit\n",
        "    is_valid = verify_smiles(generated_smiles)\n",
        "    if is_valid:\n",
        "      return generated_smiles\n",
        "    else:\n",
        "      return \"INVALID\"\n",
        "\n",
        "    # return generated_smiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "K3f3fLDBGVlI"
      },
      "outputs": [],
      "source": [
        "# Dataset class for SMILES strings\n",
        "\n",
        "# Contemplate Protein To Vector Encoding\n",
        "class SMILESDataset(Dataset):\n",
        "    def __init__(self, smiles_list, max_length=150, char_to_idx=None):\n",
        "        '''\n",
        "        Initializes the SMILESDataset with a list of SMILES strings.\n",
        "\n",
        "        Args:\n",
        "            smiles_list (list): List of SMILES strings\n",
        "            max_length (int): Maximum length of the SMILES strings\n",
        "            char_to_idx (dict): Character-to-index mapping\n",
        "\n",
        "        The dataset will one-hot encode each character in a SMILES string to a fixed-size tensor of shape (max_length * vocab_size).\n",
        "        If a SMILES string is shorter than max_length, it will be padded with zeros. If longer, it will be truncated.\n",
        "        '''\n",
        "        self.smiles_list = smiles_list\n",
        "        self.max_length = max_length\n",
        "\n",
        "        if char_to_idx is None:\n",
        "            raise ValueError(\"Please provide a fixed character-to-index mapping\")\n",
        "            # self.char_to_idx, self.idx_to_char = build_vocabulary(smiles_list)\n",
        "        else:\n",
        "            self.char_to_idx = char_to_idx\n",
        "            self.idx_to_char = {v: k for k, v in char_to_idx.items()}\n",
        "\n",
        "        self.vocab_size = len(self.char_to_idx)\n",
        "\n",
        "        original_count = len(smiles_list)\n",
        "        filtered = []\n",
        "        invalid_count = 0\n",
        "\n",
        "        for s in smiles_list:\n",
        "            s = s.strip()\n",
        "            if all(c in self.char_to_idx for c in s):\n",
        "                filtered.append(s)\n",
        "            else:\n",
        "                invalid_count += 1\n",
        "        print(f\"Total: {original_count}, Valid: {len(filtered)}, Invalid: {invalid_count}\")\n",
        "        self.smiles_list = filtered\n",
        "\n",
        "    def __len__(self):\n",
        "        '''\n",
        "        Returns:\n",
        "            int: Number of valid SMILES strings in the dataset\n",
        "        '''\n",
        "\n",
        "        return len(self.smiles_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        '''\n",
        "        Fetches the encoded version of a SMILES string at a given index.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the SMILES string to retrieve\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: One-hot encoded tensor of the SMILES string of shape (max_length * vocab_size)\n",
        "        '''\n",
        "\n",
        "        smiles = self.smiles_list[idx]\n",
        "\n",
        "        # One-hot encode the SMILES string\n",
        "        encoded = torch.zeros(self.max_length, self.vocab_size)\n",
        "        for i, char in enumerate(smiles[:self.max_length]):\n",
        "            encoded[i, self.char_to_idx[char]] = 1.0\n",
        "\n",
        "        return encoded.view(-1) #Flatten into 1D tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "p_jonbB6GWWA"
      },
      "outputs": [],
      "source": [
        "class AffineCouplingLayer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        if input_dim % 2 != 0:\n",
        "            input_dim += 1\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim // 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, input_dim // 2 * 2)\n",
        "        )\n",
        "\n",
        "        # Initialize last layer with zeros for stable training\n",
        "        nn.init.zeros_(self.net[-1].weight)\n",
        "        nn.init.zeros_(self.net[-1].bias)\n",
        "\n",
        "    def forward(self, x, reverse=False):\n",
        "        x1, x2 = x.chunk(2, dim=1)\n",
        "\n",
        "        # Get scaling and translation factors\n",
        "        st = self.net(x1)\n",
        "        s, t = st.chunk(2, dim=1)\n",
        "\n",
        "        # Apply scaling with numerical stability\n",
        "        scale_factor = 0.001\n",
        "        s = torch.tanh(s) * scale_factor\n",
        "\n",
        "        # Compute log determinant (only from the scaling part)\n",
        "        log_det = torch.sum(s, dim=1)\n",
        "\n",
        "        if reverse:\n",
        "            # Inverse transformation\n",
        "            x2 = (x2 - t) * torch.exp(-s)\n",
        "            return torch.cat([x1, x2], dim=1), -log_det\n",
        "        else:\n",
        "            # Forward transformation\n",
        "            x2 = x2 * torch.exp(s) + t\n",
        "            return torch.cat([x1, x2], dim=1), log_det\n",
        "\n",
        "class Flow(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim + (input_dim % 2)\n",
        "\n",
        "        # Coupling layers without batch norm\n",
        "        self.layers = nn.ModuleList([\n",
        "            AffineCouplingLayer(self.input_dim, hidden_dim)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Layer normalization instead of batch norm\n",
        "        self.norms = nn.ModuleList([\n",
        "            nn.LayerNorm(self.input_dim)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, reverse=False):\n",
        "        # Initialize log determinant\n",
        "        log_det_total = torch.zeros(x.size(0), device=x.device)\n",
        "\n",
        "        # Handle odd dimensions\n",
        "        if x.size(1) % 2 != 0:\n",
        "            x = F.pad(x, (0, 1), 'constant', 0)\n",
        "\n",
        "        # Process through layers\n",
        "        if reverse:\n",
        "            for layer, norm in zip(reversed(self.layers), reversed(self.norms)):\n",
        "                x = norm(x)\n",
        "                x, log_det = layer(x, reverse=True)\n",
        "                log_det_total = log_det_total + log_det\n",
        "        else:\n",
        "            for layer, norm in zip(self.layers, self.norms):\n",
        "                x.norm()\n",
        "                x, log_det = layer(x, reverse=False)\n",
        "                log_det_total = log_det_total + log_det\n",
        "\n",
        "        return x, log_det_total\n",
        "\n",
        "    def get_latent(self, x):\n",
        "        \"\"\"Generate latent representation\"\"\"\n",
        "        z, _ = self.forward(x)\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Fu-zT8Y0GchV"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim, vocab_size):\n",
        "        super(VAE, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.fc1 = nn.Linear(input_dim, 256)\n",
        "        self.fc_mu = nn.Linear(256, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(256, latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.fc3 = nn.Linear(latent_dim, 256)\n",
        "        self.fc4 = nn.Linear(256, input_dim)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return self.fc_mu(h1), self.fc_logvar(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, self.input_dim))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "\n",
        "        recon_x = self.decode(z)\n",
        "        return recon_x, mu, logvar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "uKLLOBctGe4m"
      },
      "outputs": [],
      "source": [
        "def compute_vae_loss(recon_x, x, mu, logvar, dataset):\n",
        "    \"\"\"\n",
        "    Compute VAE loss with dataset-specific vocabulary size.\n",
        "\n",
        "    Args:\n",
        "        recon_x: Reconstructed input from VAE\n",
        "        x: Original input data\n",
        "        mu: Mean from VAE encoder\n",
        "        logvar: Log variance from VAE encoder\n",
        "        dataset: Dataset object containing vocab_size and max_length\n",
        "    \"\"\"\n",
        "    batch_size = x.size(0)\n",
        "    vocab_size = dataset.vocab_size\n",
        "    seq_len = dataset.max_length\n",
        "\n",
        "    # Reshape tensors to match dataset dimensions\n",
        "    x = x.view(batch_size, seq_len * vocab_size)\n",
        "    recon_x = recon_x.view(batch_size, seq_len * vocab_size)\n",
        "\n",
        "    # Reconstruction loss (BCE)\n",
        "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "\n",
        "    # KL divergence\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    # Weight the KLD term\n",
        "    beta = 0.1  # Adjust this weight to balance reconstruction vs. KLD\n",
        "\n",
        "    return BCE + beta * KLD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCUtI1XDGhUU",
        "outputId": "e0e5b771-00a2-4249-9cc1-414db6c0892a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique characters: 26\n",
            "Unique characters in dataset:\n",
            "['$', '(', ')', '-', '1', '2', '3', '4', '5', '=', 'B', 'C', 'F', 'H', 'N', 'O', 'S', '[', ']', '^', 'c', 'l', 'n', 'o', 'r', 's']\n",
            "Total: 10000, Valid: 10000, Invalid: 0\n",
            "Total: 10000, Valid: 10000, Invalid: 0\n",
            "Training Vocabulary Size: 26\n",
            "Test Vocabulary Size: 26\n",
            "# Train SMILES after filtering: 10000\n",
            "# Test SMILES after filtering: 10000\n",
            "Number of batches in train_loader: 625\n",
            "Number of batches in test_loader: 625\n"
          ]
        }
      ],
      "source": [
        "# Load SMILES strings\n",
        "#with open('dataset/train.txt', 'r') as f:\n",
        "    #smiles_train = [line.strip() for line in f][1:]\n",
        "\n",
        "#with open('dataset/test.txt', 'r') as f:\n",
        "     #smiles_test = [line.strip() for line in f]\n",
        "\n",
        "# dataset/train.txt\n",
        "smiles_train = load_smiles_from_csv('/content/train.txt', split_type='train')\n",
        "# dataset/test.txt\n",
        "smiles_test = load_smiles_from_csv('/content/test.txt', split_type='test')  # if test rows are in same file\n",
        "\n",
        "# Apply cleaning to your SMILES\n",
        "smiles_train = [clean_smiles(smiles) for smiles in smiles_train]\n",
        "smiles_test = [clean_smiles(smiles) for smiles in smiles_test]\n",
        "smiles_train = process_smiles(smiles_train)\n",
        "smiles_test = process_smiles(smiles_test)\n",
        "smiles_train = smiles_train[:10000]\n",
        "smiles_test = smiles_test[:10000]\n",
        "\n",
        "# print(f\"Raw SMILES loaded: train={len(smiles_train)}, test={len(smiles_test)}\") # output for testing purposes\n",
        "all_smiles = smiles_train + smiles_test\n",
        "unique_chars = extract_unique_chars(all_smiles)\n",
        "\n",
        "print(f\"Total unique characters: {len(unique_chars)}\")\n",
        "print(\"Unique characters in dataset:\")\n",
        "print(unique_chars)\n",
        "\n",
        "# Use extracted unique characters to rebuild vocabulary\n",
        "VALID_CHARS = unique_chars\n",
        "char_to_idx = {c: i for i, c in enumerate(VALID_CHARS)}\n",
        "idx_to_char = {i: c for c, i in char_to_idx.items()}\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SMILESDataset(smiles_train, max_length=150, char_to_idx=char_to_idx)\n",
        "test_dataset = SMILESDataset(smiles_test, max_length=150, char_to_idx=char_to_idx)\n",
        "print(\"Training Vocabulary Size:\", train_dataset.vocab_size)\n",
        "print(\"Test Vocabulary Size:\", test_dataset.vocab_size) # Should be the same\n",
        "\n",
        "\n",
        "print(f\"# Train SMILES after filtering: {len(train_dataset)}\")\n",
        "print(f\"# Test SMILES after filtering: {len(test_dataset)}\")\n",
        "# train_dataset = SMILESDataset(smiles_train)\n",
        "# test_dataset = SMILESDataset(smiles_test, char_to_idx=train_dataset.char_to_idx)  # Share vocabulary\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)  # No need to shuffle test data\n",
        "\n",
        "print(f\"Number of batches in train_loader: {len(train_loader)}\")\n",
        "print(f\"Number of batches in test_loader: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4IMnFlmGjfl",
        "outputId": "8049c9d2-e566-4946-ad46-86cbf4f8d006"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "\n",
            "Sample SMILES visualizations:\n",
            "\n",
            "Sample 1\n",
            "Original : ^CCCS(=O)c1ccc2[nH]c(=NC(=O)OC)[nH]c2c1$\n",
            "Decoded  : CCCS(=O)c1ccc2[nH]c(=NC(=O)OC)[nH]c2c1\n",
            "Shape    : torch.Size([3900])\n",
            "\n",
            "Sample 2\n",
            "Original : ^CC(C)(C)C(=O)C(Oc1ccc(Cl)cc1)n1ccnc1$\n",
            "Decoded  : CC(C)(C)C(=O)C(Oc1ccc(Cl)cc1)n1ccnc1\n",
            "Shape    : torch.Size([3900])\n",
            "\n",
            "Sample 3\n",
            "Original : ^Cc1c(Cl)cccc1Nc1ncccc1C(=O)OCC(O)CO$\n",
            "Decoded  : Cc1c(Cl)cccc1Nc1ncccc1C(=O)OCC(O)CO\n",
            "Shape    : torch.Size([3900])\n"
          ]
        }
      ],
      "source": [
        "# Check a batch of data\n",
        "for i, data in enumerate(train_loader):\n",
        "    if i == 0:  # Just visualize the first batch\n",
        "        print(data)\n",
        "        break\n",
        "\n",
        "# Visualize 3 samples\n",
        "print(\"\\nSample SMILES visualizations:\")\n",
        "for i in range(3):\n",
        "    encoded = train_dataset[i]\n",
        "    original = train_dataset.smiles_list[i]\n",
        "    decoded = decode_smiles(encoded, train_dataset.idx_to_char)\n",
        "\n",
        "    print(f\"\\nSample {i+1}\")\n",
        "    print(f\"Original : {original}\")\n",
        "    print(f\"Decoded  : {decoded}\")\n",
        "    print(f\"Shape    : {encoded.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CY8HBgZXGlki",
        "outputId": "b6320fb0-808c-4a66-dc21-8021a45cb4a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Vocab size: 26\n",
            "max_length: 150\n",
            "Input dim: 3900\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "import torch\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device = torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Instantiate the VAE model\n",
        "input_dim = train_dataset.vocab_size * train_dataset.max_length  # Flatten the input (max_length x vocab_size)\n",
        "latent_dim = 128\n",
        "\n",
        "vocab_size = train_dataset.vocab_size\n",
        "max_length = train_dataset.max_length\n",
        "\n",
        "print(\"Vocab size:\", train_dataset.vocab_size)\n",
        "print(\"max_length:\", train_dataset.max_length)\n",
        "print(\"Input dim:\", input_dim)\n",
        "\n",
        "flow = Flow(input_dim = input_dim, hidden_dim=256, num_layers=4).to(device)\n",
        "vae = VAE(input_dim, latent_dim, len(idx_to_char)).to(device)\n",
        "\n",
        "# Optimizer\n",
        "# Separate optimizers for Flow and VAE\n",
        "flow_optimizer = torch.optim.Adam(flow.parameters(), lr=0.0001)\n",
        "vae_optimizer = torch.optim.Adam(vae.parameters(), lr=0.0001)\n",
        "\n",
        "# Training parameters\n",
        "epochs = 100\n",
        "early_stop_patience = 5\n",
        "best_loss = float('inf')\n",
        "patience_counter = 0\n",
        "min_delta = 0.001\n",
        "\n",
        "# Lists to track losses\n",
        "flow_losses = []\n",
        "vae_losses = []\n",
        "total_losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "78J_amyCGoLW"
      },
      "outputs": [],
      "source": [
        "class CombinedModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim, num_flow_layers):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize FLOW for creating latent representation\n",
        "        self.flow = Flow(\n",
        "            input_dim=input_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_layers=num_flow_layers\n",
        "        )\n",
        "\n",
        "        # Projection layer to match the output dimensions of Flow to VAE input\n",
        "        self.projection_layer = nn.Linear(latent_dim, input_dim)\n",
        "\n",
        "        # Initialize VAE to work with FLOW's output\n",
        "        self.vae = VAE(\n",
        "            input_dim=input_dim,  # Flow preserves dimensionality\n",
        "            latent_dim=latent_dim,\n",
        "            vocab_size=vocab_size\n",
        "        )\n",
        "\n",
        "        self.flow_to_vae_projection = nn.Linear(input_dim, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First pass through FLOW to get latent representation\n",
        "        z_flow, ldj = self.flow(x)\n",
        "\n",
        "        # Project z_flow to the required latent dimension for VAE\n",
        "        z_flow_projected =  self.flow_to_vae_projection(z_flow) # self.projection_layer(z_flow)\n",
        "\n",
        "        # Use FLOW's output as input to VAE\n",
        "        recon_x, mu, logvar = self.vae(z_flow_projected)\n",
        "\n",
        "        return recon_x, mu, logvar, ldj, z_flow\n",
        "\n",
        "def train_step(model, data, flow_optimizer, vae_optimizer, device):\n",
        "    \"\"\"Separated training step for FLOW and VAE\"\"\"\n",
        "    data = data.to(device)\n",
        "\n",
        "    # 1. Train FLOW\n",
        "    flow_optimizer.zero_grad()\n",
        "    z_flow, ldj = model.flow(data)\n",
        "    flow_loss = -ldj.mean()  # Maximize log-likelihood\n",
        "    flow_loss.backward()\n",
        "    flow_optimizer.step()\n",
        "\n",
        "    # 2. Train VAE using FLOW's output (detached)\n",
        "    vae_optimizer.zero_grad()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        z_flow, _ = model.flow(data)  # Get fresh flow output\n",
        "        z_proj = model.flow_to_vae_projection(z_flow)  # Project to match VAE input\n",
        "    recon_batch, mu, logvar = model.vae(z_flow.detach())\n",
        "    vae_loss = compute_vae_loss(recon_batch, data, mu, logvar, train_dataset)\n",
        "    vae_loss.backward()\n",
        "    vae_optimizer.step()\n",
        "\n",
        "    return flow_loss.item(), vae_loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17DbZST2QSlE",
        "outputId": "c12c1ecc-fb3d-4b41-c300-47809cae8397"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1] Complete | Avg Flow Loss: -7.0767 | Avg VAE Loss: 6913.6247\n",
            "\n",
            "Epoch [2] Complete | Avg Flow Loss: -7.7997 | Avg VAE Loss: 2029.4208\n",
            "\n",
            "Epoch [3] Complete | Avg Flow Loss: -7.7999 | Avg VAE Loss: 1962.5749\n",
            "\n",
            "Epoch [4] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 1913.4216\n",
            "\n",
            "Epoch [5] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 1829.2306\n",
            "\n",
            "Epoch [6] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 1769.4888\n",
            "\n",
            "Epoch [7] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 1713.8262\n",
            "\n",
            "Epoch [8] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 1648.1831\n",
            "\n",
            "Epoch [9] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 1582.8250\n",
            "\n",
            "Epoch [10] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 1518.9068\n",
            "\n",
            "Epoch [11] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 1461.7836\n",
            "\n",
            "Epoch [12] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 1403.4849\n",
            "\n",
            "Epoch [13] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 1344.8789\n",
            "\n",
            "Epoch [14] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 1293.5906\n",
            "\n",
            "Epoch [15] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 1250.7126\n",
            "\n",
            "Epoch [16] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 1210.5938\n",
            "\n",
            "Epoch [17] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 1172.5594\n",
            "\n",
            "Epoch [18] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 1133.8521\n",
            "\n",
            "Epoch [19] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 1094.4404\n",
            "\n",
            "Epoch [20] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 1057.0978\n",
            "\n",
            "Epoch [21] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 1023.5008\n",
            "\n",
            "Epoch [22] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 992.2180\n",
            "\n",
            "Epoch [23] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 963.6057\n",
            "\n",
            "Epoch [24] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 938.2254\n",
            "\n",
            "Epoch [25] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 913.6069\n",
            "\n",
            "Epoch [26] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 889.9047\n",
            "\n",
            "Epoch [27] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 867.9876\n",
            "\n",
            "Epoch [28] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 846.6610\n",
            "\n",
            "Epoch [29] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 828.1326\n",
            "\n",
            "Epoch [30] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 809.0289\n",
            "\n",
            "Epoch [31] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 791.3200\n",
            "\n",
            "Epoch [32] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 775.2140\n",
            "\n",
            "Epoch [33] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 759.9020\n",
            "\n",
            "Epoch [34] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 745.6638\n",
            "\n",
            "Epoch [35] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 732.3381\n",
            "\n",
            "Epoch [36] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 719.7627\n",
            "\n",
            "Epoch [37] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 707.6755\n",
            "\n",
            "Epoch [38] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 695.9664\n",
            "\n",
            "Epoch [39] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 686.2021\n",
            "\n",
            "Epoch [40] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 676.1878\n",
            "\n",
            "Epoch [41] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 666.5631\n",
            "\n",
            "Epoch [42] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 657.4033\n",
            "\n",
            "Epoch [43] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 648.2366\n",
            "\n",
            "Epoch [44] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 639.5490\n",
            "\n",
            "Epoch [45] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 631.6451\n",
            "\n",
            "Epoch [46] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 623.7077\n",
            "\n",
            "Epoch [47] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 616.2733\n",
            "\n",
            "Epoch [48] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 608.1565\n",
            "\n",
            "Epoch [49] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 600.9406\n",
            "\n",
            "Epoch [50] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 593.0844\n",
            "\n",
            "Epoch [51] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 586.8874\n",
            "\n",
            "Epoch [52] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 579.2203\n",
            "\n",
            "Epoch [53] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 572.7942\n",
            "\n",
            "Epoch [54] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 566.3282\n",
            "\n",
            "Epoch [55] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 560.4838\n",
            "\n",
            "Epoch [56] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 553.9939\n",
            "\n",
            "Epoch [57] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 548.1492\n",
            "\n",
            "Epoch [58] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 542.6273\n",
            "\n",
            "Epoch [59] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 536.7336\n",
            "\n",
            "Epoch [60] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 531.1525\n",
            "\n",
            "Epoch [61] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 526.5914\n",
            "\n",
            "Epoch [62] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 521.5330\n",
            "\n",
            "Epoch [63] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 517.1977\n",
            "\n",
            "Epoch [64] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 512.2697\n",
            "\n",
            "Epoch [65] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 507.0446\n",
            "\n",
            "Epoch [66] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 503.5367\n",
            "\n",
            "Epoch [67] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 499.4080\n",
            "\n",
            "Epoch [68] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 495.0203\n",
            "\n",
            "Epoch [69] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 490.3583\n",
            "\n",
            "Epoch [70] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 486.2879\n",
            "\n",
            "Epoch [71] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 482.9682\n",
            "\n",
            "Epoch [72] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 478.7789\n",
            "\n",
            "Epoch [73] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 475.0766\n",
            "\n",
            "Epoch [74] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 471.0663\n",
            "\n",
            "Epoch [75] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 467.2938\n",
            "\n",
            "Epoch [76] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 464.1781\n",
            "\n",
            "Epoch [77] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 459.8203\n",
            "\n",
            "Epoch [78] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 457.6022\n",
            "\n",
            "Epoch [79] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 453.9822\n",
            "\n",
            "Epoch [80] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 450.5427\n",
            "\n",
            "Epoch [81] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 446.9096\n",
            "\n",
            "Epoch [82] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 443.5427\n",
            "\n",
            "Epoch [83] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 441.0734\n",
            "\n",
            "Epoch [84] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 438.6065\n",
            "\n",
            "Epoch [85] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 434.9186\n",
            "\n",
            "Epoch [86] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 433.0865\n",
            "\n",
            "Epoch [87] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 429.2112\n",
            "\n",
            "Epoch [88] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 426.9529\n",
            "\n",
            "Epoch [89] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 424.4821\n",
            "\n",
            "Epoch [90] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 421.8403\n",
            "\n",
            "Epoch [91] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 419.0002\n",
            "\n",
            "Epoch [92] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 417.2457\n",
            "\n",
            "Epoch [93] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 414.5666\n",
            "\n",
            "Epoch [94] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 411.9850\n",
            "\n",
            "Epoch [95] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 410.0483\n",
            "\n",
            "Epoch [96] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 407.8871\n",
            "\n",
            "Epoch [97] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 405.2458\n",
            "\n",
            "Epoch [98] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 402.7264\n",
            "\n",
            "Epoch [99] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 401.2502\n",
            "\n",
            "Epoch [100] Complete | Avg Flow Loss: -7.8000 | Avg VAE Loss: 398.7603\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "model = CombinedModel(\n",
        "    input_dim=input_dim,\n",
        "    hidden_dim=256,\n",
        "    latent_dim=latent_dim,\n",
        "    num_flow_layers=4\n",
        ").to(device)\n",
        "\n",
        "flow_optimizer = optim.Adam(model.flow.parameters(), lr=0.0001)\n",
        "vae_optimizer = optim.Adam(model.vae.parameters(), lr=0.0001)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    epoch_flow_loss = 0\n",
        "    epoch_vae_loss = 0\n",
        "    num_batches = len(train_loader)\n",
        "    #print(f'Epoch#: {epoch}')\n",
        "\n",
        "    for batch_idx, data in enumerate(train_loader):\n",
        "        flow_loss, vae_loss = train_step(\n",
        "            model, data, flow_optimizer, vae_optimizer, device\n",
        "        )\n",
        "\n",
        "        epoch_flow_loss += flow_loss\n",
        "        epoch_vae_loss += vae_loss\n",
        "\n",
        "        # if (batch_idx + 1) % 100 == 0:\n",
        "        #     print(f'Batch [{batch_idx + 1}/{len(train_loader)}] | '\n",
        "        #           f'Flow Loss: {flow_loss:.4f} | '\n",
        "        #           f'VAE Loss: {vae_loss:.4f}')\n",
        "    avg_flow_loss = epoch_flow_loss / num_batches\n",
        "    avg_vae_loss = epoch_vae_loss / num_batches\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}] Complete | '\n",
        "          f'Avg Flow Loss: {avg_flow_loss:.4f} | '\n",
        "          f'Avg VAE Loss: {avg_vae_loss:.4f}\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 291,
      "metadata": {
        "id": "lDB-QQyPQXTz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d056b083-fd45-4e7c-ca21-8652c2e0d2e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INVALID\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[00:19:42] SMILES Parse Error: syntax error while parsing: H(Ss)(l=2F]nN5c(sSF3[2co=l]OrH1F=\n",
            "[00:19:42] SMILES Parse Error: Failed parsing SMILES 'H(Ss)(l=2F]nN5c(sSF3[2co=l]OrH1F=' for input: 'H(Ss)(l=2F]nN5c(sSF3[2co=l]OrH1F='\n"
          ]
        }
      ],
      "source": [
        "# Generate a new molecule from VAE by sampling from the latent space\n",
        "#generated_smiles = generate_smiles(vae, latent_dim, train_dataset.idx_to_char, max_length, vocab_size)  # pass idx_to_char\n",
        "# Generate molecules using the trained models\n",
        "temperature = 1.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    z = torch.randn(1, latent_dim).to(device)\n",
        "    #print(f\"Original z shape: {z.shape}\")  # Before projection\n",
        "    z_projected = model.projection_layer(z)\n",
        "    #print(f\"Projected z shape: {z_projected.shape}\")  # After projection\n",
        "    z_flow, _ = model.flow(z_projected)\n",
        "    z_flow_vae_compatible = model.flow_to_vae_projection(z_flow)\n",
        "    #print(f\"Flow to VAE projected shape: {z_flow_vae_compatible.shape}\")\n",
        "    generated = model.vae.decode(z_flow_vae_compatible)\n",
        "\n",
        "    # Add postprocessing to convert to SMILES\n",
        "    probs = F.softmax(generated.view(max_length, vocab_size) / temperature, dim=-1)\n",
        "\n",
        "    # Sample the next character from the probability distribution\n",
        "    generated_tokens_indices = torch.multinomial(probs, 1).cpu().numpy().flatten()\n",
        "\n",
        "    # Iterate through indices to build the SMILES string\n",
        "    generated_smiles = \"\".join([idx_to_char.get(i, \"\") for i in generated_tokens_indices])\n",
        "    # generated_smiles = generated_smiles.replace('^', '').replace('$', '')\n",
        "    if '$' in generated_smiles:\n",
        "      generated_smiles = generated_smiles.split('$')[0]\n",
        "\n",
        "    generated_smiles = generated_smiles.replace('^', '')\n",
        "\n",
        "    # Verification using rdkit\n",
        "    is_valid = verify_smiles(generated_smiles)\n",
        "    if is_valid:\n",
        "      print(f'{generated_smiles}')\n",
        "    else:\n",
        "      print(\"INVALID\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}