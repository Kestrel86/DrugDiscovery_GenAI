{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dl_LFACtFxLV",
        "outputId": "b17d6ab3-49b8-480c-ef0e-13c6509df1e0"
      },
      "outputs": [],
      "source": [
        "# !pip install rdkit-pypi\n",
        "# !git clone https://github.com/molecularsets/moses.git\n",
        "\n",
        "import matplotlib as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from rdkit import Chem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "6_cUokLgF8gD"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def load_smiles_from_csv(path, split_type='train'):\n",
        "    '''\n",
        "    Loads SMILES strings from a CSV file.\n",
        "\n",
        "    Args:\n",
        "        path (str): Path to the CSV file\n",
        "        split_type (str): Split type ('train' or 'test')\n",
        "\n",
        "    Returns:\n",
        "        list: List of SMILES strings\n",
        "    '''\n",
        "    smiles = []\n",
        "    with open(path, 'r') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            if row['SPLIT'].strip().lower() == split_type:\n",
        "                smiles.append(row['SMILES'].strip())\n",
        "    return smiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "xYSD_yd3F_qy"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Functions are from the RNN model we have but not entirely sure where they would fit in the VAE\n",
        "Currently working on implementation, the process_smiles function may help in creating valid molecules\n",
        "'''\n",
        "\n",
        "# Function to add start and end tokens\n",
        "def process_smiles(smiles_list):\n",
        "    return [\"^\" + s + \"$\" for s in smiles_list]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "1wJys-XBGH93"
      },
      "outputs": [],
      "source": [
        "def extract_unique_atoms(smiles_list):\n",
        "    unique_atoms = set()\n",
        "\n",
        "    for smiles in smiles_list:\n",
        "        # Parse the SMILES string\n",
        "        molecule = Chem.MolFromSmiles(smiles)\n",
        "\n",
        "        # If the molecule is valid, extract the atoms\n",
        "        if molecule:\n",
        "            for atom in molecule.GetAtoms():\n",
        "                # Get the atom symbol and add it to the set\n",
        "                unique_atoms.add(atom.GetSymbol())\n",
        "\n",
        "    # Convert the set of unique atoms to a list and return it\n",
        "    return list(unique_atoms)\n",
        "\n",
        "def tokenize_smiles(smiles):\n",
        "    \"\"\"\n",
        "    Tokenizes a SMILES string into meaningful tokens.\n",
        "    Handles multi-character atoms like Cl and Br.\n",
        "    \"\"\"\n",
        "    regex = r\"\"\"\n",
        "        Cl|Br|                         # Two-character atoms\n",
        "        \\[[^\\[\\]]*?\\]|                # Bracket expressions [NH4+], [C@H], etc.\n",
        "        \\%\\d{2}|                      # Ring numbers like %10\n",
        "        \\d|                           # Ring closures 1-9\n",
        "        =|#|-|\\+|\\\\|/|\\.|:|~|         # Bonds and stereochemistry\n",
        "        [A-Za-z]|                     # Single-letter atoms\n",
        "        [\\(\\)]                        # Branches\n",
        "    \"\"\"\n",
        "    pattern = re.compile(regex, re.X)\n",
        "    return pattern.findall(smiles)\n",
        "def extract_unique_tokens(smiles_list):\n",
        "    \"\"\"\n",
        "    Extract unique tokens from a list of SMILES strings.\n",
        "    Handles multi-character atoms (Cl, Br), symbols, etc.\n",
        "    \"\"\"\n",
        "    unique_tokens = set()\n",
        "    for smiles in smiles_list:\n",
        "        tokens = tokenize_smiles(smiles)\n",
        "        unique_tokens.update(tokens)\n",
        "    return sorted(unique_tokens)\n",
        "\n",
        "def clean_smiles(smiles):\n",
        "    '''\n",
        "    Cleans a SMILES string by removing unwanted characters.\n",
        "\n",
        "    Args:\n",
        "        smiles (str): SMILES string\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned SMILES string\n",
        "    '''\n",
        "    # Remove unwanted metadata and special characters\n",
        "    cleaned = smiles.split(',')[0].strip()\n",
        "    cleaned = cleaned.replace('#', '')  # Remove '#' characters\n",
        "    cleaned = cleaned.replace('$', '')  # Remove '$' characters\n",
        "    cleaned = cleaned.replace('^', '')  # Remove '^' characters if present\n",
        "    return cleaned\n",
        "\n",
        "# trying out start and end characters\n",
        "def decode_smiles(one_hot_tensor, idx_to_char):\n",
        "    '''\n",
        "    Decodes a one-hot encoded tensor back to a SMILES string, stopping at the end token.\n",
        "    '''\n",
        "    smiles = ''\n",
        "    one_hot_tensor = one_hot_tensor.view(-1, len(idx_to_char))\n",
        "    for row in one_hot_tensor:\n",
        "        idx = row.argmax().item()\n",
        "        char = idx_to_char[idx]\n",
        "        if char == '$' and len(smiles) > 0:  # End of sequence\n",
        "            break\n",
        "        if char != '^':  # Skip start token\n",
        "            smiles += char\n",
        "    return smiles.strip()\n",
        "\n",
        "def verify_smiles(smiles):\n",
        "  '''\n",
        "  Verifies the validity of a SMILES string using RDKit.\n",
        "\n",
        "  Args:\n",
        "      smiles (str): SMILES string to verify\n",
        "\n",
        "  Returns:\n",
        "      bool: True if valid, False otherwise\n",
        "  '''\n",
        "  mol = Chem.MolFromSmiles(smiles)\n",
        "  return mol is not None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "y7wjyy_pGPj1"
      },
      "outputs": [],
      "source": [
        "# Generate a new molecule from VAE by sampling from the latent space\n",
        "def generate_smiles(model, latent_dim, idx_to_char, char_to_idx, max_length, temperature=1.0):\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # 1. Sample latent vector\n",
        "        z = torch.randn(1, latent_dim).to(device)\n",
        "        z_projected = model.projection_layer(z)\n",
        "        z_flow, _ = model.flow(z_projected)\n",
        "        z_flow_vae_compatible = model.flow_to_vae_projection(z_flow)\n",
        "\n",
        "        # 2. Generate character-by-character\n",
        "        generated_indices = []\n",
        "        input_sequence = []\n",
        "\n",
        "        current_char = '^'  # Start token\n",
        "        for t in range(max_length):\n",
        "            # Encode current input sequence to one-hot (if needed)\n",
        "            input_seq = [char_to_idx.get(c, 0) for c in input_sequence + [current_char]]\n",
        "            one_hot = torch.zeros(max_length, len(char_to_idx), device=device)\n",
        "            for i, idx in enumerate(input_seq):\n",
        "                one_hot[i, idx] = 1.0\n",
        "\n",
        "            one_hot = one_hot.view(1, -1)  # Flatten for VAE\n",
        "\n",
        "            # Forward through decoder using latent\n",
        "            decoder_output = model.vae.decode(z_flow_vae_compatible)\n",
        "\n",
        "            # Get distribution for the current position\n",
        "            probs = F.softmax(\n",
        "                decoder_output.view(max_length, len(char_to_idx))[t] / temperature,\n",
        "                dim=-1\n",
        "            )\n",
        "\n",
        "            if t >= 0:\n",
        "                probs[char_to_idx['^']] = 0.0\n",
        "                #probs[char_to_idx['_']] = 0.0\n",
        "                probs = probs / probs.sum()\n",
        "\n",
        "            next_char_idx = torch.multinomial(probs, 1).item()\n",
        "            next_char = idx_to_char.get(next_char_idx, '')\n",
        "\n",
        "            if next_char == '$':\n",
        "                break\n",
        "\n",
        "            generated_indices.append(next_char_idx)\n",
        "            input_sequence.append(current_char)\n",
        "            current_char = next_char\n",
        "\n",
        "        generated_smiles = ''.join([idx_to_char.get(i, '') for i in generated_indices])\n",
        "            # Verification using rdkit\n",
        "        is_valid = verify_smiles(generated_smiles)\n",
        "        if is_valid:\n",
        "            return generated_smiles\n",
        "        else:\n",
        "            return \"INVALID\"\n",
        "\n",
        "        # return generated_smiles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "K3f3fLDBGVlI"
      },
      "outputs": [],
      "source": [
        "# Dataset class for SMILES strings\n",
        "\n",
        "# Contemplate Protein To Vector Encoding\n",
        "class SMILESDataset(Dataset):\n",
        "    def __init__(self, smiles_list, max_length=70, char_to_idx=None):\n",
        "        '''\n",
        "        Initializes the SMILESDataset with a list of SMILES strings.\n",
        "\n",
        "        Args:\n",
        "            smiles_list (list): List of SMILES strings\n",
        "            max_length (int): Maximum length of the SMILES strings\n",
        "            char_to_idx (dict): Character-to-index mapping\n",
        "\n",
        "        The dataset will one-hot encode each character in a SMILES string to a fixed-size tensor of shape (max_length * vocab_size).\n",
        "        If a SMILES string is shorter than max_length, it will be padded with zeros. If longer, it will be truncated.\n",
        "        '''\n",
        "        self.smiles_list = smiles_list\n",
        "        self.max_length = max_length\n",
        "\n",
        "        if char_to_idx is None:\n",
        "            raise ValueError(\"Please provide a fixed character-to-index mapping\")\n",
        "            # self.char_to_idx, self.idx_to_char = build_vocabulary(smiles_list)\n",
        "        else:\n",
        "            self.char_to_idx = char_to_idx\n",
        "            self.idx_to_char = {v: k for k, v in char_to_idx.items()}\n",
        "\n",
        "        self.vocab_size = len(self.char_to_idx)\n",
        "\n",
        "        original_count = len(smiles_list)\n",
        "        filtered = []\n",
        "        invalid_count = 0\n",
        "\n",
        "        for s in smiles_list:\n",
        "            s = s.strip()\n",
        "            if all(c in self.char_to_idx for c in s):\n",
        "                filtered.append(s)\n",
        "            else:\n",
        "                invalid_count += 1\n",
        "        print(f\"Total: {original_count}, Valid: {len(filtered)}, Invalid: {invalid_count}\")\n",
        "        self.smiles_list = filtered\n",
        "\n",
        "    def __len__(self):\n",
        "        '''\n",
        "        Returns:\n",
        "            int: Number of valid SMILES strings in the dataset\n",
        "        '''\n",
        "\n",
        "        return len(self.smiles_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        '''\n",
        "        Fetches the encoded version of a SMILES string at a given index.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the SMILES string to retrieve\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: One-hot encoded tensor of the SMILES string of shape (max_length * vocab_size)\n",
        "        '''\n",
        "\n",
        "        smiles = self.smiles_list[idx]\n",
        "\n",
        "        # One-hot encode the SMILES string\n",
        "        encoded = torch.zeros(self.max_length, self.vocab_size)\n",
        "        for i, char in enumerate(smiles[:self.max_length]):\n",
        "            encoded[i, self.char_to_idx[char]] = 1.0\n",
        "\n",
        "        return encoded.view(-1) #Flatten into 1D tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "p_jonbB6GWWA"
      },
      "outputs": [],
      "source": [
        "class AffineCouplingLayer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        if input_dim % 2 != 0:\n",
        "            input_dim += 1\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim // 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, input_dim // 2 * 2)\n",
        "        )\n",
        "\n",
        "        # Initialize last layer with zeros for stable training\n",
        "        nn.init.zeros_(self.net[-1].weight)\n",
        "        nn.init.zeros_(self.net[-1].bias)\n",
        "\n",
        "    def forward(self, x, reverse=False):\n",
        "        x1, x2 = x.chunk(2, dim=1)\n",
        "\n",
        "        # Get scaling and translation factors\n",
        "        st = self.net(x1)\n",
        "        s, t = st.chunk(2, dim=1)\n",
        "\n",
        "        # Apply scaling with numerical stability\n",
        "        scale_factor = 0.001\n",
        "        s = torch.tanh(s) * scale_factor\n",
        "\n",
        "        # Compute log determinant (only from the scaling part)\n",
        "        log_det = torch.sum(s, dim=1)\n",
        "\n",
        "        if reverse:\n",
        "            # Inverse transformation\n",
        "            x2 = (x2 - t) * torch.exp(-s)\n",
        "            return torch.cat([x1, x2], dim=1), -log_det\n",
        "        else:\n",
        "            # Forward transformation\n",
        "            x2 = x2 * torch.exp(s) + t\n",
        "            return torch.cat([x1, x2], dim=1), log_det\n",
        "\n",
        "class Flow(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim + (input_dim % 2)\n",
        "\n",
        "        # Coupling layers without batch norm\n",
        "        self.layers = nn.ModuleList([\n",
        "            AffineCouplingLayer(self.input_dim, hidden_dim)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Layer normalization instead of batch norm\n",
        "        self.norms = nn.ModuleList([\n",
        "            nn.LayerNorm(self.input_dim)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, reverse=False):\n",
        "        # Initialize log determinant\n",
        "        log_det_total = torch.zeros(x.size(0), device=x.device)\n",
        "\n",
        "        # Handle odd dimensions\n",
        "        if x.size(1) % 2 != 0:\n",
        "            x = F.pad(x, (0, 1), 'constant', 0)\n",
        "\n",
        "        # Process through layers\n",
        "        if reverse:\n",
        "            for layer, norm in zip(reversed(self.layers), reversed(self.norms)):\n",
        "                x = norm(x)\n",
        "                x, log_det = layer(x, reverse=True)\n",
        "                log_det_total = log_det_total + log_det\n",
        "        else:\n",
        "            for layer, norm in zip(self.layers, self.norms):\n",
        "                #x.norm()\n",
        "                x = norm(x)\n",
        "                x, log_det = layer(x, reverse=False)\n",
        "                log_det_total = log_det_total + log_det\n",
        "\n",
        "        return x, log_det_total\n",
        "\n",
        "    def get_latent(self, x):\n",
        "        \"\"\"Generate latent representation\"\"\"\n",
        "        z, _ = self.forward(x)\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "Fu-zT8Y0GchV"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim, vocab_size):\n",
        "        super(VAE, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        # Encoder\n",
        "        # self.fc1 = nn.Linear(input_dim, 256)\n",
        "        # self.fc_mu = nn.Linear(256, latent_dim)\n",
        "        # self.fc_logvar = nn.Linear(256, latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.fc3 = nn.Linear(latent_dim, 256)\n",
        "        self.fc4 = nn.Linear(256, input_dim)\n",
        "\n",
        "\n",
        "    # def encode(self, x):\n",
        "    #     h1 = F.relu(self.fc1(x))\n",
        "    #     return self.fc_mu(h1), self.fc_logvar(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # mu, logvar = self.encode(x.view(-1, self.input_dim))\n",
        "        # z = self.reparameterize(mu, logvar)\n",
        "\n",
        "        recon_x = self.decode(x)\n",
        "        # return recon_x, mu, logvar\n",
        "        return recon_x, None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "uKLLOBctGe4m"
      },
      "outputs": [],
      "source": [
        "def compute_vae_loss(recon_x, x, mu, logvar, dataset):\n",
        "    \"\"\"\n",
        "    Compute VAE loss with dataset-specific vocabulary size.\n",
        "\n",
        "    Args:\n",
        "        recon_x: Reconstructed input from VAE\n",
        "        x: Original input data\n",
        "        mu: Mean from VAE encoder\n",
        "        logvar: Log variance from VAE encoder\n",
        "        dataset: Dataset object containing vocab_size and max_length\n",
        "    \"\"\"\n",
        "    batch_size = x.size(0)\n",
        "    vocab_size = dataset.vocab_size\n",
        "    seq_len = dataset.max_length\n",
        "\n",
        "    # Reshape tensors to match dataset dimensions\n",
        "    x = x.view(batch_size, seq_len * vocab_size)\n",
        "    recon_x = recon_x.view(batch_size, seq_len * vocab_size)\n",
        "\n",
        "    # Reconstruction loss (BCE)\n",
        "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "\n",
        "    # skip KL divergence when using FLOW latent space\n",
        "    if mu is None or logvar is None:\n",
        "        return BCE\n",
        "\n",
        "    # KL divergence\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    # Weight the KLD term\n",
        "    beta = 0.1  # Adjust this weight to balance reconstruction vs. KLD\n",
        "\n",
        "    return BCE + beta * KLD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCUtI1XDGhUU",
        "outputId": "ff91801b-0c70-4a1d-e4a7-efcc8d33038b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['N', 'Cl', 'F', 'S', 'Br', 'O', 'C']\n",
            "59\n",
            "Total unique characters: 24\n",
            "Unique characters in dataset:\n",
            "['^', '$', '(', ')', '1', '2', '3', '4', '5', '6', '=', 'Br', 'C', 'Cl', 'F', 'N', 'O', 'S', '[H]', '[nH]', 'c', 'n', 'o', 's']\n",
            "Total: 100000, Valid: 60694, Invalid: 39306\n",
            "Total: 100000, Valid: 63678, Invalid: 36322\n",
            "Training Vocabulary Size: 24\n",
            "Test Vocabulary Size: 24\n",
            "# Train SMILES after filtering: 60694\n",
            "# Test SMILES after filtering: 63678\n",
            "Number of batches in train_loader: 1897\n",
            "Number of batches in test_loader: 1990\n"
          ]
        }
      ],
      "source": [
        "# Load SMILES strings\n",
        "#with open('dataset/train.txt', 'r') as f:\n",
        "    #smiles_train = [line.strip() for line in f][1:]\n",
        "\n",
        "#with open('dataset/test.txt', 'r') as f:\n",
        "     #smiles_test = [line.strip() for line in f]\n",
        "\n",
        "# dataset/train.txt\n",
        "smiles_train = load_smiles_from_csv('dataset/train.txt', split_type='train')\n",
        "# dataset/test.txt\n",
        "smiles_test = load_smiles_from_csv('dataset/test.txt', split_type='test')  # if test rows are in same file\n",
        "\n",
        "# Apply cleaning to your SMILES\n",
        "smiles_train = [clean_smiles(smiles) for smiles in smiles_train]\n",
        "smiles_test = [clean_smiles(smiles) for smiles in smiles_test]\n",
        "\n",
        "\n",
        "smiles_train = smiles_train[:100000]\n",
        "smiles_test = smiles_test[:100000]\n",
        "\n",
        "# print(f\"Raw SMILES loaded: train={len(smiles_train)}, test={len(smiles_test)}\") # output for testing purposes\n",
        "all_smiles = smiles_train + smiles_test\n",
        "print(extract_unique_atoms(all_smiles))\n",
        "all_smiles = process_smiles(all_smiles)\n",
        "unique_chars = ['^', '$'] + extract_unique_tokens(all_smiles)\n",
        "l = max(all_smiles, key=len)\n",
        "print(len(l))\n",
        "\n",
        "print(f\"Total unique characters: {len(unique_chars)}\")\n",
        "print(\"Unique characters in dataset:\")\n",
        "print(unique_chars)\n",
        "\n",
        "# Use extracted unique characters to rebuild vocabulary\n",
        "VALID_CHARS = unique_chars # leave open for adding special characters\n",
        "char_to_idx = {c: i for i, c in enumerate(VALID_CHARS)}\n",
        "idx_to_char = {i: c for c, i in char_to_idx.items()}\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SMILESDataset(smiles_train, max_length=70, char_to_idx=char_to_idx)\n",
        "test_dataset = SMILESDataset(smiles_test, max_length=70, char_to_idx=char_to_idx)\n",
        "print(\"Training Vocabulary Size:\", train_dataset.vocab_size)\n",
        "print(\"Test Vocabulary Size:\", test_dataset.vocab_size) # Should be the same\n",
        "\n",
        "\n",
        "print(f\"# Train SMILES after filtering: {len(train_dataset)}\")\n",
        "print(f\"# Test SMILES after filtering: {len(test_dataset)}\")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)  # No need to shuffle test data\n",
        "\n",
        "print(f\"Number of batches in train_loader: {len(train_loader)}\")\n",
        "print(f\"Number of batches in test_loader: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4IMnFlmGjfl",
        "outputId": "c22a3b29-679e-4a70-a1dd-9097e53a6792"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "\n",
            "Sample SMILES visualizations:\n",
            "\n",
            "Sample 1\n",
            "Original : Cn1cnc2c1c(=O)n(CC(O)CO)c(=O)n2C\n",
            "Decoded  : Cn1cnc2c1c(=O)n(CC(O)CO)c(=O)n2C\n",
            "Shape    : torch.Size([1680])\n",
            "\n",
            "Sample 2\n",
            "Original : CCOC(=O)c1cncn1C1CCCc2ccccc21\n",
            "Decoded  : CCOC(=O)c1cncn1C1CCCc2ccccc21\n",
            "Shape    : torch.Size([1680])\n",
            "\n",
            "Sample 3\n",
            "Original : COc1ccccc1OC(=O)Oc1ccccc1OC\n",
            "Decoded  : COc1ccccc1OC(=O)Oc1ccccc1OC\n",
            "Shape    : torch.Size([1680])\n"
          ]
        }
      ],
      "source": [
        "# Check a batch of data\n",
        "for i, data in enumerate(train_loader):\n",
        "    if i == 0:  # Just visualize the first batch\n",
        "        print(data)\n",
        "        break\n",
        "\n",
        "# Visualize 3 samples\n",
        "print(\"\\nSample SMILES visualizations:\")\n",
        "for i in range(3):\n",
        "    encoded = train_dataset[i]\n",
        "    original = train_dataset.smiles_list[i]\n",
        "    decoded = decode_smiles(encoded, train_dataset.idx_to_char)\n",
        "\n",
        "    print(f\"\\nSample {i+1}\")\n",
        "    print(f\"Original : {original}\")\n",
        "    print(f\"Decoded  : {decoded}\")\n",
        "    print(f\"Shape    : {encoded.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CY8HBgZXGlki",
        "outputId": "cf0da79b-0ee6-4e23-cdfb-aa19a6685951"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Vocab size: 24\n",
            "max_length: 70\n",
            "Input dim: 1680\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "import torch\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device = torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Instantiate the VAE model\n",
        "input_dim = train_dataset.vocab_size * train_dataset.max_length  # Flatten the input (max_length x vocab_size)\n",
        "latent_dim = 128\n",
        "\n",
        "vocab_size = train_dataset.vocab_size\n",
        "max_length = train_dataset.max_length\n",
        "\n",
        "print(\"Vocab size:\", train_dataset.vocab_size)\n",
        "print(\"max_length:\", train_dataset.max_length)\n",
        "print(\"Input dim:\", input_dim)\n",
        "\n",
        "flow = Flow(input_dim = input_dim, hidden_dim=256, num_layers=4).to(device)\n",
        "vae = VAE(input_dim, latent_dim, len(idx_to_char)).to(device)\n",
        "\n",
        "# Optimizer\n",
        "# Separate optimizers for Flow and VAE\n",
        "flow_optimizer = torch.optim.Adam(flow.parameters(), lr=0.0001)\n",
        "vae_optimizer = torch.optim.Adam(vae.parameters(), lr=0.0001)\n",
        "\n",
        "# Training parameters\n",
        "epochs = 50\n",
        "early_stop_patience = 5\n",
        "best_loss = float('inf')\n",
        "patience_counter = 0\n",
        "min_delta = 0.001\n",
        "\n",
        "# Lists to track losses\n",
        "flow_losses = []\n",
        "vae_losses = []\n",
        "total_losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "78J_amyCGoLW"
      },
      "outputs": [],
      "source": [
        "class CombinedModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim, num_flow_layers):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize FLOW for creating latent representation\n",
        "        self.flow = Flow(\n",
        "            input_dim=input_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_layers=num_flow_layers\n",
        "        )\n",
        "\n",
        "        # Projection layer to match the output dimensions of Flow to VAE input\n",
        "        self.projection_layer = nn.Linear(latent_dim, input_dim)  # For initial projection\n",
        "\n",
        "        # Initialize VAE to work with FLOW's output\n",
        "        self.vae = VAE(\n",
        "            input_dim=input_dim,  # Flow preserves dimensionality\n",
        "            latent_dim=latent_dim,\n",
        "            vocab_size=vocab_size\n",
        "        )\n",
        "\n",
        "        self.flow_to_vae_projection = nn.Linear(input_dim, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First pass through FLOW to get latent representation\n",
        "        z_flow, ldj = self.flow(x)\n",
        "\n",
        "        # Project z_flow to the required latent dimension for VAE\n",
        "        z_flow_projected =  self.flow_to_vae_projection(z_flow) # self.projection_layer(z_flow)\n",
        "\n",
        "        # Use FLOW's output as input to VAE\n",
        "        recon_x = self.vae.decode(z_flow_projected)  # Directly use the decoder\n",
        "        mu, logvar = None, None # Encoder bypassed\n",
        "\n",
        "        return recon_x, mu, logvar, ldj, z_flow\n",
        "\n",
        "def train_step(model, data, flow_optimizer, vae_optimizer, device):\n",
        "    \"\"\"Separated training step for FLOW and VAE\"\"\"\n",
        "    data = data.to(device)\n",
        "\n",
        "    def flow_nll_loss(z, log_det):\n",
        "        log_pz = -0.5 * (z ** 2 + torch.log(torch.tensor(2 * torch.pi, device=z.device))).sum(dim=1)\n",
        "        log_px = log_pz + log_det\n",
        "        return -log_px.mean()\n",
        "\n",
        "    # 1. Train FLOW\n",
        "    flow_optimizer.zero_grad()\n",
        "    z_flow, ldj = model.flow(data)\n",
        "    flow_loss = flow_nll_loss(z_flow, ldj)\n",
        "    #flow_loss = -ldj.mean()  # Maximize log-likelihood\n",
        "    flow_loss.backward()\n",
        "    flow_optimizer.step()\n",
        "\n",
        "    # 2. Train VAE using FLOW's output (detached)\n",
        "    vae_optimizer.zero_grad()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # z_flow, _ = model.flow(data)  # Get fresh flow output\n",
        "        z_flow, _ = model.flow(data)\n",
        "        z_proj = model.flow_to_vae_projection(z_flow)  # Project to match VAE input\n",
        "\n",
        "\n",
        "    recon_batch = model.vae.decode(z_proj.detach()) \n",
        "    vae_loss = compute_vae_loss(recon_batch, data, None, None, train_dataset)\n",
        "    vae_loss.backward()\n",
        "    vae_optimizer.step()\n",
        "\n",
        "    return flow_loss.item(), vae_loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17DbZST2QSlE",
        "outputId": "b6e861d1-d18c-4b9f-f446-e3d32a3a9e34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1] Complete | Avg Flow Loss: 1565.5416 | Avg VAE Loss: 10668.0280\n",
            "\n",
            "Epoch [2] Complete | Avg Flow Loss: 1540.9686 | Avg VAE Loss: 3665.8563\n",
            "\n",
            "Epoch [3] Complete | Avg Flow Loss: 1540.7562 | Avg VAE Loss: 3540.1671\n",
            "\n",
            "Epoch [4] Complete | Avg Flow Loss: 1540.6570 | Avg VAE Loss: 3517.3152\n",
            "\n",
            "Epoch [5] Complete | Avg Flow Loss: 1540.5863 | Avg VAE Loss: 3511.0192\n",
            "\n",
            "Epoch [6] Complete | Avg Flow Loss: 1540.5182 | Avg VAE Loss: 3509.0007\n",
            "\n",
            "Epoch [7] Complete | Avg Flow Loss: 1540.4810 | Avg VAE Loss: 3508.2104\n",
            "\n",
            "Epoch [8] Complete | Avg Flow Loss: 1541.9540 | Avg VAE Loss: 3512.1114\n",
            "\n",
            "Epoch [9] Complete | Avg Flow Loss: 1540.5108 | Avg VAE Loss: 3506.9706\n",
            "\n",
            "Epoch [10] Complete | Avg Flow Loss: 1540.4851 | Avg VAE Loss: 3507.2651\n",
            "\n",
            "Epoch [11] Complete | Avg Flow Loss: 1540.4713 | Avg VAE Loss: 3507.3556\n",
            "\n",
            "Epoch [12] Complete | Avg Flow Loss: 1540.4640 | Avg VAE Loss: 3507.5043\n",
            "\n",
            "Epoch [13] Complete | Avg Flow Loss: 1540.4623 | Avg VAE Loss: 3507.4975\n",
            "\n",
            "Epoch [14] Complete | Avg Flow Loss: 1540.4593 | Avg VAE Loss: 3507.5511\n",
            "\n",
            "Epoch [15] Complete | Avg Flow Loss: 1540.4583 | Avg VAE Loss: 3507.5708\n",
            "\n",
            "Epoch [16] Complete | Avg Flow Loss: 1540.4578 | Avg VAE Loss: 3507.6260\n",
            "\n",
            "Epoch [17] Complete | Avg Flow Loss: 1540.5095 | Avg VAE Loss: 3510.1663\n",
            "\n",
            "Epoch [18] Complete | Avg Flow Loss: 1540.4576 | Avg VAE Loss: 3507.5921\n",
            "\n",
            "Epoch [19] Complete | Avg Flow Loss: 1540.4573 | Avg VAE Loss: 3507.5850\n",
            "\n",
            "Epoch [20] Complete | Avg Flow Loss: 1540.4572 | Avg VAE Loss: 3507.6133\n",
            "\n",
            "Epoch [21] Complete | Avg Flow Loss: 1540.4572 | Avg VAE Loss: 3507.6376\n",
            "\n",
            "Epoch [22] Complete | Avg Flow Loss: 1540.5418 | Avg VAE Loss: 3509.2696\n",
            "\n",
            "Epoch [23] Complete | Avg Flow Loss: 1540.4573 | Avg VAE Loss: 3507.4680\n",
            "\n",
            "Epoch [24] Complete | Avg Flow Loss: 1540.4592 | Avg VAE Loss: 3507.5397\n",
            "\n",
            "Epoch [25] Complete | Avg Flow Loss: 1540.4572 | Avg VAE Loss: 3507.4627\n",
            "\n",
            "Epoch [26] Complete | Avg Flow Loss: 1540.4573 | Avg VAE Loss: 3507.4804\n",
            "\n",
            "Epoch [27] Complete | Avg Flow Loss: 1540.4572 | Avg VAE Loss: 3507.5151\n",
            "\n",
            "Epoch [28] Complete | Avg Flow Loss: 1540.4598 | Avg VAE Loss: 3507.5386\n",
            "\n",
            "Epoch [29] Complete | Avg Flow Loss: 1540.4568 | Avg VAE Loss: 3507.4452\n",
            "\n",
            "Epoch [30] Complete | Avg Flow Loss: 1540.4571 | Avg VAE Loss: 3507.5028\n",
            "\n",
            "Epoch [31] Complete | Avg Flow Loss: 1540.4571 | Avg VAE Loss: 3507.4724\n",
            "\n",
            "Epoch [32] Complete | Avg Flow Loss: 1540.4603 | Avg VAE Loss: 3507.6064\n",
            "\n",
            "Epoch [33] Complete | Avg Flow Loss: 1540.4568 | Avg VAE Loss: 3507.4832\n",
            "\n",
            "Epoch [34] Complete | Avg Flow Loss: 1540.4569 | Avg VAE Loss: 3507.4615\n",
            "\n",
            "Epoch [35] Complete | Avg Flow Loss: 1540.4570 | Avg VAE Loss: 3507.4363\n",
            "\n",
            "Epoch [36] Complete | Avg Flow Loss: 1540.4570 | Avg VAE Loss: 3507.5178\n",
            "\n",
            "Epoch [37] Complete | Avg Flow Loss: 1540.4637 | Avg VAE Loss: 3507.6881\n",
            "\n",
            "Epoch [38] Complete | Avg Flow Loss: 1540.4568 | Avg VAE Loss: 3507.3812\n",
            "\n",
            "Epoch [39] Complete | Avg Flow Loss: 1540.4569 | Avg VAE Loss: 3507.4006\n",
            "\n",
            "Epoch [40] Complete | Avg Flow Loss: 1540.4569 | Avg VAE Loss: 3507.3859\n",
            "\n",
            "Epoch [41] Complete | Avg Flow Loss: 1540.4570 | Avg VAE Loss: 3507.3695\n",
            "\n",
            "Epoch [42] Complete | Avg Flow Loss: 1540.4601 | Avg VAE Loss: 3507.4251\n",
            "\n",
            "Epoch [43] Complete | Avg Flow Loss: 1540.4568 | Avg VAE Loss: 3507.2325\n",
            "\n",
            "Epoch [44] Complete | Avg Flow Loss: 1540.4569 | Avg VAE Loss: 3507.2279\n",
            "\n",
            "Epoch [45] Complete | Avg Flow Loss: 1540.4569 | Avg VAE Loss: 3507.2106\n",
            "\n",
            "Epoch [46] Complete | Avg Flow Loss: 1540.4577 | Avg VAE Loss: 3507.2719\n",
            "\n",
            "Epoch [47] Complete | Avg Flow Loss: 1540.4568 | Avg VAE Loss: 3507.2189\n",
            "\n",
            "Epoch [48] Complete | Avg Flow Loss: 1540.4569 | Avg VAE Loss: 3507.2148\n",
            "\n",
            "Epoch [49] Complete | Avg Flow Loss: 1540.4571 | Avg VAE Loss: 3507.1581\n",
            "\n",
            "Epoch [50] Complete | Avg Flow Loss: 1540.4572 | Avg VAE Loss: 3507.1804\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_vae_losses = []\n",
        "train_flow_losses = []\n",
        "epoch_numbers = []\n",
        "\n",
        "# Training loop\n",
        "model = CombinedModel(\n",
        "    input_dim=input_dim,\n",
        "    hidden_dim=256,\n",
        "    latent_dim=latent_dim,\n",
        "    num_flow_layers=4\n",
        ").to(device)\n",
        "\n",
        "flow_optimizer = optim.Adam(model.flow.parameters(), lr=0.0001)\n",
        "vae_optimizer = optim.Adam(model.vae.parameters(), lr=0.0001)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    epoch_flow_loss = 0\n",
        "    epoch_vae_loss = 0\n",
        "    num_batches = len(train_loader)\n",
        "    #print(f'Epoch#: {epoch}')\n",
        "\n",
        "    for batch_idx, data in enumerate(train_loader):\n",
        "        flow_loss, vae_loss = train_step(\n",
        "            model, data, flow_optimizer, vae_optimizer, device\n",
        "        )\n",
        "\n",
        "        epoch_flow_loss += flow_loss\n",
        "        epoch_vae_loss += vae_loss\n",
        "\n",
        "        # if (batch_idx + 1) % 100 == 0:\n",
        "        #     print(f'Batch [{batch_idx + 1}/{len(train_loader)}] | '\n",
        "        #           f'Flow Loss: {flow_loss:.4f} | '\n",
        "        #           f'VAE Loss: {vae_loss:.4f}')\n",
        "    avg_flow_loss = epoch_flow_loss / num_batches\n",
        "    avg_vae_loss = epoch_vae_loss / num_batches\n",
        "\n",
        "    # Store losses for plotting\n",
        "    train_flow_losses.append(avg_flow_loss)\n",
        "    train_vae_losses.append(avg_vae_loss)\n",
        "    epoch_numbers.append(epoch + 1)\n",
        "    \n",
        "    print(f'Epoch [{epoch + 1}] Complete | '\n",
        "          f'Avg Flow Loss: {avg_flow_loss:.4f} | '\n",
        "          f'Avg VAE Loss: {avg_vae_loss:.4f}\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot the losses\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epoch_numbers, train_vae_losses, label='VAE Loss', color='blue')\n",
        "plt.plot(epoch_numbers, train_flow_losses, label='Flow Loss', color='red')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Losses Over Time')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Save the plot if desired\n",
        "plt.savefig('training_losses.png')\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "esPmFb-V0m5_"
      },
      "outputs": [],
      "source": [
        "torch.save(model, 'combined_model_entire.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yQZNoi2RfzG",
        "outputId": "abc9ca0d-3749-4989-9e2c-4e6b6ea3e813"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[03:39:26] SMILES Parse Error: unclosed ring for input: 'SNS32on1c5[nH]Ncsn13c[H]4Cl[H][nH]N'\n",
            "[03:39:26] SMILES Parse Error: syntax error while parsing: (42C13s=\n",
            "[03:39:26] SMILES Parse Error: check for mistakes around position 1:\n",
            "[03:39:26] (42C13s=\n",
            "[03:39:26] ^\n",
            "[03:39:26] SMILES Parse Error: Failed parsing SMILES '(42C13s=' for input: '(42C13s='\n",
            "[03:39:26] SMILES Parse Error: extra close parentheses while parsing: OOo2oNs3nBrC35FO1NF3O566)=S[nH]cClS1Cl[H]s55c)4c[nH]S3ClOF)s6ClO3O\n",
            "[03:39:26] SMILES Parse Error: check for mistakes around position 25:\n",
            "[03:39:26] oNs3nBrC35FO1NF3O566)=S[nH]cClS1Cl[H]s55c\n",
            "[03:39:26] ~~~~~~~~~~~~~~~~~~~~^\n",
            "[03:39:26] SMILES Parse Error: Failed parsing SMILES 'OOo2oNs3nBrC35FO1NF3O566)=S[nH]cClS1Cl[H]s55c)4c[nH]S3ClOF)s6ClO3O' for input: 'OOo2oNs3nBrC35FO1NF3O566)=S[nH]cClS1Cl[H]s55c)4c[nH]S3ClOF)s6ClO3O'\n",
            "[03:39:26] SMILES Parse Error: extra close parentheses while parsing: FncoBro)3O41CSClCcnC4)1ON1\n",
            "[03:39:26] SMILES Parse Error: check for mistakes around position 8:\n",
            "[03:39:26] FncoBro)3O41CSClCcnC4)1ON1\n",
            "[03:39:26] ~~~~~~~^\n",
            "[03:39:26] SMILES Parse Error: Failed parsing SMILES 'FncoBro)3O41CSClCcnC4)1ON1' for input: 'FncoBro)3O41CSClCcnC4)1ON1'\n",
            "[03:39:26] SMILES Parse Error: extra open parentheses while parsing: S5(Cl[nH]\n",
            "[03:39:26] SMILES Parse Error: check for mistakes around position 3:\n",
            "[03:39:26] S5(Cl[nH]\n",
            "[03:39:26] ~~^\n",
            "[03:39:26] SMILES Parse Error: Failed parsing SMILES 'S5(Cl[nH]' for input: 'S5(Cl[nH]'\n",
            "[03:39:26] SMILES Parse Error: unclosed ring for input: '[nH]ocC6'\n",
            "[03:39:26] SMILES Parse Error: syntax error while parsing: N6(6OCl=4OCl[H]o=ccsS[nH](5[H]5S[H]S566O6[nH]5S[nH]=O\n",
            "[03:39:26] SMILES Parse Error: check for mistakes around position 4:\n",
            "[03:39:26] N6(6OCl=4OCl[H]o=ccsS[nH](5[H]5S[H]S566O6\n",
            "[03:39:26] ~~~^\n",
            "[03:39:26] SMILES Parse Error: Failed parsing SMILES 'N6(6OCl=4OCl[H]o=ccsS[nH](5[H]5S[H]S566O6[nH]5S[nH]=O' for input: 'N6(6OCl=4OCl[H]o=ccsS[nH](5[H]5S[H]S566O6[nH]5S[nH]=O'\n",
            "[03:39:27] SMILES Parse Error: syntax error while parsing: So156nC(no6oNnsNFCooONOCcBr62c5ClN1Oo5o1Br[H]Cl)S[H]c==[nH]\n",
            "[03:39:27] SMILES Parse Error: check for mistakes around position 55:\n",
            "[03:39:27] 1Oo5o1Br[H]Cl)S[H]c==[nH]\n",
            "[03:39:27] ~~~~~~~~~~~~~~~~~~~~^\n",
            "[03:39:27] SMILES Parse Error: Failed parsing SMILES 'So156nC(no6oNnsNFCooONOCcBr62c5ClN1Oo5o1Br[H]Cl)S[H]c==[nH]' for input: 'So156nC(no6oNnsNFCooONOCcBr62c5ClN1Oo5o1Br[H]Cl)S[H]c==[nH]'\n",
            "[03:39:27] SMILES Parse Error: extra close parentheses while parsing: C1)oSO3C[H]o([nH]4)oBrnBr2OOSS)C1))35Brs1)soN)[nH]Cl[nH]\n",
            "[03:39:27] SMILES Parse Error: check for mistakes around position 3:\n",
            "[03:39:27] C1)oSO3C[H]o([nH]4)oBrnBr2OOSS)C1))35Brs1\n",
            "[03:39:27] ~~^\n",
            "[03:39:27] SMILES Parse Error: Failed parsing SMILES 'C1)oSO3C[H]o([nH]4)oBrnBr2OOSS)C1))35Brs1)soN)[nH]Cl[nH]' for input: 'C1)oSO3C[H]o([nH]4)oBrnBr2OOSS)C1))35Brs1)soN)[nH]Cl[nH]'\n",
            "[03:39:27] SMILES Parse Error: syntax error while parsing: (NssClFsF[H]ClF2F5NClCcs)cF==3c2S1Oc[H]S\n",
            "[03:39:27] SMILES Parse Error: check for mistakes around position 1:\n",
            "[03:39:27] (NssClFsF[H]ClF2F5NClCcs)cF==3c2S1Oc[H]S\n",
            "[03:39:27] ^\n",
            "[03:39:27] SMILES Parse Error: Failed parsing SMILES '(NssClFsF[H]ClF2F5NClCcs)cF==3c2S1Oc[H]S' for input: '(NssClFsF[H]ClF2F5NClCcs)cF==3c2S1Oc[H]S'\n"
          ]
        }
      ],
      "source": [
        "temperature = 1.0\n",
        "for i in range(10):\n",
        "  generate_smiles(model, latent_dim, idx_to_char, char_to_idx, max_length, temperature)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
