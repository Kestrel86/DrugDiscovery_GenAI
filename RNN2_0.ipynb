{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "RNN model testing"
      ],
      "metadata": {
        "id": "rhf0zVh-psv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdkit-pypi -qqq\n",
        "!git clone https://github.com/molecularsets/moses.git\n"
      ],
      "metadata": {
        "id": "rTjrUH15NieO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2b4eae3-73ee-4218-d2de-7af9bd958d2f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCloning into 'moses'...\n",
            "remote: Enumerating objects: 1957, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 1957 (delta 0), reused 2 (delta 0), pack-reused 1953 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1957/1957), 164.05 MiB | 14.59 MiB/s, done.\n",
            "Resolving deltas: 100% (1068/1068), done.\n",
            "Filtering content: 100% (68/68), 323.72 MiB | 51.49 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: open train.csv\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('moses/data/train.csv')\n",
        "print(df.head())\n",
        "print(df.shape)"
      ],
      "metadata": {
        "id": "s5wTTBW2PqNC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a2b7a1d-30fa-4f62-f003-be0b5b94b549"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                   SMILES  SPLIT\n",
            "0  CCCS(=O)c1ccc2[nH]c(=NC(=O)OC)[nH]c2c1  train\n",
            "1    CC(C)(C)C(=O)C(Oc1ccc(Cl)cc1)n1ccnc1  train\n",
            "2     Cc1c(Cl)cccc1Nc1ncccc1C(=O)OCC(O)CO  train\n",
            "3        Cn1cnc2c1c(=O)n(CC(O)CO)c(=O)n2C  train\n",
            "4          CC1Oc2ccc(Cl)cc2N(CC(O)CO)C1=O  train\n",
            "(1584663, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "JwVP3ljmt0wL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "58gwzbS7tfh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r4vpjZisttIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5DWCnByF-dDE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7823bea7-b2aa-4c00-8053-2391f2c754ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COC(=O)C1=C(C)NC(=O)NC1c1cc(C(F)(F)F)ccc1OC)c1nncn1CC)c1cccnc1C1CC1C)C(C)CC#N)C(C)C)C1CC1COCC1CC1)c1c\n",
            "CCOc1ccccc1OC(=O)Cc1ccon1)c1cnn2cccnc12)c1ccco1)C1CC1C)C1CC1C)C1CC1CC1=O)CC1(C)CCOCC1)C1CC1OCCO1C)C(C\n",
            "CCOc1ccccc1NS(=O)(=O)c1cc(C)sc1C(=O)OC)C(C)C1=O)c1ccccc1OCC(N)=O)c1cccnc1NC(C)=O)C(C)C)C(C)C)c1ccco1)\n",
            "COc1ccc(OC)c(Cc2nc3cccc(C(=O)OC)c3[nH]2)c1OC(F)F)C(N)=O)c1ccccc1F)C1CC1CC1CC1CC1=O)CC1(C)Cl)c1nc[nH]n\n",
            "CCn1cc(S(=O)(=O)N2CCCCC2)cc1C(=O)OCC#CCN1CCOCC1C1CC1)C(N)=O)c1ccccc1F)C(C)CC#N)C(C)(C)C)C(C)CC#N)C(C)\n",
            "C(#CCC(=O)N1CCCC(C(F)(F)F)C1)c1ccccc1F)C1CCC1(=O)NC1CC1)C1CC1CC1=O)c1ccncc1F)C1CC1C1CC1(C)CC1F)C(=O)O\n",
            "COc1cc(CNC(=O)Nc2cnn(CC3CCCO3)c2)ccn1C1CC1CC1CC1)C(N)=O)c1ccccc1F)C1CC1C1CC1)C(C)C)c1nscc1C#N)C(N)=O)\n",
            "COc1ccc(NC(=O)NCc2cc(C3CC3)cn2C)cn1)c1ccccc1F)c1ccncc1F)C1CC1C)C1CC1(C)OC(C)(C)O1)C1CC1CC1C)C1(C)CC1(\n",
            "CCCCc1ccc(C2CCCN2C(=O)C(C)NS(C)(=O)=O)cc1OCC(C)CC)c1COC(C)=O)c1ccccc1Cl)c1cccnc1OCCCC#N)C(C)C#N)C1CC1\n",
            "COC(=O)CC1CCCCN1C(=O)Nc1cccnc1-n1cccn1)c1ccccc1F)C1CC1C1CC1)c1ccncc1Cl)c1ccncc1F)C1CC1C1CC1)C(=O)OC)c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ED94Tup20151"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wTpKo4Zt1OaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k7OZkfVP1e3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from rdkit import Chem, RDLogger\n",
        "RDLogger.DisableLog('rdApp.*')  # Suppress RDKit warnings\n",
        "\n",
        "# Function to add start and end tokens\n",
        "def process_smiles(smiles_list):\n",
        "    return [\"^\" + s + \"$\" for s in smiles_list]\n",
        "\n",
        "# Create character dictionaries including special tokens\n",
        "def create_vocab(smiles_list):\n",
        "    all_chars = sorted(list(set(''.join(smiles_list))))\n",
        "    char2idx = {ch: i + 1 for i, ch in enumerate(all_chars)}\n",
        "    char2idx['<PAD>'] = 0  # Padding token\n",
        "    idx2char = {i: ch for ch, i in char2idx.items()}\n",
        "    return char2idx, idx2char, len(char2idx)\n",
        "\n",
        "# Enhanced tokenization\n",
        "def tokenize(smiles, char2idx):\n",
        "    return [char2idx.get(c, 0) for c in smiles]  # Default to 0 if unknown\n",
        "\n",
        "def detokenize(tokens, idx2char):\n",
        "    return ''.join([idx2char.get(t, '') for t in tokens if t != 0])\n",
        "\n",
        "# Improved dataset class\n",
        "class SMILESDataset(Dataset):\n",
        "    def __init__(self, smiles_list, char2idx, seq_len=120):\n",
        "        self.smiles_list = smiles_list\n",
        "        self.char2idx = char2idx\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.smiles_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        smiles = self.smiles_list[idx]\n",
        "        if len(smiles) > self.seq_len:\n",
        "            smiles = smiles[:self.seq_len]\n",
        "\n",
        "        tokens = tokenize(smiles, self.char2idx)\n",
        "        x = torch.tensor(tokens[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(tokens[1:], dtype=torch.long)\n",
        "\n",
        "        # Padding\n",
        "        pad_len = self.seq_len - 1 - len(x)\n",
        "        if pad_len > 0:\n",
        "            x = torch.cat([x, torch.zeros(pad_len, dtype=torch.long)])\n",
        "            y = torch.cat([y, torch.zeros(pad_len, dtype=torch.long)])\n",
        "\n",
        "        return x, y\n",
        "\n",
        "# Enhanced RNN model\n",
        "class RNNGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=256, hidden_dim=512, num_layers=3, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.rnn = nn.LSTM(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        batch_size = x.size(0)\n",
        "        emb = self.dropout(self.embedding(x))\n",
        "\n",
        "        if hidden is None:\n",
        "            # Initialize hidden states\n",
        "            h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n",
        "            c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(x.device)\n",
        "            hidden = (h0, c0)\n",
        "\n",
        "        out, hidden = self.rnn(emb, hidden)\n",
        "        out = self.dropout(out)\n",
        "        logits = self.fc(out)\n",
        "        return logits, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "        return (h0, c0)\n",
        "\n",
        "# Sample a new molecule\n",
        "def generate_molecule(model, char2idx, idx2char, device, max_len=100, temperature=0.8):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Start with the start token\n",
        "        start_token = char2idx['^']\n",
        "        current = torch.tensor([[start_token]], dtype=torch.long).to(device)\n",
        "        hidden = None\n",
        "        result = [start_token]\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            output, hidden = model(current, hidden)\n",
        "            output = output[:, -1, :] / temperature\n",
        "            probs = torch.softmax(output, dim=-1)\n",
        "\n",
        "            # Sample from the probability distribution\n",
        "            next_token = torch.multinomial(probs, 1).item()\n",
        "            result.append(next_token)\n",
        "            current = torch.tensor([[next_token]], dtype=torch.long).to(device)\n",
        "\n",
        "            # Stop if end token is generated\n",
        "            if idx2char[next_token] == '$':\n",
        "                break\n",
        "\n",
        "        generated = detokenize(result, idx2char)\n",
        "        # Remove start/end tokens for validation\n",
        "        clean_smiles = generated.replace('^', '').replace('$', '')\n",
        "        return clean_smiles, is_valid_smiles(clean_smiles)\n",
        "\n",
        "# Validate SMILES with RDKit\n",
        "def is_valid_smiles(smiles):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    return mol is not None\n",
        "\n",
        "# Training with teacher forcing and validation\n",
        "def train_model(model, dataloader, optimizer, criterion, device, epochs,\n",
        "                char2idx, idx2char, save_path='smiles_rnn_model.pth'):\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for x, y in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits, _ = model(x)\n",
        "            loss = criterion(logits.view(-1, len(char2idx)), y.view(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)  # Gradient clipping\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg_loss = epoch_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1} loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Generate and validate some molecules\n",
        "        if (epoch + 1) % 5 == 0 or epoch == epochs - 1:\n",
        "            valid_count = 0\n",
        "            n_samples = 10\n",
        "            print(\"\\nGenerating sample molecules:\")\n",
        "            for _ in range(n_samples):\n",
        "                mol, valid = generate_molecule(model, char2idx, idx2char, device)\n",
        "                validity = \"✓\" if valid else \"✗\"\n",
        "                print(f\"{mol} {validity}\")\n",
        "                if valid:\n",
        "                    valid_count += 1\n",
        "            print(f\"Validity: {valid_count}/{n_samples} ({valid_count/n_samples*100:.1f}%)\")\n",
        "\n",
        "        # Save best model\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"Model saved to {save_path}\")\n",
        "\n",
        "# Main training pipeline\n",
        "def run_training(dataframe, smiles_column='SMILES', batch_size=128, epochs=20):\n",
        "    # Get SMILES strings from DataFrame\n",
        "    smiles_list = dataframe[smiles_column].tolist()\n",
        "\n",
        "    # Process data\n",
        "    processed_smiles = process_smiles(smiles_list)\n",
        "    char2idx, idx2char, vocab_size = create_vocab(processed_smiles)\n",
        "\n",
        "    # Create dataset and data loader\n",
        "    dataset = SMILESDataset(processed_smiles, char2idx, seq_len=120)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Setup model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = RNNGenerator(vocab_size).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "    # Train\n",
        "    train_model(model, loader, optimizer, criterion, device, epochs=epochs,\n",
        "                char2idx=char2idx, idx2char=idx2char)\n",
        "\n",
        "    return model, char2idx, idx2char\n",
        "\n",
        "# Generate a batch of molecules\n",
        "def generate_molecules(model, char2idx, idx2char, device, n=25, temperature=0.8):\n",
        "    valid_mols = []\n",
        "    attempts = 0\n",
        "    max_attempts = n * 5  # Try up to 5x the requested number\n",
        "\n",
        "    while len(valid_mols) < n and attempts < max_attempts:\n",
        "        smiles, valid = generate_molecule(model, char2idx, idx2char, device, temperature=temperature)\n",
        "        attempts += 1\n",
        "        if valid:\n",
        "            # Check for duplicates\n",
        "            if smiles not in valid_mols:\n",
        "                valid_mols.append(smiles)\n",
        "\n",
        "    return valid_mols, len(valid_mols)/attempts if attempts > 0 else 0"
      ],
      "metadata": {
        "id": "1ydRfMAmLxU5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution script\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    df = pd.read_csv('moses/data/train.csv')\n",
        "    print(\"Dataset information:\")\n",
        "    print(df.head())\n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "\n",
        "    # Check if 'SMILES' column exists, adjust if needed\n",
        "    smiles_column = 'SMILES'\n",
        "    if smiles_column not in df.columns:\n",
        "        # Try to find a column that might contain SMILES strings\n",
        "        for col in df.columns:\n",
        "            if any(c in '()[]=' for c in df[col].iloc[0]):\n",
        "                smiles_column = col\n",
        "                print(f\"Using column '{smiles_column}' for SMILES data\")\n",
        "                break\n",
        "\n",
        "    # Train the model\n",
        "    print(f\"\\nTraining model on {df.shape[0]} SMILES strings...\")\n",
        "    model, char2idx, idx2char = run_training(df, smiles_column=smiles_column)\n",
        "\n",
        "    # Generate and evaluate new molecules\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(\"\\nGenerating new molecules...\")\n",
        "    molecules, validity_rate = generate_molecules(model, char2idx, idx2char, device, n=20)\n",
        "\n",
        "    print(f\"\\nGenerated {len(molecules)} valid molecules with {validity_rate*100:.1f}% validity rate\")\n",
        "    print(\"\\nSample valid molecules:\")\n",
        "    for i, mol in enumerate(molecules[:10], 1):\n",
        "        print(f\"{i}. {mol}\")\n",
        "\n",
        "    # Optional: Save to CSV\n",
        "    if molecules:\n",
        "        output_df = pd.DataFrame({'generated_smiles': molecules})\n",
        "        output_path = 'generated_molecules.csv'\n",
        "        output_df.to_csv(output_path, index=False)\n",
        "        print(f\"\\nSaved generated molecules to {output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "cTEoc-mgMH55",
        "outputId": "7994e3de-5303-4bff-ded6-39653609e15b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset information:\n",
            "                                   SMILES  SPLIT\n",
            "0  CCCS(=O)c1ccc2[nH]c(=NC(=O)OC)[nH]c2c1  train\n",
            "1    CC(C)(C)C(=O)C(Oc1ccc(Cl)cc1)n1ccnc1  train\n",
            "2     Cc1c(Cl)cccc1Nc1ncccc1C(=O)OCC(O)CO  train\n",
            "3        Cn1cnc2c1c(=O)n(CC(O)CO)c(=O)n2C  train\n",
            "4          CC1Oc2ccc(Cl)cc2N(CC(O)CO)C1=O  train\n",
            "Dataset shape: (1584663, 2)\n",
            "\n",
            "Training model on 1584663 SMILES strings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:  12%|█▏        | 1463/12381 [00:59<07:22, 24.65it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-5b78ec6140df>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nTraining model on {df.shape[0]} SMILES strings...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx2char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmiles_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msmiles_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Generate and evaluate new molecules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-ed9ef81583f2>\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(dataframe, smiles_column, batch_size, epochs)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     train_model(model, loader, optimizer, criterion, device, epochs=epochs,\n\u001b[0m\u001b[1;32m    193\u001b[0m                 char2idx=char2idx, idx2char=idx2char)\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-ed9ef81583f2>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, optimizer, criterion, device, epochs, char2idx, idx2char, save_path)\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar2idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}