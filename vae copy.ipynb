{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "e53c5634",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "e53c5634",
        "outputId": "61bc9203-bde6-4269-978f-c60c57cb1128"
      },
      "outputs": [],
      "source": [
        "# !pip install rdkit-pypi\n",
        "# !git clone https://github.com/molecularsets/moses.git\n",
        "\n",
        "import matplotlib as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from rdkit import Chem\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "1c227beb",
      "metadata": {
        "id": "1c227beb"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def load_smiles_from_csv(path, split_type='train'):\n",
        "    '''\n",
        "    Loads SMILES strings from a CSV file.\n",
        "\n",
        "    Args:\n",
        "        path (str): Path to the CSV file\n",
        "        split_type (str): Split type ('train' or 'test')\n",
        "\n",
        "    Returns:\n",
        "        list: List of SMILES strings\n",
        "    '''\n",
        "    smiles = []\n",
        "    with open(path, 'r') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            if row['SPLIT'].strip().lower() == split_type:\n",
        "                smiles.append(row['SMILES'].strip())\n",
        "    return smiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "QFYVVczcKLoV",
      "metadata": {
        "id": "QFYVVczcKLoV"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Functions are from the RNN model we have but not entirely sure where they would fit in the VAE\n",
        "Currently working on implementation, the process_smiles function may help in creating valid molecules\n",
        "'''\n",
        "\n",
        "# Function to add start and end tokens\n",
        "def process_smiles(smiles_list):\n",
        "    return [\"^\" + s + \"$\" for s in smiles_list]\n",
        "\n",
        "# Create character dictionaries including special tokens\n",
        "def create_vocab(smiles_list):\n",
        "    all_chars = sorted(list(set(''.join(smiles_list))))\n",
        "    char2idx = {ch: i + 1 for i, ch in enumerate(all_chars)}\n",
        "    char2idx[''] = 0  # Padding token\n",
        "    idx2char = {i: ch for ch, i in char2idx.items()}\n",
        "    return char2idx, idx2char, len(char2idx)\n",
        "\n",
        "# Enhanced tokenization\n",
        "def tokenize(smiles, char2idx):\n",
        "    return [char2idx.get(c, 0) for c in smiles]  # Default to 0 if unknown\n",
        "\n",
        "def detokenize(tokens, idx2char):\n",
        "    return ''.join([idx2char.get(t, '') for t in tokens if t != 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "2bd0c0a6",
      "metadata": {
        "id": "2bd0c0a6"
      },
      "outputs": [],
      "source": [
        "def extract_unique_chars(smiles_list):\n",
        "    '''\n",
        "    Extracts unique characters from a list of SMILES strings.\n",
        "\n",
        "    Args:\n",
        "        smiles_list (list): List of SMILES strings\n",
        "\n",
        "    Returns:\n",
        "        list: List of unique characters\n",
        "    '''\n",
        "    unique_chars = set()\n",
        "    for smiles in smiles_list:\n",
        "        unique_chars.update(smiles.strip())\n",
        "    return sorted(unique_chars)\n",
        "\n",
        "def clean_smiles(smiles):\n",
        "    '''\n",
        "    Cleans a SMILES string by removing unwanted characters.\n",
        "\n",
        "    Args:\n",
        "        smiles (str): SMILES string\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned SMILES string\n",
        "    '''\n",
        "    # Remove unwanted metadata and special characters\n",
        "    cleaned = smiles.split(',')[0].strip()\n",
        "    cleaned = cleaned.replace('#', '')  # Remove '#' characters\n",
        "    cleaned = cleaned.replace('$', '')  # Remove '$' characters\n",
        "    cleaned = cleaned.replace('^', '')  # Remove '^' characters if present\n",
        "    return cleaned\n",
        "\n",
        "def decode_smiles(one_hot_tensor, idx_to_char):\n",
        "    '''\n",
        "    Decodes a one-hot encoded tensor back to SMILES.\n",
        "\n",
        "    Args:\n",
        "        one_hot_tensor (torch.Tensor): One-hot encoded tensor\n",
        "    '''\n",
        "    smiles = ''\n",
        "    one_hot_tensor = one_hot_tensor.view(-1, len(idx_to_char))  # unflatten\n",
        "    for row in one_hot_tensor:\n",
        "        idx = row.argmax().item()\n",
        "        smiles += idx_to_char[idx]\n",
        "    return smiles.strip()\n",
        "\n",
        "def verify_smiles(smiles):\n",
        "  '''\n",
        "  Verifies the validity of a SMILES string using RDKit.\n",
        "\n",
        "  Args:\n",
        "      smiles (str): SMILES string to verify\n",
        "\n",
        "  Returns:\n",
        "      bool: True if valid, False otherwise\n",
        "  '''\n",
        "  mol = Chem.MolFromSmiles(smiles)\n",
        "  return mol is not None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "2dd695f2",
      "metadata": {
        "id": "2dd695f2"
      },
      "outputs": [],
      "source": [
        "# Generate a new molecule from VAE by sampling from the latent space\n",
        "def generate_smiles(model, latent_dim, idx_to_char, max_length, vocab_size, temperature=1.0):\n",
        "    '''\n",
        "    Generates a new SMILES string by sampling from the VAE's latent space.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): VAE model\n",
        "        latent_dim (int): Dimension of the latent space\n",
        "    '''\n",
        "    z = torch.randn(1, latent_dim).to(model.fc1.weight.device)  # Ensure z is on the same device as the model\n",
        "    with torch.no_grad():\n",
        "        generated = model.decode(z)\n",
        "\n",
        "    # Add postprocessing to convert to SMILES\n",
        "    probs = F.softmax(generated.view(max_length, vocab_size) / temperature, dim=-1)\n",
        "\n",
        "    # Sample the next character from the probability distribution\n",
        "    generated_tokens_indices = torch.multinomial(probs, 1).cpu().numpy().flatten()\n",
        "\n",
        "    # Iterate through indices to build the SMILES string\n",
        "    generated_smiles = \"\".join([idx_to_char.get(i, \"\") for i in generated_tokens_indices])\n",
        "    generated_smiles = generated_smiles.replace('^', '').replace('$', '')\n",
        "\n",
        "    # Verification using rdkit\n",
        "    is_valid = verify_smiles(generated_smiles)\n",
        "    if is_valid:\n",
        "      return generated_smiles\n",
        "    else:\n",
        "      return \"INVALID\"\n",
        "\n",
        "    # return generated_smiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "062ff3ba",
      "metadata": {
        "id": "062ff3ba"
      },
      "outputs": [],
      "source": [
        "# Dataset class for SMILES strings\n",
        "\n",
        "# Contemplate Protein To Vector Encoding\n",
        "class SMILESDataset(Dataset):\n",
        "    def __init__(self, smiles_list, max_length=150, char_to_idx=None):\n",
        "        '''\n",
        "        Initializes the SMILESDataset with a list of SMILES strings.\n",
        "\n",
        "        Args:\n",
        "            smiles_list (list): List of SMILES strings\n",
        "            max_length (int): Maximum length of the SMILES strings\n",
        "            char_to_idx (dict): Character-to-index mapping\n",
        "\n",
        "        The dataset will one-hot encode each character in a SMILES string to a fixed-size tensor of shape (max_length * vocab_size).\n",
        "        If a SMILES string is shorter than max_length, it will be padded with zeros. If longer, it will be truncated.\n",
        "        '''\n",
        "        self.smiles_list = smiles_list\n",
        "        self.max_length = max_length\n",
        "\n",
        "        if char_to_idx is None:\n",
        "            raise ValueError(\"Please provide a fixed character-to-index mapping\")\n",
        "            # self.char_to_idx, self.idx_to_char = build_vocabulary(smiles_list)\n",
        "        else:\n",
        "            self.char_to_idx = char_to_idx\n",
        "            self.idx_to_char = {v: k for k, v in char_to_idx.items()}\n",
        "\n",
        "        self.vocab_size = len(self.char_to_idx)\n",
        "\n",
        "        original_count = len(smiles_list)\n",
        "        filtered = []\n",
        "        invalid_count = 0\n",
        "\n",
        "        for s in smiles_list:\n",
        "            s = s.strip()\n",
        "            if all(c in self.char_to_idx for c in s):\n",
        "                filtered.append(s)\n",
        "            else:\n",
        "                invalid_count += 1\n",
        "        print(f\"Total: {original_count}, Valid: {len(filtered)}, Invalid: {invalid_count}\")\n",
        "        self.smiles_list = filtered\n",
        "\n",
        "    def __len__(self):\n",
        "        '''\n",
        "        Returns:\n",
        "            int: Number of valid SMILES strings in the dataset\n",
        "        '''\n",
        "\n",
        "        return len(self.smiles_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        '''\n",
        "        Fetches the encoded version of a SMILES string at a given index.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the SMILES string to retrieve\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: One-hot encoded tensor of the SMILES string of shape (max_length * vocab_size)\n",
        "        '''\n",
        "\n",
        "        smiles = self.smiles_list[idx]\n",
        "\n",
        "        # One-hot encode the SMILES string\n",
        "        encoded = torch.zeros(self.max_length, self.vocab_size)\n",
        "        for i, char in enumerate(smiles[:self.max_length]):\n",
        "            encoded[i, self.char_to_idx[char]] = 1.0\n",
        "\n",
        "        return encoded.view(-1) #Flatten into 1D tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "oN5phZo8tZC1",
      "metadata": {
        "id": "oN5phZo8tZC1"
      },
      "outputs": [],
      "source": [
        "class AffineCouplingLayer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        if input_dim % 2 != 0:\n",
        "            input_dim += 1\n",
        "        \n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim // 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, input_dim // 2 * 2)\n",
        "        )\n",
        "        \n",
        "        # Initialize last layer with zeros for stable training\n",
        "        nn.init.zeros_(self.net[-1].weight)\n",
        "        nn.init.zeros_(self.net[-1].bias)\n",
        "\n",
        "    def forward(self, x, reverse=False):\n",
        "        x1, x2 = x.chunk(2, dim=1)\n",
        "        \n",
        "        # Get scaling and translation factors\n",
        "        st = self.net(x1)\n",
        "        s, t = st.chunk(2, dim=1)\n",
        "        \n",
        "        # Apply scaling with numerical stability\n",
        "        scale_factor = 0.001\n",
        "        s = torch.tanh(s) * scale_factor\n",
        "        \n",
        "        # Compute log determinant (only from the scaling part)\n",
        "        log_det = torch.sum(s, dim=1)\n",
        "        \n",
        "        if reverse:\n",
        "            # Inverse transformation\n",
        "            x2 = (x2 - t) * torch.exp(-s)\n",
        "            return torch.cat([x1, x2], dim=1), -log_det\n",
        "        else:\n",
        "            # Forward transformation\n",
        "            x2 = x2 * torch.exp(s) + t\n",
        "            return torch.cat([x1, x2], dim=1), log_det\n",
        "\n",
        "class Flow(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim + (input_dim % 2)\n",
        "        \n",
        "        # Coupling layers without batch norm\n",
        "        self.layers = nn.ModuleList([\n",
        "            AffineCouplingLayer(self.input_dim, hidden_dim)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        \n",
        "        # Layer normalization instead of batch norm\n",
        "        self.norms = nn.ModuleList([\n",
        "            nn.LayerNorm(self.input_dim)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, reverse=False):\n",
        "        # Initialize log determinant\n",
        "        log_det_total = torch.zeros(x.size(0), device=x.device)\n",
        "        \n",
        "        # Handle odd dimensions\n",
        "        if x.size(1) % 2 != 0:\n",
        "            x = F.pad(x, (0, 1), 'constant', 0)\n",
        "        \n",
        "        # Process through layers\n",
        "        if reverse:\n",
        "            for layer in reversed(self.layers):\n",
        "                x, log_det = layer(x, reverse=True)\n",
        "                log_det_total = log_det_total + log_det\n",
        "        else:\n",
        "            for layer in self.layers:\n",
        "                x, log_det = layer(x, reverse=False)\n",
        "                log_det_total = log_det_total + log_det\n",
        "        \n",
        "        return x, log_det_total\n",
        "\n",
        "    def get_latent(self, x):\n",
        "        \"\"\"Generate latent representation\"\"\"\n",
        "        z, _ = self.forward(x)\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "3HyjMRPVDcTs",
      "metadata": {
        "id": "3HyjMRPVDcTs"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim, vocab_size):\n",
        "        super(VAE, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.fc1 = nn.Linear(input_dim, 256)\n",
        "        self.fc_mu = nn.Linear(256, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(256, latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.fc3 = nn.Linear(latent_dim, 256)\n",
        "        self.fc4 = nn.Linear(256, input_dim)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return self.fc_mu(h1), self.fc_logvar(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, self.input_dim))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "\n",
        "        recon_x = self.decode(z)\n",
        "        return recon_x, mu, logvar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "a5781a3e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_vae_loss(recon_x, x, mu, logvar, dataset):\n",
        "    \"\"\"\n",
        "    Compute VAE loss with dataset-specific vocabulary size.\n",
        "    \n",
        "    Args:\n",
        "        recon_x: Reconstructed input from VAE\n",
        "        x: Original input data\n",
        "        mu: Mean from VAE encoder\n",
        "        logvar: Log variance from VAE encoder\n",
        "        dataset: Dataset object containing vocab_size and max_length\n",
        "    \"\"\"\n",
        "    batch_size = x.size(0)\n",
        "    vocab_size = dataset.vocab_size\n",
        "    seq_len = dataset.max_length\n",
        "    \n",
        "    # Reshape tensors to match dataset dimensions\n",
        "    x = x.view(batch_size, seq_len * vocab_size)\n",
        "    recon_x = recon_x.view(batch_size, seq_len * vocab_size)\n",
        "    \n",
        "    # Reconstruction loss (BCE)\n",
        "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "    \n",
        "    # KL divergence\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    \n",
        "    # Weight the KLD term\n",
        "    beta = 0.1  # Adjust this weight to balance reconstruction vs. KLD\n",
        "    \n",
        "    return BCE + beta * KLD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "bb2c250f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb2c250f",
        "outputId": "14a29115-105f-4579-bbfb-d7a2758dcfe6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total unique characters: 25\n",
            "Unique characters in dataset:\n",
            "['(', ')', '-', '1', '2', '3', '4', '5', '6', '=', 'B', 'C', 'F', 'H', 'N', 'O', 'S', '[', ']', 'c', 'l', 'n', 'o', 'r', 's']\n",
            "Total: 1584663, Valid: 1584663, Invalid: 0\n",
            "Total: 176074, Valid: 176074, Invalid: 0\n",
            "Training Vocabulary Size: 25\n",
            "Test Vocabulary Size: 25\n",
            "# Train SMILES after filtering: 1584663\n",
            "# Test SMILES after filtering: 176074\n",
            "Number of batches in train_loader: 198083\n",
            "Number of batches in test_loader: 22010\n"
          ]
        }
      ],
      "source": [
        "# Load SMILES strings\n",
        "# with open('dataset/train.txt', 'r') as f:\n",
        "#     smiles_train = [line.strip() for line in f][1:]\n",
        "\n",
        "# with open('dataset/test.txt', 'r') as f:\n",
        "#     smiles_test = [line.strip() for line in f]\n",
        "\n",
        "smiles_train = load_smiles_from_csv('dataset/train.txt', split_type='train')\n",
        "smiles_test = load_smiles_from_csv('dataset/test.txt', split_type='test')  # if test rows are in same file\n",
        "\n",
        "# Apply cleaning to your SMILES\n",
        "smiles_train = [clean_smiles(smiles) for smiles in smiles_train]\n",
        "smiles_test = [clean_smiles(smiles) for smiles in smiles_test]\n",
        "# smiles_train = process_smiles(smiles_train)\n",
        "# smiles_test = process_smiles(smiles_test)\n",
        "# smiles_train = smiles_train[:10000]\n",
        "# smiles_test = smiles_test[:10000]\n",
        "\n",
        "# print(f\"Raw SMILES loaded: train={len(smiles_train)}, test={len(smiles_test)}\") # output for testing purposes\n",
        "all_smiles = smiles_train + smiles_test\n",
        "unique_chars = extract_unique_chars(all_smiles)\n",
        "\n",
        "print(f\"Total unique characters: {len(unique_chars)}\")\n",
        "print(\"Unique characters in dataset:\")\n",
        "print(unique_chars)\n",
        "\n",
        "# Use extracted unique characters to rebuild vocabulary\n",
        "VALID_CHARS = unique_chars\n",
        "char_to_idx = {c: i for i, c in enumerate(VALID_CHARS)}\n",
        "idx_to_char = {i: c for c, i in char_to_idx.items()}\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SMILESDataset(smiles_train, max_length=150, char_to_idx=char_to_idx)\n",
        "test_dataset = SMILESDataset(smiles_test, max_length=150, char_to_idx=char_to_idx)\n",
        "print(\"Training Vocabulary Size:\", train_dataset.vocab_size)\n",
        "print(\"Test Vocabulary Size:\", test_dataset.vocab_size) # Should be the same\n",
        "\n",
        "\n",
        "print(f\"# Train SMILES after filtering: {len(train_dataset)}\")\n",
        "print(f\"# Test SMILES after filtering: {len(test_dataset)}\")\n",
        "# train_dataset = SMILESDataset(smiles_train)\n",
        "# test_dataset = SMILESDataset(smiles_test, char_to_idx=train_dataset.char_to_idx)  # Share vocabulary\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)  # No need to shuffle test data\n",
        "\n",
        "print(f\"Number of batches in train_loader: {len(train_loader)}\")\n",
        "print(f\"Number of batches in test_loader: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "a8590f4b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8590f4b",
        "outputId": "94bb0e95-a6c9-4808-ce3c-b270c9bb30e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "\n",
            "Sample SMILES visualizations:\n",
            "\n",
            "Sample 1\n",
            "Original : CCCS(=O)c1ccc2[nH]c(=NC(=O)OC)[nH]c2c1\n",
            "Decoded  : CCCS(=O)c1ccc2[nH]c(=NC(=O)OC)[nH]c2c1((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((\n",
            "Shape    : torch.Size([3750])\n",
            "\n",
            "Sample 2\n",
            "Original : CC(C)(C)C(=O)C(Oc1ccc(Cl)cc1)n1ccnc1\n",
            "Decoded  : CC(C)(C)C(=O)C(Oc1ccc(Cl)cc1)n1ccnc1((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((\n",
            "Shape    : torch.Size([3750])\n",
            "\n",
            "Sample 3\n",
            "Original : Cc1c(Cl)cccc1Nc1ncccc1C(=O)OCC(O)CO\n",
            "Decoded  : Cc1c(Cl)cccc1Nc1ncccc1C(=O)OCC(O)CO(((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((\n",
            "Shape    : torch.Size([3750])\n"
          ]
        }
      ],
      "source": [
        "# Check a batch of data\n",
        "for i, data in enumerate(train_loader):\n",
        "    if i == 0:  # Just visualize the first batch\n",
        "        print(data)\n",
        "        break\n",
        "\n",
        "# Visualize 3 samples\n",
        "print(\"\\nSample SMILES visualizations:\")\n",
        "for i in range(3):\n",
        "    encoded = train_dataset[i]\n",
        "    original = train_dataset.smiles_list[i]\n",
        "    decoded = decode_smiles(encoded, train_dataset.idx_to_char)\n",
        "\n",
        "    print(f\"\\nSample {i+1}\")\n",
        "    print(f\"Original : {original}\")\n",
        "    print(f\"Decoded  : {decoded}\")\n",
        "    print(f\"Shape    : {encoded.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "706c7ef0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Vocab size: 25\n",
            "max_length: 150\n",
            "Input dim: 3750\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "import torch\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device = torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Instantiate the VAE model\n",
        "input_dim = train_dataset.vocab_size * train_dataset.max_length  # Flatten the input (max_length x vocab_size)\n",
        "latent_dim = 128\n",
        "\n",
        "vocab_size = train_dataset.vocab_size\n",
        "max_length = train_dataset.max_length\n",
        "\n",
        "print(\"Vocab size:\", train_dataset.vocab_size)\n",
        "print(\"max_length:\", train_dataset.max_length)\n",
        "print(\"Input dim:\", input_dim)\n",
        "\n",
        "flow = Flow(input_dim = input_dim, hidden_dim=256, num_layers=4).to(device)\n",
        "vae = VAE(input_dim, latent_dim, len(idx_to_char)).to(device)\n",
        "\n",
        "# Optimizer\n",
        "# Separate optimizers for Flow and VAE\n",
        "flow_optimizer = torch.optim.Adam(flow.parameters(), lr=0.0001)\n",
        "vae_optimizer = torch.optim.Adam(vae.parameters(), lr=0.0001)\n",
        "\n",
        "# Training parameters\n",
        "epochs = 100\n",
        "early_stop_patience = 5\n",
        "best_loss = float('inf')\n",
        "patience_counter = 0\n",
        "min_delta = 0.001\n",
        "\n",
        "# Lists to track losses\n",
        "flow_losses = []\n",
        "vae_losses = []\n",
        "total_losses = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "da8d0204",
      "metadata": {},
      "outputs": [],
      "source": [
        "class CombinedModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim, num_flow_layers):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Initialize FLOW for creating latent representation\n",
        "        self.flow = Flow(\n",
        "            input_dim=input_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_layers=num_flow_layers\n",
        "        )\n",
        "        \n",
        "        # Initialize VAE to work with FLOW's output\n",
        "        self.vae = VAE(\n",
        "            input_dim=input_dim,  # Flow preserves dimensionality\n",
        "            latent_dim=latent_dim,\n",
        "            vocab_size=vocab_size\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First pass through FLOW to get latent representation\n",
        "        z_flow, ldj = self.flow(x)\n",
        "        \n",
        "        # Use FLOW's output as input to VAE\n",
        "        recon_x, mu, logvar = self.vae(z_flow)\n",
        "        \n",
        "        return recon_x, mu, logvar, ldj, z_flow\n",
        "\n",
        "def train_step(model, data, flow_optimizer, vae_optimizer, device):\n",
        "    \"\"\"Separated training step for FLOW and VAE\"\"\"\n",
        "    data = data.to(device)\n",
        "    \n",
        "    # 1. Train FLOW\n",
        "    flow_optimizer.zero_grad()\n",
        "    z_flow, ldj = model.flow(data)\n",
        "    flow_loss = -ldj.mean()  # Maximize log-likelihood\n",
        "    flow_loss.backward()\n",
        "    flow_optimizer.step()\n",
        "    \n",
        "    # 2. Train VAE using FLOW's output (detached)\n",
        "    vae_optimizer.zero_grad()\n",
        "    with torch.no_grad():\n",
        "        z_flow, _ = model.flow(data)  # Get fresh flow output\n",
        "    recon_batch, mu, logvar = model.vae(z_flow.detach())\n",
        "    vae_loss = compute_vae_loss(recon_batch, data, mu, logvar, train_dataset)\n",
        "    vae_loss.backward()\n",
        "    vae_optimizer.step()\n",
        "    \n",
        "    return flow_loss.item(), vae_loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "3cf1fadf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch [100/198083] | Flow Loss: -7.3042 | VAE Loss: 8470.0967\n",
            "Batch [200/198083] | Flow Loss: -7.4689 | VAE Loss: 1408.8718\n",
            "Batch [300/198083] | Flow Loss: -7.4943 | VAE Loss: 1129.9821\n",
            "Batch [400/198083] | Flow Loss: -7.4985 | VAE Loss: 1106.0850\n",
            "Batch [500/198083] | Flow Loss: -7.4982 | VAE Loss: 1098.4154\n",
            "Batch [600/198083] | Flow Loss: -7.4996 | VAE Loss: 1129.1578\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[92], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m epoch_vae_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m---> 18\u001b[0m     flow_loss, vae_loss \u001b[38;5;241m=\u001b[39m train_step(\n\u001b[0;32m     19\u001b[0m         model, data, flow_optimizer, vae_optimizer, device\n\u001b[0;32m     20\u001b[0m     )\n\u001b[0;32m     22\u001b[0m     epoch_flow_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m flow_loss\n\u001b[0;32m     23\u001b[0m     epoch_vae_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m vae_loss\n",
            "Cell \u001b[1;32mIn[91], line 36\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, data, flow_optimizer, vae_optimizer, device)\u001b[0m\n\u001b[0;32m     34\u001b[0m z_flow, ldj \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mflow(data)\n\u001b[0;32m     35\u001b[0m flow_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mldj\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# Maximize log-likelihood\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m flow_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     37\u001b[0m flow_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# 2. Train VAE using FLOW's output (detached)\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\valde\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    583\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\valde\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\valde\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "model = CombinedModel(\n",
        "    input_dim=input_dim,\n",
        "    hidden_dim=256,\n",
        "    latent_dim=latent_dim,\n",
        "    num_flow_layers=4\n",
        ").to(device)\n",
        "\n",
        "flow_optimizer = optim.Adam(model.flow.parameters(), lr=0.0001)\n",
        "vae_optimizer = optim.Adam(model.vae.parameters(), lr=0.0001)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    epoch_flow_loss = 0\n",
        "    epoch_vae_loss = 0\n",
        "    \n",
        "    for batch_idx, data in enumerate(train_loader):\n",
        "        flow_loss, vae_loss = train_step(\n",
        "            model, data, flow_optimizer, vae_optimizer, device\n",
        "        )\n",
        "        \n",
        "        epoch_flow_loss += flow_loss\n",
        "        epoch_vae_loss += vae_loss\n",
        "        \n",
        "        if (batch_idx + 1) % 100 == 0:\n",
        "            print(f'Batch [{batch_idx + 1}/{len(train_loader)}] | '\n",
        "                  f'Flow Loss: {flow_loss:.4f} | '\n",
        "                  f'VAE Loss: {vae_loss:.4f}')\n",
        "\n",
        "    # Generate molecules using the trained models\n",
        "    with torch.no_grad():\n",
        "        z = torch.randn(1, latent_dim).to(device)\n",
        "        z_flow, _ = model.flow(z)\n",
        "        generated = model.vae.decode(z_flow)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MenC6MTLaiIg",
      "metadata": {
        "id": "MenC6MTLaiIg"
      },
      "source": [
        "Flow add, it created a significant drop in loss but then negative loss and break model. Possibly try to make the model stop before that happens of adjust the parameters with flow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc567905",
      "metadata": {
        "id": "bc567905"
      },
      "outputs": [],
      "source": [
        "# Generate a new molecule from VAE by sampling from the latent space\n",
        "generated_smiles = generate_smiles(vae, latent_dim, train_dataset.idx_to_char, max_length, vocab_size)  # pass idx_to_char\n",
        "\n",
        "print(f\"Generated SMILES: {generated_smiles}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h-nqTr8BddZi",
      "metadata": {
        "id": "h-nqTr8BddZi"
      },
      "source": [
        "Thoughts:\n",
        "\n",
        "Potetnial invalidity due to:\n",
        "Insufficient training\n",
        "More data needed\n",
        "Expansion of latent space"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gtDExC7pI0tw",
      "metadata": {
        "id": "gtDExC7pI0tw"
      },
      "source": [
        "Problems recorded:\n",
        "\n",
        "Enountered a problem where the model is taking a terabyte of data at once and breaking.\n",
        "  For now adjusted the number of strings in the dataset\n",
        "  Learned that the model is not properly breaking up the strings and just taking them whole.\n",
        "\n",
        "Encountered trouble with model only printing carbon and negative learning value\n",
        "  Learned that vocab was not correctly understood by the model\n",
        "  Adjusted how the data was recorded and one-hot encoding.\n",
        "\n",
        "VAE model original version had problems in learning and structure may be off.\n",
        "  To keep it simple for now we used a simple designed VAE version but taken from GPT as a template.\n",
        "\n",
        "Problem with FLOW use and negative Loss\n",
        "  Jacobian is improperly recorded and used. Need to rework Flow Model\n",
        "  Look into Kosaraju GLOW model as a proper reference"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
