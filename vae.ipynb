{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes:\n",
    "'''\n",
    "VAE can be used as the standard and template if we plan to continue as a type of survey of various other models. (honestly would be the easiest and get the most information?)\n",
    "\n",
    "Use VAE to collect latent space that will be used to construct the new data \n",
    "\n",
    "One-hot encoding be used for the data\n",
    "\n",
    "Testing with Moses in performance and accuracy\n",
    "\n",
    "One work with Moses and familiarize? Team work on models for testing?\n",
    "\n",
    "Moses provides various models: LatentGAN, CharRNN (interesting with use of generation by character by character or through block by block)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imported code from MOSES, needs to be modified and missing other py files included in folder... https://github.com/molecularsets/moses/blob/master/moses/vae/model.py\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, vocab, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocabulary = vocab\n",
    "        # Special symbols\n",
    "        for ss in ('bos', 'eos', 'unk', 'pad'):\n",
    "            setattr(self, ss, getattr(vocab, ss))\n",
    "\n",
    "        # Word embeddings layer\n",
    "        n_vocab, d_emb = len(vocab), vocab.vectors.size(1)\n",
    "        self.x_emb = nn.Embedding(n_vocab, d_emb, self.pad)\n",
    "        self.x_emb.weight.data.copy_(vocab.vectors)\n",
    "        if config.freeze_embeddings:\n",
    "            self.x_emb.weight.requires_grad = False\n",
    "\n",
    "        # Encoder\n",
    "        if config.q_cell == 'gru':\n",
    "            self.encoder_rnn = nn.GRU(\n",
    "                d_emb,\n",
    "                config.q_d_h,\n",
    "                num_layers=config.q_n_layers,\n",
    "                batch_first=True,\n",
    "                dropout=config.q_dropout if config.q_n_layers > 1 else 0,\n",
    "                bidirectional=config.q_bidir\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid q_cell type, should be one of the ('gru',)\"\n",
    "            )\n",
    "\n",
    "        q_d_last = config.q_d_h * (2 if config.q_bidir else 1)\n",
    "        self.q_mu = nn.Linear(q_d_last, config.d_z)\n",
    "        self.q_logvar = nn.Linear(q_d_last, config.d_z)\n",
    "\n",
    "        # Decoder\n",
    "        if config.d_cell == 'gru':\n",
    "            self.decoder_rnn = nn.GRU(\n",
    "                d_emb + config.d_z,\n",
    "                config.d_d_h,\n",
    "                num_layers=config.d_n_layers,\n",
    "                batch_first=True,\n",
    "                dropout=config.d_dropout if config.d_n_layers > 1 else 0\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid d_cell type, should be one of the ('gru',)\"\n",
    "            )\n",
    "\n",
    "        self.decoder_lat = nn.Linear(config.d_z, config.d_d_h)\n",
    "        self.decoder_fc = nn.Linear(config.d_d_h, n_vocab)\n",
    "\n",
    "        # Grouping the model's parameters\n",
    "        self.encoder = nn.ModuleList([\n",
    "            self.encoder_rnn,\n",
    "            self.q_mu,\n",
    "            self.q_logvar\n",
    "        ])\n",
    "        self.decoder = nn.ModuleList([\n",
    "            self.decoder_rnn,\n",
    "            self.decoder_lat,\n",
    "            self.decoder_fc\n",
    "        ])\n",
    "        self.vae = nn.ModuleList([\n",
    "            self.x_emb,\n",
    "            self.encoder,\n",
    "            self.decoder\n",
    "        ])\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def string2tensor(self, string, device='model'):\n",
    "        ids = self.vocabulary.string2ids(string, add_bos=True, add_eos=True)\n",
    "        tensor = torch.tensor(\n",
    "            ids, dtype=torch.long,\n",
    "            device=self.device if device == 'model' else device\n",
    "        )\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def tensor2string(self, tensor):\n",
    "        ids = tensor.tolist()\n",
    "        string = self.vocabulary.ids2string(ids, rem_bos=True, rem_eos=True)\n",
    "\n",
    "        return string\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Do the VAE forward step\n",
    "\n",
    "        :param x: list of tensors of longs, input sentence x\n",
    "        :return: float, kl term component of loss\n",
    "        :return: float, recon component of loss\n",
    "        \"\"\"\n",
    "\n",
    "        # Encoder: x -> z, kl_loss\n",
    "        z, kl_loss = self.forward_encoder(x)\n",
    "\n",
    "        # Decoder: x, z -> recon_loss\n",
    "        recon_loss = self.forward_decoder(x, z)\n",
    "\n",
    "        return kl_loss, recon_loss\n",
    "\n",
    "    def forward_encoder(self, x):\n",
    "        \"\"\"Encoder step, emulating z ~ E(x) = q_E(z|x)\n",
    "\n",
    "        :param x: list of tensors of longs, input sentence x\n",
    "        :return: (n_batch, d_z) of floats, sample of latent vector z\n",
    "        :return: float, kl term component of loss\n",
    "        \"\"\"\n",
    "\n",
    "        x = [self.x_emb(i_x) for i_x in x]\n",
    "        x = nn.utils.rnn.pack_sequence(x)\n",
    "\n",
    "        _, h = self.encoder_rnn(x, None)\n",
    "\n",
    "        h = h[-(1 + int(self.encoder_rnn.bidirectional)):]\n",
    "        h = torch.cat(h.split(1), dim=-1).squeeze(0)\n",
    "\n",
    "        mu, logvar = self.q_mu(h), self.q_logvar(h)\n",
    "        eps = torch.randn_like(mu)\n",
    "        z = mu + (logvar / 2).exp() * eps\n",
    "\n",
    "        kl_loss = 0.5 * (logvar.exp() + mu ** 2 - 1 - logvar).sum(1).mean()\n",
    "\n",
    "        return z, kl_loss\n",
    "\n",
    "    def forward_decoder(self, x, z):\n",
    "        \"\"\"Decoder step, emulating x ~ G(z)\n",
    "\n",
    "        :param x: list of tensors of longs, input sentence x\n",
    "        :param z: (n_batch, d_z) of floats, latent vector z\n",
    "        :return: float, recon component of loss\n",
    "        \"\"\"\n",
    "\n",
    "        lengths = [len(i_x) for i_x in x]\n",
    "\n",
    "        x = nn.utils.rnn.pad_sequence(x, batch_first=True,\n",
    "                                      padding_value=self.pad)\n",
    "        x_emb = self.x_emb(x)\n",
    "\n",
    "        z_0 = z.unsqueeze(1).repeat(1, x_emb.size(1), 1)\n",
    "        x_input = torch.cat([x_emb, z_0], dim=-1)\n",
    "        x_input = nn.utils.rnn.pack_padded_sequence(x_input, lengths,\n",
    "                                                    batch_first=True)\n",
    "\n",
    "        h_0 = self.decoder_lat(z)\n",
    "        h_0 = h_0.unsqueeze(0).repeat(self.decoder_rnn.num_layers, 1, 1)\n",
    "\n",
    "        output, _ = self.decoder_rnn(x_input, h_0)\n",
    "\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "        y = self.decoder_fc(output)\n",
    "\n",
    "        recon_loss = F.cross_entropy(\n",
    "            y[:, :-1].contiguous().view(-1, y.size(-1)),\n",
    "            x[:, 1:].contiguous().view(-1),\n",
    "            ignore_index=self.pad\n",
    "        )\n",
    "\n",
    "        return recon_loss\n",
    "\n",
    "    def sample_z_prior(self, n_batch):\n",
    "        \"\"\"Sampling z ~ p(z) = N(0, I)\n",
    "\n",
    "        :param n_batch: number of batches\n",
    "        :return: (n_batch, d_z) of floats, sample of latent z\n",
    "        \"\"\"\n",
    "\n",
    "        return torch.randn(n_batch, self.q_mu.out_features,\n",
    "                           device=self.x_emb.weight.device)\n",
    "\n",
    "    def sample(self, n_batch, max_len=100, z=None, temp=1.0):\n",
    "        \"\"\"Generating n_batch samples in eval mode (`z` could be\n",
    "        not on same device)\n",
    "\n",
    "        :param n_batch: number of sentences to generate\n",
    "        :param max_len: max len of samples\n",
    "        :param z: (n_batch, d_z) of floats, latent vector z or None\n",
    "        :param temp: temperature of softmax\n",
    "        :return: list of tensors of strings, samples sequence x\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            if z is None:\n",
    "                z = self.sample_z_prior(n_batch)\n",
    "            z = z.to(self.device)\n",
    "            z_0 = z.unsqueeze(1)\n",
    "\n",
    "            # Initial values\n",
    "            h = self.decoder_lat(z)\n",
    "            h = h.unsqueeze(0).repeat(self.decoder_rnn.num_layers, 1, 1)\n",
    "            w = torch.tensor(self.bos, device=self.device).repeat(n_batch)\n",
    "            x = torch.tensor([self.pad], device=self.device).repeat(n_batch,\n",
    "                                                                    max_len)\n",
    "            x[:, 0] = self.bos\n",
    "            end_pads = torch.tensor([max_len], device=self.device).repeat(\n",
    "                n_batch)\n",
    "            eos_mask = torch.zeros(n_batch, dtype=torch.uint8,\n",
    "                                   device=self.device)\n",
    "\n",
    "            # Generating cycle\n",
    "            for i in range(1, max_len):\n",
    "                x_emb = self.x_emb(w).unsqueeze(1)\n",
    "                x_input = torch.cat([x_emb, z_0], dim=-1)\n",
    "\n",
    "                o, h = self.decoder_rnn(x_input, h)\n",
    "                y = self.decoder_fc(o.squeeze(1))\n",
    "                y = F.softmax(y / temp, dim=-1)\n",
    "\n",
    "                w = torch.multinomial(y, 1)[:, 0]\n",
    "                x[~eos_mask, i] = w[~eos_mask]\n",
    "                i_eos_mask = ~eos_mask & (w == self.eos)\n",
    "                end_pads[i_eos_mask] = i + 1\n",
    "                eos_mask = eos_mask | i_eos_mask\n",
    "\n",
    "            # Converting `x` to list of tensors\n",
    "            new_x = []\n",
    "            for i in range(x.size(0)):\n",
    "                new_x.append(x[i, :end_pads[i]])\n",
    "\n",
    "            return [self.tensor2string(i_x) for i_x in new_x]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
