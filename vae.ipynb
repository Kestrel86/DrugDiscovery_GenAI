{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e53c5634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\valde\\anaconda3\\Lib\\site-packages\\molsets-1.0-py3.11.egg\\moses\\metrics\\utils.py:24: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  _mcf.append(_pains, sort=True)['smarts'].values]\n"
     ]
    }
   ],
   "source": [
    "import moses\n",
    "import matplotlib as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e64afbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Functions are from the RNN model we have but not entirely sure where they would fit in the VAE\n",
    "Currently working on implementation, the process_smiles function may help in creating valid molecules\n",
    "'''\n",
    "\n",
    "# Function to add start and end tokens\n",
    "def process_smiles(smiles_list):\n",
    "    return [\"^\" + s + \"$\" for s in smiles_list]\n",
    "\n",
    "# Create character dictionaries including special tokens\n",
    "def create_vocab(smiles_list):\n",
    "    all_chars = sorted(list(set(''.join(smiles_list))))\n",
    "    char2idx = {ch: i + 1 for i, ch in enumerate(all_chars)}\n",
    "    char2idx[''] = 0  # Padding token\n",
    "    idx2char = {i: ch for ch, i in char2idx.items()}\n",
    "    return char2idx, idx2char, len(char2idx)\n",
    "\n",
    "# Enhanced tokenization\n",
    "def tokenize(smiles, char2idx):\n",
    "    return [char2idx.get(c, 0) for c in smiles]  # Default to 0 if unknown\n",
    "\n",
    "def detokenize(tokens, idx2char):\n",
    "    return ''.join([idx2char.get(t, '') for t in tokens if t != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "062ff3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_smiles_from_csv(path, split_type='train'):\n",
    "    '''\n",
    "    Loads SMILES strings from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the CSV file\n",
    "        split_type (str): Split type ('train' or 'test')\n",
    "\n",
    "    Returns:\n",
    "        list: List of SMILES strings\n",
    "    '''\n",
    "    smiles = []\n",
    "    with open(path, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            if row['SPLIT'].strip().lower() == split_type:\n",
    "                smiles.append(row['SMILES'].strip())\n",
    "    return smiles\n",
    "\n",
    "def extract_unique_chars(smiles_list):\n",
    "    '''\n",
    "    Extracts unique characters from a list of SMILES strings.\n",
    "\n",
    "    Args:\n",
    "        smiles_list (list): List of SMILES strings\n",
    "\n",
    "    Returns:\n",
    "        list: List of unique characters\n",
    "    '''\n",
    "    unique_chars = set()\n",
    "    for smiles in smiles_list:\n",
    "        unique_chars.update(smiles.strip())\n",
    "    return sorted(unique_chars)\n",
    "\n",
    "def clean_smiles(smiles):\n",
    "    '''\n",
    "    Cleans a SMILES string by removing unwanted characters.\n",
    "\n",
    "    Args:\n",
    "        smiles (str): SMILES string\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned SMILES string\n",
    "    '''\n",
    "    # Remove unwanted metadata like \",train\" or \",SPLIT\"\n",
    "    return smiles.split(',')[0].strip()\n",
    "\n",
    "def verify_smiles(smiles):\n",
    "  '''\n",
    "  Verifies the validity of a SMILES string using RDKit.\n",
    "\n",
    "  Args:\n",
    "      smiles (str): SMILES string to verify\n",
    "\n",
    "  Returns:\n",
    "      bool: True if valid, False otherwise\n",
    "  '''\n",
    "  mol = Chem.MolFromSmiles(smiles)\n",
    "  return mol is not None\n",
    "\n",
    "def decode_smiles(one_hot_tensor, idx_to_char):\n",
    "    '''\n",
    "    Decodes a one-hot encoded tensor back to SMILES.\n",
    "\n",
    "    Args:\n",
    "        one_hot_tensor (torch.Tensor): One-hot encoded tensor\n",
    "    '''\n",
    "    smiles = ''\n",
    "    one_hot_tensor = one_hot_tensor.view(-1, len(idx_to_char))  # unflatten\n",
    "    for row in one_hot_tensor:\n",
    "        idx = row.argmax().item()\n",
    "        smiles += idx_to_char[idx]\n",
    "    return smiles.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e2abef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new molecule from VAE by sampling from the latent space\n",
    "\n",
    "# added default values for what the vocab and length generally are\n",
    "def generate_smiles(model, latent_dim=64, idx_to_char=None, temperature=1.0):\n",
    "    '''\n",
    "    Generates a new SMILES string by sampling from the VAE's latent space.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): VAE model\n",
    "        latent_dim (int): Dimension of the latent space\n",
    "    '''\n",
    "    z = torch.randn(1, latent_dim).to(model.fc1.weight.device)  # Ensure z is on the same device as the model\n",
    "    with torch.no_grad():\n",
    "        generated = model.decode(z)  # Use decode instead of decoder\n",
    "    # Add postprocessing to convert to SMILES\n",
    "    # generated_tokens_indices = torch.argmax(generated, dim=-1).cpu().numpy().flatten()\n",
    "    #probs = F.softmax(generated / temperature, dim=-1)\n",
    "    probs = F.softmax(generated.view(max_length, vocab_size) / temperature, dim=-1)\n",
    "\n",
    "    # Sample the next character from the probability distribution\n",
    "    generated_tokens_indices = torch.multinomial(probs, 1).cpu().numpy().flatten()\n",
    "\n",
    "    # # === ✅ DEBUG BLOCK START ===\n",
    "    # print(\"Sample vocab mapping:\", list(idx_to_char.items())[:10])\n",
    "    # unknowns = [i for i in generated_tokens_indices if i not in idx_to_char]\n",
    "    # if unknowns:\n",
    "    #     print(\"⚠️ Unknown token indices:\", unknowns)\n",
    "    # else:\n",
    "    #     print(\"✅ All indices map to known tokens.\")\n",
    "    # # === ✅ DEBUG BLOCK END ===\n",
    "\n",
    "    # print(\"Vocab size:\", len(idx_to_char))\n",
    "    # print(\"Max token index generated:\", max(generated_tokens_indices))\n",
    "\n",
    "    # # Print generated tokens and indices for debugging\n",
    "    # print(\"Generated tokens indices:\", generated_tokens_indices)\n",
    "    # print(\"Generated tokens:\", [idx_to_char.get(i, \"<UNK>\") for i in generated_tokens_indices])\n",
    "\n",
    "    # Iterate through indices to build the SMILES string\n",
    "    generated_smiles = \"\".join([idx_to_char.get(i, \"\") for i in generated_tokens_indices])\n",
    "\n",
    "    # Verification using rdkit\n",
    "    is_valid = verify_smiles(generated_smiles)\n",
    "    if is_valid:\n",
    "      return generated_smiles\n",
    "    else:\n",
    "      return \"INVALID\"\n",
    "\n",
    "    # return generated_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feacf2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for SMILES strings\n",
    "class SMILESDataset(Dataset):\n",
    "    def __init__(self, smiles_list, max_length=150, char_to_idx=None):\n",
    "        '''\n",
    "        Initializes the SMILESDataset with a list of SMILES strings.\n",
    "\n",
    "        Args:\n",
    "            smiles_list (list): List of SMILES strings\n",
    "            max_length (int): Maximum length of the SMILES strings\n",
    "            char_to_idx (dict): Character-to-index mapping\n",
    "\n",
    "        The dataset will one-hot encode each character in a SMILES string to a fixed-size tensor of shape (max_length * vocab_size).\n",
    "        If a SMILES string is shorter than max_length, it will be padded with zeros. If longer, it will be truncated.\n",
    "        '''\n",
    "        self.smiles_list = smiles_list\n",
    "        self.max_length = max_length\n",
    "\n",
    "        if char_to_idx is None:\n",
    "            raise ValueError(\"Please provide a fixed character-to-index mapping\")\n",
    "            # self.char_to_idx, self.idx_to_char = build_vocabulary(smiles_list)\n",
    "        else:\n",
    "            self.char_to_idx = char_to_idx\n",
    "            self.idx_to_char = {v: k for k, v in char_to_idx.items()}\n",
    "\n",
    "        self.vocab_size = len(self.char_to_idx)\n",
    "\n",
    "        original_count = len(smiles_list)\n",
    "        filtered = []\n",
    "        invalid_count = 0\n",
    "\n",
    "        for s in smiles_list:\n",
    "            s = s.strip()\n",
    "            if all(c in self.char_to_idx for c in s):\n",
    "                filtered.append(s)\n",
    "            else:\n",
    "                invalid_count += 1\n",
    "        print(f\"Total: {original_count}, Valid: {len(filtered)}, Invalid: {invalid_count}\")\n",
    "        self.smiles_list = filtered\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns:\n",
    "            int: Number of valid SMILES strings in the dataset\n",
    "        '''\n",
    "\n",
    "        return len(self.smiles_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Fetches the encoded version of a SMILES string at a given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the SMILES string to retrieve\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: One-hot encoded tensor of the SMILES string of shape (max_length * vocab_size)\n",
    "        '''\n",
    "\n",
    "        smiles = self.smiles_list[idx]\n",
    "        # One-hot encode the SMILES string\n",
    "        encoded = torch.zeros(self.max_length, self.vocab_size)\n",
    "        for i, char in enumerate(smiles[:self.max_length]):\n",
    "            encoded[i, self.char_to_idx[char]] = 1.0\n",
    "\n",
    "        return encoded.view(-1) #Flatten into 1D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c227beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rework flow, completely all over the place with its math and needs to be changed with how Jacobian is used\n",
    "\n",
    "\n",
    "class Flow(nn.Module):\n",
    "    def __init__(self, input_dim, num_flows):\n",
    "        super(Flow, self).__init__()\n",
    "        self.layers = nn.ModuleList([AffineCouplingLayer(input_dim) for _ in range(num_flows)])\n",
    "\n",
    "    def forward(self, x, reverse=False):\n",
    "        log_det_jacobian = 0\n",
    "        if not reverse:\n",
    "            for layer in self.layers:\n",
    "                x, ldj = layer(x, reverse=False)\n",
    "                log_det_jacobian += ldj\n",
    "        else:\n",
    "            for layer in reversed(self.layers):\n",
    "                x, ldj = layer(x, reverse=True)\n",
    "                log_det_jacobian += ldj\n",
    "        return x, log_det_jacobian\n",
    "\n",
    "\n",
    "class AffineCouplingLayer(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(AffineCouplingLayer, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim // 2, input_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim, input_dim // 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, reverse=False):\n",
    "        x1, x2 = x.chunk(2, dim=1)  # Split input into two halves\n",
    "        if not reverse:\n",
    "            log_s = self.net(x1)\n",
    "            t = self.net(x1)\n",
    "            x2 = x2 * torch.exp(log_s) + t\n",
    "            log_det_jacobian = log_s.sum(dim=1)\n",
    "        else:\n",
    "            log_s = self.net(x1)\n",
    "            t = self.net(x1)\n",
    "            x2 = (x2 - t) * torch.exp(-log_s)\n",
    "            log_det_jacobian = -log_s.sum(dim=1)\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        return x, log_det_jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bd0c0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z is latent representation of input data\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, vocab_size):\n",
    "        super(VAE, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc_mu = nn.Linear(256, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(latent_dim, 256)\n",
    "        self.fc4 = nn.Linear(256, input_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc_mu(h1), self.fc_logvar(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, self.input_dim))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "\n",
    "        # Pass z through flow model\n",
    "        #z, log_det_jacobian = flow_model(z)\n",
    "\n",
    "        recon_x = self.decode(z)\n",
    "        return recon_x, mu, logvar, #log_det_jacobian # jacobian is added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "973932b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "  # log_det_jacobian is a parameter added for flow\n",
    "    batch_size = x.size(0)\n",
    "    vocab_size = 29\n",
    "    seq_len = x.size(1) // vocab_size  # should be 150\n",
    "\n",
    "    # Reshape both to [batch * seq_len, vocab_size]\n",
    "    x = x.view(batch_size, seq_len, vocab_size).view(-1, vocab_size)\n",
    "    recon_x = recon_x.view(batch_size, seq_len, vocab_size).view(-1, vocab_size)\n",
    "\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    #flow_loss = torch.sum(log_det_jacobian)\n",
    "\n",
    "    #return BCE + flow_loss\n",
    "    return BCE + KLD\n",
    "\n",
    "    # remove the kl divergence\n",
    "\n",
    "    # look into log det jacobian, fix problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a34a1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique characters: 29\n",
      "Unique characters in dataset:\n",
      "['#', '(', ')', '-', '1', '2', '3', '4', '5', '=', 'B', 'C', 'E', 'F', 'H', 'I', 'L', 'M', 'N', 'O', 'S', '[', ']', 'c', 'l', 'n', 'o', 'r', 's']\n",
      "Total: 10000, Valid: 10000, Invalid: 0\n",
      "Total: 10000, Valid: 10000, Invalid: 0\n",
      "Training Vocabulary Size: 29\n",
      "Test Vocabulary Size: 29\n",
      "# Train SMILES after filtering: 10000\n",
      "# Test SMILES after filtering: 10000\n",
      "Number of batches in train_loader: 1250\n",
      "Number of batches in test_loader: 1250\n"
     ]
    }
   ],
   "source": [
    "# normalize the data?\n",
    "# dont think we are, needed if using sigmoid for the deocder and BCE loss\n",
    "\n",
    "# Load SMILES strings\n",
    "with open('dataset/train.txt', 'r') as f:\n",
    "    smiles_train = [line.strip() for line in f]\n",
    "\n",
    "with open('dataset/test.txt', 'r') as f:\n",
    "    smiles_test = [line.strip() for line in f]\n",
    "\n",
    "# Apply cleaning to your SMILES\n",
    "smiles_train = [clean_smiles(smiles) for smiles in smiles_train]\n",
    "smiles_test = [clean_smiles(smiles) for smiles in smiles_test]\n",
    "smiles_train = smiles_train[:10000]\n",
    "smiles_test = smiles_test[:10000]\n",
    "\n",
    "# smiles_train = load_smiles_from_csv('dataset/train.txt', split_type='train')\n",
    "# smiles_test = load_smiles_from_csv('dataset/test.txt', split_type='test')  # if test rows are in same file\n",
    "\n",
    "# print(f\"Raw SMILES loaded: train={len(smiles_train)}, test={len(smiles_test)}\") # output for testing purposes\n",
    "all_smiles = smiles_train + smiles_test\n",
    "unique_chars = extract_unique_chars(all_smiles)\n",
    "\n",
    "print(f\"Total unique characters: {len(unique_chars)}\")\n",
    "print(\"Unique characters in dataset:\")\n",
    "print(unique_chars)\n",
    "\n",
    "# Use extracted unique characters to rebuild vocabulary\n",
    "VALID_CHARS = unique_chars\n",
    "char_to_idx = {c: i for i, c in enumerate(VALID_CHARS)}\n",
    "idx_to_char = {i: c for c, i in char_to_idx.items()}\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SMILESDataset(smiles_train, max_length=150, char_to_idx=char_to_idx)\n",
    "test_dataset = SMILESDataset(smiles_test, max_length=150, char_to_idx=char_to_idx)\n",
    "print(\"Training Vocabulary Size:\", train_dataset.vocab_size)\n",
    "print(\"Test Vocabulary Size:\", test_dataset.vocab_size) # Should be the same\n",
    "\n",
    "\n",
    "print(f\"# Train SMILES after filtering: {len(train_dataset)}\")\n",
    "print(f\"# Test SMILES after filtering: {len(test_dataset)}\")\n",
    "# train_dataset = SMILESDataset(smiles_train)\n",
    "# test_dataset = SMILESDataset(smiles_test, char_to_idx=train_dataset.char_to_idx)  # Share vocabulary\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)  # No need to shuffle test data\n",
    "\n",
    "print(f\"Number of batches in train_loader: {len(train_loader)}\")\n",
    "print(f\"Number of batches in test_loader: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb2c250f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "\n",
      "Sample SMILES visualizations:\n",
      "\n",
      "Sample 1\n",
      "Original : SMILES\n",
      "Decoded  : SMILES################################################################################################################################################\n",
      "Shape    : torch.Size([4350])\n",
      "\n",
      "Sample 2\n",
      "Original : CCCS(=O)c1ccc2[nH]c(=NC(=O)OC)[nH]c2c1\n",
      "Decoded  : CCCS(=O)c1ccc2[nH]c(=NC(=O)OC)[nH]c2c1################################################################################################################\n",
      "Shape    : torch.Size([4350])\n",
      "\n",
      "Sample 3\n",
      "Original : CC(C)(C)C(=O)C(Oc1ccc(Cl)cc1)n1ccnc1\n",
      "Decoded  : CC(C)(C)C(=O)C(Oc1ccc(Cl)cc1)n1ccnc1##################################################################################################################\n",
      "Shape    : torch.Size([4350])\n"
     ]
    }
   ],
   "source": [
    "# Check a batch of data\n",
    "for i, data in enumerate(train_loader):\n",
    "    if i == 0:  # Just visualize the first batch\n",
    "        print(data)\n",
    "        break\n",
    "\n",
    "# Visualize 3 samples\n",
    "print(\"\\nSample SMILES visualizations:\")\n",
    "for i in range(3):\n",
    "    encoded = train_dataset[i]\n",
    "    original = train_dataset.smiles_list[i]\n",
    "    decoded = decode_smiles(encoded, train_dataset.idx_to_char)\n",
    "\n",
    "    print(f\"\\nSample {i+1}\")\n",
    "    print(f\"Original : {original}\")\n",
    "    print(f\"Decoded  : {decoded}\")\n",
    "    print(f\"Shape    : {encoded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f21946e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Vocab size: 29\n",
      "max_length: 150\n",
      "Input dim: 4350\n",
      "Epoch [1/100], Train Loss: 2969.57891015625\n",
      "Epoch [1/100], Test Loss: 1133.6417221191407\n",
      "Epoch [2/100], Train Loss: 1065.87243828125\n",
      "Epoch [2/100], Test Loss: 1051.2090211914062\n",
      "Epoch [3/100], Train Loss: 1022.4547129882812\n",
      "Epoch [3/100], Test Loss: 1027.7546665527343\n",
      "Epoch [4/100], Train Loss: 1001.4437251464843\n",
      "Epoch [4/100], Test Loss: 1011.0516913574219\n",
      "Epoch [5/100], Train Loss: 985.6617461914062\n",
      "Epoch [5/100], Test Loss: 988.7768387695312\n",
      "Epoch [6/100], Train Loss: 958.1156721679688\n",
      "Epoch [6/100], Test Loss: 962.3720509765625\n",
      "Epoch [7/100], Train Loss: 931.314619921875\n",
      "Epoch [7/100], Test Loss: 935.6422159667969\n",
      "Epoch [8/100], Train Loss: 901.3379482910157\n",
      "Epoch [8/100], Test Loss: 908.65354296875\n",
      "Epoch [9/100], Train Loss: 873.2577166503906\n",
      "Epoch [9/100], Test Loss: 883.4883260742188\n",
      "Epoch [10/100], Train Loss: 847.8459929199219\n",
      "Epoch [10/100], Test Loss: 862.5189785644532\n",
      "Epoch [11/100], Train Loss: 826.8830677734375\n",
      "Epoch [11/100], Test Loss: 845.2999850097656\n",
      "Epoch [12/100], Train Loss: 810.9522645507813\n",
      "Epoch [12/100], Test Loss: 832.706217578125\n",
      "Epoch [13/100], Train Loss: 799.42793984375\n",
      "Epoch [13/100], Test Loss: 822.5632823242188\n",
      "Epoch [14/100], Train Loss: 787.8487215332032\n",
      "Epoch [14/100], Test Loss: 815.1199987304688\n",
      "Epoch [15/100], Train Loss: 778.3188438964844\n",
      "Epoch [15/100], Test Loss: 806.6072126953125\n",
      "Epoch [16/100], Train Loss: 769.23384765625\n",
      "Epoch [16/100], Test Loss: 799.1890755371094\n",
      "Epoch [17/100], Train Loss: 761.7943828125\n",
      "Epoch [17/100], Test Loss: 793.445326953125\n",
      "Epoch [18/100], Train Loss: 754.1041890625\n",
      "Epoch [18/100], Test Loss: 788.428196875\n",
      "Epoch [19/100], Train Loss: 747.5005862792968\n",
      "Epoch [19/100], Test Loss: 782.66805859375\n",
      "Epoch [20/100], Train Loss: 742.1414351074219\n",
      "Epoch [20/100], Test Loss: 777.3476724609375\n",
      "Epoch [21/100], Train Loss: 734.8295024902344\n",
      "Epoch [21/100], Test Loss: 772.4584418945312\n",
      "Epoch [22/100], Train Loss: 728.8786475097656\n",
      "Epoch [22/100], Test Loss: 767.2880419921875\n",
      "Epoch [23/100], Train Loss: 724.2238193359375\n",
      "Epoch [23/100], Test Loss: 762.9261711425781\n",
      "Epoch [24/100], Train Loss: 718.8549510742188\n",
      "Epoch [24/100], Test Loss: 759.3173191162109\n",
      "Epoch [25/100], Train Loss: 714.6082828125\n",
      "Epoch [25/100], Test Loss: 755.2867142089843\n",
      "Epoch [26/100], Train Loss: 708.9533217285157\n",
      "Epoch [26/100], Test Loss: 751.1675137207031\n",
      "Epoch [27/100], Train Loss: 703.4741851074219\n",
      "Epoch [27/100], Test Loss: 747.7171313476563\n",
      "Epoch [28/100], Train Loss: 698.3989265625\n",
      "Epoch [28/100], Test Loss: 743.9923621582032\n",
      "Epoch [29/100], Train Loss: 694.2860732421875\n",
      "Epoch [29/100], Test Loss: 739.5941633789063\n",
      "Epoch [30/100], Train Loss: 689.6315984863281\n",
      "Epoch [30/100], Test Loss: 736.5701304199218\n",
      "Epoch [31/100], Train Loss: 685.9932190429688\n",
      "Epoch [31/100], Test Loss: 733.4976857910157\n",
      "Epoch [32/100], Train Loss: 681.5292100097656\n",
      "Epoch [32/100], Test Loss: 729.8837774414062\n",
      "Epoch [33/100], Train Loss: 677.2221843261718\n",
      "Epoch [33/100], Test Loss: 726.6870599609375\n",
      "Epoch [34/100], Train Loss: 672.9009300292969\n",
      "Epoch [34/100], Test Loss: 723.1799996582031\n",
      "Epoch [35/100], Train Loss: 668.3406226074219\n",
      "Epoch [35/100], Test Loss: 720.5391446533204\n",
      "Epoch [36/100], Train Loss: 664.3453537597657\n",
      "Epoch [36/100], Test Loss: 718.1137398193359\n",
      "Epoch [37/100], Train Loss: 661.1769421875\n",
      "Epoch [37/100], Test Loss: 714.5214883789063\n",
      "Epoch [38/100], Train Loss: 656.7775756347656\n",
      "Epoch [38/100], Test Loss: 712.0036777099609\n",
      "Epoch [39/100], Train Loss: 653.3079395263671\n",
      "Epoch [39/100], Test Loss: 708.7621619140625\n",
      "Epoch [40/100], Train Loss: 649.4424551757812\n",
      "Epoch [40/100], Test Loss: 706.8242873291016\n",
      "Epoch [41/100], Train Loss: 646.8010551513672\n",
      "Epoch [41/100], Test Loss: 703.9936374267578\n",
      "Epoch [42/100], Train Loss: 643.4017344238281\n",
      "Epoch [42/100], Test Loss: 702.0547144287109\n",
      "Epoch [43/100], Train Loss: 640.9290587158204\n",
      "Epoch [43/100], Test Loss: 699.5833935791015\n",
      "Epoch [44/100], Train Loss: 636.3922237304688\n",
      "Epoch [44/100], Test Loss: 697.2718793945312\n",
      "Epoch [45/100], Train Loss: 633.6510449707031\n",
      "Epoch [45/100], Test Loss: 696.4278320556641\n",
      "Epoch [46/100], Train Loss: 631.4680893798828\n",
      "Epoch [46/100], Test Loss: 694.4904303955078\n",
      "Epoch [47/100], Train Loss: 628.3339190673828\n",
      "Epoch [47/100], Test Loss: 691.9111138671875\n",
      "Epoch [48/100], Train Loss: 626.7610531738281\n",
      "Epoch [48/100], Test Loss: 690.5225874511718\n",
      "Epoch [49/100], Train Loss: 624.4185392822266\n",
      "Epoch [49/100], Test Loss: 688.2472180664063\n",
      "Epoch [50/100], Train Loss: 621.1424080078125\n",
      "Epoch [50/100], Test Loss: 686.9694872070313\n",
      "Epoch [51/100], Train Loss: 618.5133373779297\n",
      "Epoch [51/100], Test Loss: 685.9077027832031\n",
      "Epoch [52/100], Train Loss: 615.7513654296876\n",
      "Epoch [52/100], Test Loss: 683.216957055664\n",
      "Epoch [53/100], Train Loss: 612.7647235351562\n",
      "Epoch [53/100], Test Loss: 681.3261086425781\n",
      "Epoch [54/100], Train Loss: 612.2711278320312\n",
      "Epoch [54/100], Test Loss: 679.4145753417969\n",
      "Epoch [55/100], Train Loss: 608.8312564941406\n",
      "Epoch [55/100], Test Loss: 678.1589854492188\n",
      "Epoch [56/100], Train Loss: 606.3171523925781\n",
      "Epoch [56/100], Test Loss: 676.7585964599609\n",
      "Epoch [57/100], Train Loss: 604.0646405761719\n",
      "Epoch [57/100], Test Loss: 675.3542117675781\n",
      "Epoch [58/100], Train Loss: 601.7488639404297\n",
      "Epoch [58/100], Test Loss: 674.141865258789\n",
      "Epoch [59/100], Train Loss: 600.2615075683593\n",
      "Epoch [59/100], Test Loss: 672.9288943603516\n",
      "Epoch [60/100], Train Loss: 597.7334107177734\n",
      "Epoch [60/100], Test Loss: 671.2903361083985\n",
      "Epoch [61/100], Train Loss: 595.7259610107421\n",
      "Epoch [61/100], Test Loss: 670.50421328125\n",
      "Epoch [62/100], Train Loss: 593.5932438720703\n",
      "Epoch [62/100], Test Loss: 669.8556382080078\n",
      "Epoch [63/100], Train Loss: 591.4689135009766\n",
      "Epoch [63/100], Test Loss: 667.476460546875\n",
      "Epoch [64/100], Train Loss: 589.1621616210938\n",
      "Epoch [64/100], Test Loss: 667.2526008300781\n",
      "Epoch [65/100], Train Loss: 587.924353125\n",
      "Epoch [65/100], Test Loss: 665.7152352294922\n",
      "Epoch [66/100], Train Loss: 585.7416284667969\n",
      "Epoch [66/100], Test Loss: 664.9729456298828\n",
      "Epoch [67/100], Train Loss: 585.0162302734375\n",
      "Epoch [67/100], Test Loss: 662.9741585693359\n",
      "Epoch [68/100], Train Loss: 582.4625245117187\n",
      "Epoch [68/100], Test Loss: 662.2702916015625\n",
      "Epoch [69/100], Train Loss: 580.7553630859375\n",
      "Epoch [69/100], Test Loss: 661.930444921875\n",
      "Epoch [70/100], Train Loss: 579.6664933837891\n",
      "Epoch [70/100], Test Loss: 660.7852368896484\n",
      "Epoch [71/100], Train Loss: 578.3120167480469\n",
      "Epoch [71/100], Test Loss: 659.3972588378906\n",
      "Epoch [72/100], Train Loss: 576.352981616211\n",
      "Epoch [72/100], Test Loss: 659.2742655517578\n",
      "Epoch [73/100], Train Loss: 575.7893638671875\n",
      "Epoch [73/100], Test Loss: 658.1608939941407\n",
      "Epoch [74/100], Train Loss: 573.7835397949219\n",
      "Epoch [74/100], Test Loss: 658.0862479980469\n",
      "Epoch [75/100], Train Loss: 571.4882754394531\n",
      "Epoch [75/100], Test Loss: 656.62323828125\n",
      "Epoch [76/100], Train Loss: 570.9942907226563\n",
      "Epoch [76/100], Test Loss: 656.5886178710938\n",
      "Epoch [77/100], Train Loss: 569.41016328125\n",
      "Epoch [77/100], Test Loss: 654.6307635742188\n",
      "Epoch [78/100], Train Loss: 568.0536788574219\n",
      "Epoch [78/100], Test Loss: 654.9725834472656\n",
      "Epoch [79/100], Train Loss: 565.5277290283203\n",
      "Epoch [79/100], Test Loss: 655.0200658935547\n",
      "Epoch [80/100], Train Loss: 565.4036463378907\n",
      "Epoch [80/100], Test Loss: 652.7508605712891\n",
      "Epoch [81/100], Train Loss: 563.5248790771484\n",
      "Epoch [81/100], Test Loss: 653.2442686767578\n",
      "Epoch [82/100], Train Loss: 562.89753359375\n",
      "Epoch [82/100], Test Loss: 651.9366342041016\n",
      "Epoch [83/100], Train Loss: 561.0519858398437\n",
      "Epoch [83/100], Test Loss: 651.2366601074219\n",
      "Epoch [84/100], Train Loss: 560.2650584472656\n",
      "Epoch [84/100], Test Loss: 651.1190072021484\n",
      "Epoch [85/100], Train Loss: 558.9551830078125\n",
      "Epoch [85/100], Test Loss: 650.6016194335938\n",
      "Epoch [86/100], Train Loss: 557.8453103759765\n",
      "Epoch [86/100], Test Loss: 648.9395408935546\n",
      "Epoch [87/100], Train Loss: 557.0458626220703\n",
      "Epoch [87/100], Test Loss: 648.9667097900391\n",
      "Epoch [88/100], Train Loss: 555.2925442626953\n",
      "Epoch [88/100], Test Loss: 650.1709903320312\n",
      "Epoch [89/100], Train Loss: 553.6515066162109\n",
      "Epoch [89/100], Test Loss: 647.6377417236328\n",
      "Epoch [90/100], Train Loss: 553.2238615478516\n",
      "Epoch [90/100], Test Loss: 647.3643305664062\n",
      "Epoch [91/100], Train Loss: 553.1136943603516\n",
      "Epoch [91/100], Test Loss: 646.8684080078125\n",
      "Epoch [92/100], Train Loss: 551.5328494140625\n",
      "Epoch [92/100], Test Loss: 646.4886517333985\n",
      "Epoch [93/100], Train Loss: 550.2422532958984\n",
      "Epoch [93/100], Test Loss: 646.412859375\n",
      "Epoch [94/100], Train Loss: 548.7006865966797\n",
      "Epoch [94/100], Test Loss: 646.3022349365234\n",
      "Epoch [95/100], Train Loss: 548.3885009033203\n",
      "Epoch [95/100], Test Loss: 645.0170507080078\n",
      "Epoch [96/100], Train Loss: 546.4821974365234\n",
      "Epoch [96/100], Test Loss: 644.8036751953125\n",
      "Epoch [97/100], Train Loss: 546.92362109375\n",
      "Epoch [97/100], Test Loss: 644.3566545410156\n",
      "Epoch [98/100], Train Loss: 545.5107967529297\n",
      "Epoch [98/100], Test Loss: 644.045300024414\n",
      "Epoch [99/100], Train Loss: 544.1588635986328\n",
      "Epoch [99/100], Test Loss: 644.0984356933594\n",
      "Epoch [100/100], Train Loss: 543.256302368164\n",
      "Epoch [100/100], Test Loss: 642.9334056396484\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Instantiate the VAE model\n",
    "input_dim = train_dataset.vocab_size * train_dataset.max_length  # Flatten the input (max_length x vocab_size)\n",
    "latent_dim = 64\n",
    "# try lowering the latent dim as it was too high at 128\n",
    "# 128\n",
    "\n",
    "vocab_size = train_dataset.vocab_size\n",
    "max_length = train_dataset.max_length\n",
    "\n",
    "print(\"Vocab size:\", train_dataset.vocab_size)\n",
    "print(\"max_length:\", train_dataset.max_length)\n",
    "print(\"Input dim:\", input_dim)\n",
    "\n",
    "vae = VAE(input_dim, latent_dim, len(idx_to_char))\n",
    "vae.to(device)\n",
    "\n",
    "# Must define flow and affline coupling layer\n",
    "# input_dim = vae.latent_dim\n",
    "# num_flows = 4 # Number of flow layers\n",
    "# flow_model = Flow(input_dim, latent_dim, num_flows)\n",
    "# flow_model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "# 1e-5 = 0.00001\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=0.0001)\n",
    "#optimizer = torch.optim.Adam(list(vae.parameters()) + list(flow_model.parameters()), lr=0.0001)\n",
    "\n",
    "\n",
    "# Training and Evaluation loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    vae.train()  # Set model to training mode\n",
    "    train_loss = 0\n",
    "    for data in train_loader:  # Iterate over training data\n",
    "        optimizer.zero_grad()\n",
    "        # Flatten the input here before passing to the model\n",
    "        #data = data.view(-1, input_dim).to(device)\n",
    "        data = data.to(device)\n",
    "        recon_batch, mu, logvar = vae(data)\n",
    "        #recon_batch, mu, logvar, log_det_jacobian = vae(data)\n",
    "\n",
    "        #print(f\"Reconstructed output: {recon_batch[:5]}\") # testing\n",
    "        #break\n",
    "\n",
    "        loss = vae_loss(recon_batch, data, mu, logvar)\n",
    "        #loss = vae_loss(recon_batch, data, mu, logvar, log_det_jacobian)\n",
    "\n",
    "        #print(f\"batch loss: {loss.item()}\") # for testing\n",
    "\n",
    "        loss.backward() # back prop\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.step() # optimization\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/len(train_loader)}')\n",
    "\n",
    "    # Evaluation on test set\n",
    "    vae.eval()  # Set model to evaluation mode\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():  # No need to calculate gradients during evaluation\n",
    "        for data in test_loader:  # Iterate over test data\n",
    "            # Flatten the input here as well\n",
    "            #data = data.view(-1, input_dim).to(device)\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = vae(data)\n",
    "            #recon_batch, mu, logvar, log_det_jacobian = vae(data)\n",
    "            #loss = vae_loss(recon_batch, data, mu, logvar, log_det_jacobian)\n",
    "            loss = vae_loss(recon_batch, data, mu, logvar)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Test Loss: {test_loss/len(test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e81f7e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated SMILES: INVALID\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10:58:31] SMILES Parse Error: syntax error while parsing: 3n[S=EMNLH5OH2LI][2on[I4#(2rc]cl#(H[S=]53OLH))MMC34L3M[s5Sr#H)5I2F#I=lF-52(S1EBoooosI(5-3oo(]34c(s4lCo=SC2LnE-LBH1M([2l=-3)CnI=(=-)=r-Il33lL=(OH)FcLEO\n",
      "[10:58:31] SMILES Parse Error: check for mistakes around position 1:\n",
      "[10:58:31] 3n[S=EMNLH5OH2LI][2on[I4#(2rc]cl#(H[S=]53\n",
      "[10:58:31] ^\n",
      "[10:58:31] SMILES Parse Error: Failed parsing SMILES '3n[S=EMNLH5OH2LI][2on[I4#(2rc]cl#(H[S=]53OLH))MMC34L3M[s5Sr#H)5I2F#I=lF-52(S1EBoooosI(5-3oo(]34c(s4lCo=SC2LnE-LBH1M([2l=-3)CnI=(=-)=r-Il33lL=(OH)FcLEO' for input: '3n[S=EMNLH5OH2LI][2on[I4#(2rc]cl#(H[S=]53OLH))MMC34L3M[s5Sr#H)5I2F#I=lF-52(S1EBoooosI(5-3oo(]34c(s4lCo=SC2LnE-LBH1M([2l=-3)CnI=(=-)=r-Il33lL=(OH)FcLEO'\n"
     ]
    }
   ],
   "source": [
    "# Generate a new molecule from VAE by sampling from the latent space\n",
    "generated_smiles = generate_smiles(vae, latent_dim, train_dataset.idx_to_char)  # pass idx_to_char\n",
    "\n",
    "print(f\"Generated SMILES: {generated_smiles}\")\n",
    "\n",
    "# # Mock setup for quick testing\n",
    "# vae.eval()  # Set model to eval mode (disables dropout, etc.)\n",
    "\n",
    "# # Generate SMILES from random latent vector\n",
    "# try:\n",
    "#     result = generate_smiles(vae, latent_dim, idx_to_char=train_dataset.idx_to_char, temperature=1.0)\n",
    "#     print(\"Test SMILES output:\", result) # will print invalid if not valid\n",
    "# except Exception as e:\n",
    "#     print(\"Error while generating SMILES:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
