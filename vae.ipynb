{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e53c5634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import moses\n",
    "import matplotlib as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4d9d22e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMILES</th>\n",
       "      <th>SPLIT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCCS(=O)c1ccc2[nH]c(=NC(=O)OC)[nH]c2c1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CC(C)(C)C(=O)C(Oc1ccc(Cl)cc1)n1ccnc1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cc1c(Cl)cccc1Nc1ncccc1C(=O)OCC(O)CO</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cn1cnc2c1c(=O)n(CC(O)CO)c(=O)n2C</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CC1Oc2ccc(Cl)cc2N(CC(O)CO)C1=O</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   SMILES  SPLIT\n",
       "0  CCCS(=O)c1ccc2[nH]c(=NC(=O)OC)[nH]c2c1  train\n",
       "1    CC(C)(C)C(=O)C(Oc1ccc(Cl)cc1)n1ccnc1  train\n",
       "2     Cc1c(Cl)cccc1Nc1ncccc1C(=O)OCC(O)CO  train\n",
       "3        Cn1cnc2c1c(=O)n(CC(O)CO)c(=O)n2C  train\n",
       "4          CC1Oc2ccc(Cl)cc2N(CC(O)CO)C1=O  train"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"dataset/train.txt\", sep=',')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e64afbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_CHARS = list(\"@=#$()%1234567890ABCDEFGHIJKLMNOPQRSTUVWXYZ[]\\\\+-/.:\")\n",
    "char_to_idx = {c: i for i, c in enumerate(VALID_CHARS)}\n",
    "idx_to_char = {i: c for c, i in char_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "062ff3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for SMILES strings\n",
    "class SMILESDataset(Dataset):\n",
    "    def __init__(self, smiles_list, max_length=150, char_to_idx=None):\n",
    "        self.smiles_list = smiles_list\n",
    "        self.max_length = max_length\n",
    "\n",
    "        if char_to_idx is None:\n",
    "            raise ValueError(\"Please provide a fixed character-to-index mapping\")\n",
    "            # self.char_to_idx, self.idx_to_char = build_vocabulary(smiles_list)\n",
    "        else:\n",
    "            self.char_to_idx = char_to_idx\n",
    "            self.idx_to_char = {v: k for k, v in char_to_idx.items()}\n",
    "\n",
    "        self.vocab_size = len(self.char_to_idx)\n",
    "\n",
    "        self.smiles_list = []\n",
    "        for s in smiles_list:\n",
    "            s = s.strip()\n",
    "            if all(c in self.char_to_idx for c in s):\n",
    "                self.smiles_list.append(s)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smiles = self.smiles_list[idx]\n",
    "        # One-hot encode the SMILES string\n",
    "        encoded = torch.zeros(self.max_length, self.vocab_size)\n",
    "        for i, char in enumerate(smiles[:self.max_length]):\n",
    "            encoded[i, self.char_to_idx[char]] = 1.0\n",
    "\n",
    "        # # Pad with zeros\n",
    "        # if len(smiles) < self.max_length:\n",
    "        #     encoded[len(smiles):, :] = 0.0\n",
    "\n",
    "        return encoded.view(-1) #Flatten into 1D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e2abef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, 1024)  # Increased input layer size\n",
    "        self.fc2 = nn.Linear(1024, 512)  # Added an extra hidden layer\n",
    "        self.fc3 = nn.Linear(512, 256)  # Added another extra hidden layer for more complexity\n",
    "        self.fc21 = nn.Linear(256, latent_dim)  # Mean of latent distribution\n",
    "        self.fc22 = nn.Linear(256, latent_dim)  # Log variance of latent distribution\n",
    "\n",
    "        # Decoder\n",
    "        self.fc4 = nn.Linear(latent_dim, 256)  # Mirroring Encoder structure\n",
    "        self.fc5 = nn.Linear(256, 512) # Mirroring Encoder structure\n",
    "        self.fc6 = nn.Linear(512, 1024) # Mirroring Encoder structure\n",
    "        self.fc7 = nn.Linear(1024, input_dim)  # Output layer\n",
    "\n",
    "    # Note for later, changed the architecture to add dropout\n",
    "    def encode(self, x):\n",
    "        h1 = self.dropout(F.relu(self.fc1(x)))\n",
    "        h2 = self.dropout(F.relu(self.fc2(h1)))\n",
    "        h3 = self.dropout(F.relu(self.fc3(h2)))\n",
    "        return self.fc21(h3), self.fc22(h3)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h4 = self.dropout(F.relu(self.fc4(z)))\n",
    "        h5 = self.dropout(F.relu(self.fc5(h4)))\n",
    "        h6 = self.dropout(F.relu(self.fc6(h5)))\n",
    "        return torch.sigmoid(self.fc7(h6))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, self.input_dim))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feacf2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(recon_x, x, mu, logvar, beta=0.01):\n",
    "    BCE = F.binary_cross_entropy_with_logits(recon_x, x.view(-1, recon_x.size(1)), reduction='mean')\n",
    "    # mean seemed to do better\n",
    "    KL = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KL\n",
    "\n",
    "    #return BCE + beta * KL\n",
    "# add KL annealing factor to hlelp in slowing learning and avoid KL divergence dominating loss early?\n",
    "# or binary cross entropy with logits to handle the loss\n",
    "# binary_cross_entropy vs BCE with logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1c227beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def load_smiles_from_csv(path, split_type='train'):\n",
    "    smiles = []\n",
    "    with open(path, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            if row['SPLIT'].strip().lower() == split_type:\n",
    "                smiles.append(row['SMILES'].strip())\n",
    "    return smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "bb2c250f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Vocabulary Size: 51\n",
      "Test Vocabulary Size: 51\n",
      "# Train SMILES after filtering: 26481\n",
      "# Test SMILES after filtering: 2964\n",
      "Number of batches in train_loader: 3311\n",
      "Number of batches in test_loader: 371\n"
     ]
    }
   ],
   "source": [
    "# Load SMILES strings\n",
    "with open('dataset/train.txt', 'r') as f:\n",
    "    smiles_train = [line.strip() for line in f]\n",
    "\n",
    "with open('dataset/test.txt', 'r') as f:\n",
    "    smiles_test = [line.strip() for line in f]\n",
    "\n",
    "smiles_train = load_smiles_from_csv('dataset/train.txt', split_type='train')\n",
    "smiles_test = load_smiles_from_csv('dataset/test.txt', split_type='test')  # if test rows are in same file\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SMILESDataset(smiles_train, max_length=50, char_to_idx=char_to_idx)\n",
    "test_dataset = SMILESDataset(smiles_test, max_length=50, char_to_idx=char_to_idx)\n",
    "print(\"Training Vocabulary Size:\", train_dataset.vocab_size)\n",
    "print(\"Test Vocabulary Size:\", test_dataset.vocab_size) # Should be the same\n",
    "\n",
    "\n",
    "print(f\"# Train SMILES after filtering: {len(train_dataset)}\")\n",
    "print(f\"# Test SMILES after filtering: {len(test_dataset)}\")\n",
    "# train_dataset = SMILESDataset(smiles_train)\n",
    "# test_dataset = SMILESDataset(smiles_test, char_to_idx=train_dataset.char_to_idx)  # Share vocabulary\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)  # No need to shuffle test data\n",
    "\n",
    "print(f\"Number of batches in train_loader: {len(train_loader)}\")\n",
    "print(f\"Number of batches in test_loader: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91ba139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 51\n",
      "max_length: 50\n",
      "Input dim: 2550\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[155], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m vae\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Set model to training mode\u001b[39;00m\n\u001b[0;32m     23\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_loader:  \u001b[38;5;66;03m# Iterate over training data\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Flatten the input here before passing to the model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\valde\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\valde\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\valde\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\valde\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[143], line 28\u001b[0m, in \u001b[0;36mSMILESDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     26\u001b[0m smiles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmiles_list[idx]\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# One-hot encode the SMILES string\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m encoded \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, char \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(smiles[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length]):\n\u001b[0;32m     30\u001b[0m     encoded[i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchar_to_idx[char]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the VAE model\n",
    "input_dim = train_dataset.vocab_size * train_dataset.max_length  # Flatten the input (max_length x vocab_size)\n",
    "latent_dim = 128\n",
    "\n",
    "vocab_size = train_dataset.vocab_size\n",
    "max_length = train_dataset.max_length\n",
    "\n",
    "print(\"Vocab size:\", train_dataset.vocab_size)\n",
    "print(\"max_length:\", train_dataset.max_length)\n",
    "print(\"Input dim:\", input_dim)\n",
    "\n",
    "vae = VAE(input_dim, latent_dim)\n",
    "vae.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=0.0001)\n",
    "\n",
    "# Training and Evaluation loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    vae.train()  # Set model to training mode\n",
    "    train_loss = 0\n",
    "    for data in train_loader:  # Iterate over training data\n",
    "        optimizer.zero_grad()\n",
    "        # Flatten the input here before passing to the model\n",
    "        data = data.view(-1, input_dim).to(device)\n",
    "        recon_batch, mu, logvar = vae(data)\n",
    "        loss = vae_loss(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/len(train_loader)}')\n",
    "\n",
    "    # Evaluation on test set\n",
    "    vae.eval()  # Set model to evaluation mode\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():  # No need to calculate gradients during evaluation\n",
    "        for data in test_loader:  # Iterate over test data\n",
    "            # Flatten the input here as well\n",
    "            data = data.view(-1, input_dim).to(device)\n",
    "            recon_batch, mu, logvar = vae(data)\n",
    "            loss = vae_loss(recon_batch, data, mu, logvar)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Test Loss: {test_loss/len(test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2dd695f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new molecule from VAE by sampling from the latent space\n",
    "def generate_smiles(model, latent_dim=64, idx_to_char=None, temperature=1.0):\n",
    "    z = torch.randn(1, latent_dim).to(model.fc1.weight.device)  # Ensure z is on the same device as the model\n",
    "    with torch.no_grad():\n",
    "        generated = model.decode(z)  # Use decode instead of decoder\n",
    "    # Add postprocessing to convert to SMILES\n",
    "    # generated_tokens_indices = torch.argmax(generated, dim=-1).cpu().numpy().flatten()\n",
    "    probs = F.softmax(generated / temperature, dim=-1)\n",
    "\n",
    "    # Sample the next character from the probability distribution\n",
    "    generated_tokens_indices = torch.multinomial(probs, 1).cpu().numpy().flatten()\n",
    "    \n",
    "    # Print generated tokens and indices for debugging\n",
    "    print(\"Generated tokens indices:\", generated_tokens_indices)\n",
    "    print(\"Generated tokens:\", [idx_to_char.get(i, \"<UNK>\") for i in generated_tokens_indices])\n",
    "    \n",
    "    # Iterate through indices to build the SMILES string\n",
    "    generated_smiles = \"\".join([idx_to_char.get(i, \"\") for i in generated_tokens_indices])\n",
    "\n",
    "    return generated_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "bc567905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tokens indices: [742]\n",
      "Generated tokens: ['<UNK>']\n",
      "Generated SMILES: \n"
     ]
    }
   ],
   "source": [
    "# Generate a new molecule from VAE by sampling from the latent space\n",
    "generated_smiles = generate_smiles(vae, latent_dim, train_dataset.idx_to_char)  # pass idx_to_char\n",
    "\n",
    "print(f\"Generated SMILES: {generated_smiles}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
