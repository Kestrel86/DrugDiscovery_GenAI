{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e53c5634",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e53c5634",
        "outputId": "99468f8d-7385-4b22-811b-284949fd27e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCloning into 'moses'...\n",
            "remote: Enumerating objects: 1957, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 1957 (delta 0), reused 2 (delta 0), pack-reused 1953 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1957/1957), 164.05 MiB | 46.62 MiB/s, done.\n",
            "Resolving deltas: 100% (1068/1068), done.\n",
            "Filtering content: 100% (68/68), 323.72 MiB | 129.64 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!pip install rdkit-pypi -qqq\n",
        "!git clone https://github.com/molecularsets/moses.git\n",
        "\n",
        "import matplotlib as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4d9d22e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d9d22e9",
        "outputId": "83fc6c9d-53d6-47ae-cf44-22ae33d0a058"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                   SMILES  SPLIT\n",
            "0  CCCS(=O)c1ccc2[nH]c(=NC(=O)OC)[nH]c2c1  train\n",
            "1    CC(C)(C)C(=O)C(Oc1ccc(Cl)cc1)n1ccnc1  train\n",
            "2     Cc1c(Cl)cccc1Nc1ncccc1C(=O)OCC(O)CO  train\n",
            "3        Cn1cnc2c1c(=O)n(CC(O)CO)c(=O)n2C  train\n",
            "4          CC1Oc2ccc(Cl)cc2N(CC(O)CO)C1=O  train\n",
            "(1584663, 2)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('moses/data/train.csv')\n",
        "print(df.head())\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e64afbfd",
      "metadata": {
        "id": "e64afbfd"
      },
      "outputs": [],
      "source": [
        "# VALID_CHARS = list(\"@=#$()%1234567890abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ[]\\\\+-/.:\")\n",
        "# char_to_idx = {c: i for i, c in enumerate(VALID_CHARS)}\n",
        "# idx_to_char = {i: c for c, i in char_to_idx.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "062ff3ba",
      "metadata": {
        "id": "062ff3ba"
      },
      "outputs": [],
      "source": [
        "# Dataset class for SMILES strings\n",
        "class SMILESDataset(Dataset):\n",
        "    def __init__(self, smiles_list, max_length=150, char_to_idx=None):\n",
        "        self.smiles_list = smiles_list\n",
        "        self.max_length = max_length\n",
        "\n",
        "        if char_to_idx is None:\n",
        "            raise ValueError(\"Please provide a fixed character-to-index mapping\")\n",
        "            # self.char_to_idx, self.idx_to_char = build_vocabulary(smiles_list)\n",
        "        else:\n",
        "            self.char_to_idx = char_to_idx\n",
        "            self.idx_to_char = {v: k for k, v in char_to_idx.items()}\n",
        "\n",
        "        self.vocab_size = len(self.char_to_idx)\n",
        "\n",
        "        original_count = len(smiles_list)\n",
        "        filtered = []\n",
        "        invalid_count = 0\n",
        "\n",
        "        for s in smiles_list:\n",
        "            s = s.strip()\n",
        "            if all(c in self.char_to_idx for c in s):\n",
        "                filtered.append(s)\n",
        "            else:\n",
        "                invalid_count += 1\n",
        "        print(f\"Total: {original_count}, Valid: {len(filtered)}, Invalid: {invalid_count}\")\n",
        "        self.smiles_list = filtered\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.smiles_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        smiles = self.smiles_list[idx]\n",
        "        # One-hot encode the SMILES string\n",
        "        encoded = torch.zeros(self.max_length, self.vocab_size)\n",
        "        for i, char in enumerate(smiles[:self.max_length]):\n",
        "            encoded[i, self.char_to_idx[char]] = 1.0\n",
        "\n",
        "        # # Pad with zeros\n",
        "        # if len(smiles) < self.max_length:\n",
        "        #     encoded[len(smiles):, :] = 0.0\n",
        "\n",
        "        return encoded.view(-1) #Flatten into 1D tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "63e2abef",
      "metadata": {
        "id": "63e2abef"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "\n",
        "        # Encoder\n",
        "        self.fc1 = nn.Linear(input_dim, 1024)  # Increased input layer size\n",
        "        self.fc2 = nn.Linear(1024, 512)  # Added an extra hidden layer\n",
        "        self.fc3 = nn.Linear(512, 256)  # Added another extra hidden layer for more complexity\n",
        "        self.fc21 = nn.Linear(256, latent_dim)  # Mean of latent distribution\n",
        "        self.fc22 = nn.Linear(256, latent_dim)  # Log variance of latent distribution\n",
        "\n",
        "        # Decoder\n",
        "        self.fc4 = nn.Linear(latent_dim, 256)  # Mirroring Encoder structure\n",
        "        self.fc5 = nn.Linear(256, 512) # Mirroring Encoder structure\n",
        "        self.fc6 = nn.Linear(512, 1024) # Mirroring Encoder structure\n",
        "        self.fc7 = nn.Linear(1024, input_dim)  # Output layer\n",
        "\n",
        "    # Note for later, changed the architecture to add dropout\n",
        "    def encode(self, x):\n",
        "        h1 = self.dropout(F.relu(self.fc1(x)))\n",
        "        h2 = self.dropout(F.relu(self.fc2(h1)))\n",
        "        h3 = self.dropout(F.relu(self.fc3(h2)))\n",
        "        return self.fc21(h3), self.fc22(h3)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h4 = self.dropout(F.relu(self.fc4(z)))\n",
        "        h5 = self.dropout(F.relu(self.fc5(h4)))\n",
        "        h6 = self.dropout(F.relu(self.fc6(h5)))\n",
        "        return torch.sigmoid(self.fc7(h6))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, self.input_dim))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "feacf2b0",
      "metadata": {
        "id": "feacf2b0"
      },
      "outputs": [],
      "source": [
        "def vae_loss(recon_x, x, mu, logvar, beta=0.01):\n",
        "    BCE = F.binary_cross_entropy_with_logits(recon_x, x.view(-1, recon_x.size(1)), reduction='mean')\n",
        "    # mean seemed to do better\n",
        "    KL = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KL\n",
        "\n",
        "    #return BCE + beta * KL\n",
        "# add KL annealing factor to hlelp in slowing learning and avoid KL divergence dominating loss early?\n",
        "# or binary cross entropy with logits to handle the loss\n",
        "# binary_cross_entropy vs BCE with logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1c227beb",
      "metadata": {
        "id": "1c227beb"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def load_smiles_from_csv(path, split_type='train'):\n",
        "    smiles = []\n",
        "    with open(path, 'r') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            if row['SPLIT'].strip().lower() == split_type:\n",
        "                smiles.append(row['SMILES'].strip())\n",
        "    return smiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "2bd0c0a6",
      "metadata": {
        "id": "2bd0c0a6"
      },
      "outputs": [],
      "source": [
        "def extract_unique_chars(smiles_list):\n",
        "    unique_chars = set()\n",
        "    for smiles in smiles_list:\n",
        "        unique_chars.update(smiles.strip())\n",
        "    return sorted(unique_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "973932b4",
      "metadata": {
        "id": "973932b4"
      },
      "outputs": [],
      "source": [
        "def clean_smiles(smiles):\n",
        "    # Remove unwanted metadata like \",train\" or \",SPLIT\"\n",
        "    return smiles.split(',')[0].strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "2a34a1e3",
      "metadata": {
        "id": "2a34a1e3"
      },
      "outputs": [],
      "source": [
        "def decode_smiles(one_hot_tensor, idx_to_char):\n",
        "    smiles = ''\n",
        "    one_hot_tensor = one_hot_tensor.view(-1, len(idx_to_char))  # unflatten\n",
        "    for row in one_hot_tensor:\n",
        "        idx = row.argmax().item()\n",
        "        smiles += idx_to_char[idx]\n",
        "    return smiles.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "bb2c250f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb2c250f",
        "outputId": "c2a6df0f-7de4-40ad-ef34-ec76d8b44e03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique characters: 29\n",
            "Unique characters in dataset:\n",
            "['#', '(', ')', '-', '1', '2', '3', '4', '5', '=', 'B', 'C', 'E', 'F', 'H', 'I', 'L', 'M', 'N', 'O', 'S', '[', ']', 'c', 'l', 'n', 'o', 'r', 's']\n",
            "Total: 1000, Valid: 1000, Invalid: 0\n",
            "Total: 1000, Valid: 1000, Invalid: 0\n",
            "Training Vocabulary Size: 29\n",
            "Test Vocabulary Size: 29\n",
            "# Train SMILES after filtering: 1000\n",
            "# Test SMILES after filtering: 1000\n",
            "Number of batches in train_loader: 125\n",
            "Number of batches in test_loader: 125\n"
          ]
        }
      ],
      "source": [
        "# normalize the data?\n",
        "\n",
        "# Load SMILES strings\n",
        "with open('moses/data/train.csv', 'r') as f:\n",
        "    smiles_train = [line.strip() for line in f]\n",
        "\n",
        "with open('moses/data/train.csv', 'r') as f:\n",
        "    smiles_test = [line.strip() for line in f]\n",
        "\n",
        "# Apply cleaning to your SMILES\n",
        "smiles_train = [clean_smiles(smiles) for smiles in smiles_train]\n",
        "smiles_test = [clean_smiles(smiles) for smiles in smiles_test]\n",
        "smiles_train = smiles_train[:1000]\n",
        "smiles_test = smiles_test[:1000]\n",
        "\n",
        "# smiles_train = load_smiles_from_csv('dataset/train.txt', split_type='train')\n",
        "# smiles_test = load_smiles_from_csv('dataset/test.txt', split_type='test')  # if test rows are in same file\n",
        "\n",
        "# print(f\"Raw SMILES loaded: train={len(smiles_train)}, test={len(smiles_test)}\")\n",
        "all_smiles = smiles_train + smiles_test\n",
        "unique_chars = extract_unique_chars(all_smiles)\n",
        "\n",
        "print(f\"Total unique characters: {len(unique_chars)}\")\n",
        "print(\"Unique characters in dataset:\")\n",
        "print(unique_chars)\n",
        "\n",
        "# Use extracted unique characters to rebuild vocabulary\n",
        "VALID_CHARS = unique_chars\n",
        "char_to_idx = {c: i for i, c in enumerate(VALID_CHARS)}\n",
        "idx_to_char = {i: c for c, i in char_to_idx.items()}\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SMILESDataset(smiles_train, max_length=150, char_to_idx=char_to_idx)\n",
        "test_dataset = SMILESDataset(smiles_test, max_length=150, char_to_idx=char_to_idx)\n",
        "print(\"Training Vocabulary Size:\", train_dataset.vocab_size)\n",
        "print(\"Test Vocabulary Size:\", test_dataset.vocab_size) # Should be the same\n",
        "\n",
        "\n",
        "print(f\"# Train SMILES after filtering: {len(train_dataset)}\")\n",
        "print(f\"# Test SMILES after filtering: {len(test_dataset)}\")\n",
        "# train_dataset = SMILESDataset(smiles_train)\n",
        "# test_dataset = SMILESDataset(smiles_test, char_to_idx=train_dataset.char_to_idx)  # Share vocabulary\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)  # No need to shuffle test data\n",
        "\n",
        "print(f\"Number of batches in train_loader: {len(train_loader)}\")\n",
        "print(f\"Number of batches in test_loader: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "a8590f4b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8590f4b",
        "outputId": "dbae4a24-9ed9-4c65-b5da-921cbf754344"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "# Check a batch of data\n",
        "for i, data in enumerate(train_loader):\n",
        "    if i == 0:  # Just visualize the first batch\n",
        "        print(data)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8db7c97f",
      "metadata": {
        "id": "8db7c97f"
      },
      "outputs": [],
      "source": [
        "# import json\n",
        "\n",
        "# # Save vocab to JSON\n",
        "# vocab_path = 'char_vocab.json'\n",
        "# with open(vocab_path, 'w') as f:\n",
        "#     json.dump({\n",
        "#         'char_to_idx': char_to_idx,\n",
        "#         'idx_to_char': idx_to_char\n",
        "#     }, f)\n",
        "\n",
        "# print(f\"Vocabulary saved to {vocab_path}\")\n",
        "\n",
        "# call with code below\n",
        "\n",
        "# with open('char_vocab.json', 'r') as f:\n",
        "#     vocab = json.load(f)\n",
        "#     char_to_idx = vocab['char_to_idx']\n",
        "#     idx_to_char = {int(k): v for k, v in vocab['idx_to_char'].items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "238e32e3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "238e32e3",
        "outputId": "bda42794-0a52-4223-8f43-4d9a3576ee19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample SMILES visualizations:\n",
            "\n",
            "Sample 1\n",
            "Original : SMILES\n",
            "Decoded  : SMILES############################################\n",
            "Shape    : torch.Size([1400])\n",
            "\n",
            "Sample 2\n",
            "Original : CCCS(=O)c1ccc2[nH]c(=NC(=O)OC)[nH]c2c1\n",
            "Decoded  : CCCS(=O)c1ccc2[nH]c(=NC(=O)OC)[nH]c2c1############\n",
            "Shape    : torch.Size([1400])\n",
            "\n",
            "Sample 3\n",
            "Original : CC(C)(C)C(=O)C(Oc1ccc(Cl)cc1)n1ccnc1\n",
            "Decoded  : CC(C)(C)C(=O)C(Oc1ccc(Cl)cc1)n1ccnc1##############\n",
            "Shape    : torch.Size([1400])\n"
          ]
        }
      ],
      "source": [
        "# Visualize 3 samples\n",
        "print(\"\\nSample SMILES visualizations:\")\n",
        "for i in range(3):\n",
        "    encoded = train_dataset[i]\n",
        "    original = train_dataset.smiles_list[i]\n",
        "    decoded = decode_smiles(encoded, train_dataset.idx_to_char)\n",
        "\n",
        "    print(f\"\\nSample {i+1}\")\n",
        "    print(f\"Original : {original}\")\n",
        "    print(f\"Decoded  : {decoded}\")\n",
        "    print(f\"Shape    : {encoded.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "c91ba139",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c91ba139",
        "outputId": "f179bd24-e0c7-4a0a-b87a-f81b92ee383e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 29\n",
            "max_length: 150\n",
            "Input dim: 4350\n",
            "Epoch [1/100], Train Loss: 0.9636143875122071\n",
            "Epoch [1/100], Test Loss: 0.9513521523475647\n",
            "Epoch [2/100], Train Loss: 0.9013526678085327\n",
            "Epoch [2/100], Test Loss: 0.8325095505714416\n",
            "Epoch [3/100], Train Loss: 0.7607978458404541\n",
            "Epoch [3/100], Test Loss: 0.7203379073143006\n",
            "Epoch [4/100], Train Loss: 0.708881446838379\n",
            "Epoch [4/100], Test Loss: 0.7023229832649232\n",
            "Epoch [5/100], Train Loss: 0.6996560006141662\n",
            "Epoch [5/100], Test Loss: 0.6976593766212463\n",
            "Epoch [6/100], Train Loss: 0.6966817178726197\n",
            "Epoch [6/100], Test Loss: 0.6959185361862182\n",
            "Epoch [7/100], Train Loss: 0.6954539747238159\n",
            "Epoch [7/100], Test Loss: 0.6948322358131409\n",
            "Epoch [8/100], Train Loss: 0.6946798934936523\n",
            "Epoch [8/100], Test Loss: 0.6943077569007874\n",
            "Epoch [9/100], Train Loss: 0.694220862865448\n",
            "Epoch [9/100], Test Loss: 0.6939542136192322\n",
            "Epoch [10/100], Train Loss: 0.6939697327613831\n",
            "Epoch [10/100], Test Loss: 0.6937140865325928\n",
            "Epoch [11/100], Train Loss: 0.6937538285255432\n",
            "Epoch [11/100], Test Loss: 0.6935870051383972\n",
            "Epoch [12/100], Train Loss: 0.6936467609405518\n",
            "Epoch [12/100], Test Loss: 0.693495240688324\n",
            "Epoch [13/100], Train Loss: 0.6935457744598389\n",
            "Epoch [13/100], Test Loss: 0.6934208126068115\n",
            "Epoch [14/100], Train Loss: 0.6934798789024353\n",
            "Epoch [14/100], Test Loss: 0.6933490395545959\n",
            "Epoch [15/100], Train Loss: 0.6934177374839783\n",
            "Epoch [15/100], Test Loss: 0.6933180437088012\n",
            "Epoch [16/100], Train Loss: 0.693394732952118\n",
            "Epoch [16/100], Test Loss: 0.6932764759063721\n",
            "Epoch [17/100], Train Loss: 0.6933516178131104\n",
            "Epoch [17/100], Test Loss: 0.6932545437812805\n",
            "Epoch [18/100], Train Loss: 0.6933140449523926\n",
            "Epoch [18/100], Test Loss: 0.6932312569618225\n",
            "Epoch [19/100], Train Loss: 0.6932926573753357\n",
            "Epoch [19/100], Test Loss: 0.6932152709960937\n",
            "Epoch [20/100], Train Loss: 0.6932761387825013\n",
            "Epoch [20/100], Test Loss: 0.6932004017829895\n",
            "Epoch [21/100], Train Loss: 0.6932500724792481\n",
            "Epoch [21/100], Test Loss: 0.6931874399185181\n",
            "Epoch [22/100], Train Loss: 0.6932482037544251\n",
            "Epoch [22/100], Test Loss: 0.6931772956848145\n",
            "Epoch [23/100], Train Loss: 0.6932304463386536\n",
            "Epoch [23/100], Test Loss: 0.6931708807945252\n",
            "Epoch [24/100], Train Loss: 0.6932200603485107\n",
            "Epoch [24/100], Test Loss: 0.6931674380302429\n",
            "Epoch [25/100], Train Loss: 0.6932120304107666\n",
            "Epoch [25/100], Test Loss: 0.693162024974823\n",
            "Epoch [26/100], Train Loss: 0.6932074661254883\n",
            "Epoch [26/100], Test Loss: 0.6931569185256958\n",
            "Epoch [27/100], Train Loss: 0.6931976108551026\n",
            "Epoch [27/100], Test Loss: 0.6931519012451172\n",
            "Epoch [28/100], Train Loss: 0.6931898975372315\n",
            "Epoch [28/100], Test Loss: 0.6931479716300964\n",
            "Epoch [29/100], Train Loss: 0.6931882014274597\n",
            "Epoch [29/100], Test Loss: 0.6931451182365418\n",
            "Epoch [30/100], Train Loss: 0.6931808300018311\n",
            "Epoch [30/100], Test Loss: 0.6931414589881897\n",
            "Epoch [31/100], Train Loss: 0.6931785240173339\n",
            "Epoch [31/100], Test Loss: 0.6931365575790406\n",
            "Epoch [32/100], Train Loss: 0.6931711735725403\n",
            "Epoch [32/100], Test Loss: 0.6931361999511719\n",
            "Epoch [33/100], Train Loss: 0.6931691040992737\n",
            "Epoch [33/100], Test Loss: 0.6931344437599182\n",
            "Epoch [34/100], Train Loss: 0.693163878917694\n",
            "Epoch [34/100], Test Loss: 0.6931322040557861\n",
            "Epoch [35/100], Train Loss: 0.6931611180305481\n",
            "Epoch [35/100], Test Loss: 0.6931313366889954\n",
            "Epoch [36/100], Train Loss: 0.6931578660011292\n",
            "Epoch [36/100], Test Loss: 0.6931290550231933\n",
            "Epoch [37/100], Train Loss: 0.6931559085845947\n",
            "Epoch [37/100], Test Loss: 0.6931289176940918\n",
            "Epoch [38/100], Train Loss: 0.6931537027359009\n",
            "Epoch [38/100], Test Loss: 0.6931270751953125\n",
            "Epoch [39/100], Train Loss: 0.6931515669822693\n",
            "Epoch [39/100], Test Loss: 0.6931263923645019\n",
            "Epoch [40/100], Train Loss: 0.693150191783905\n",
            "Epoch [40/100], Test Loss: 0.693125750541687\n",
            "Epoch [41/100], Train Loss: 0.693146632194519\n",
            "Epoch [41/100], Test Loss: 0.6931247506141662\n",
            "Epoch [42/100], Train Loss: 0.6931451435089111\n",
            "Epoch [42/100], Test Loss: 0.6931242699623108\n",
            "Epoch [43/100], Train Loss: 0.6931442155838012\n",
            "Epoch [43/100], Test Loss: 0.6931227979660034\n",
            "Epoch [44/100], Train Loss: 0.6931431379318237\n",
            "Epoch [44/100], Test Loss: 0.693122483253479\n",
            "Epoch [45/100], Train Loss: 0.6931421008110047\n",
            "Epoch [45/100], Test Loss: 0.6931229424476624\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-add2cebf7c02>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Iterate over training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Flatten the input here before passing to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m             )\n\u001b[1;32m    744\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    973\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mset_to_none\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m                             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    976\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate the VAE model\n",
        "input_dim = train_dataset.vocab_size * train_dataset.max_length  # Flatten the input (max_length x vocab_size)\n",
        "latent_dim = 128\n",
        "\n",
        "vocab_size = train_dataset.vocab_size\n",
        "max_length = train_dataset.max_length\n",
        "\n",
        "print(\"Vocab size:\", train_dataset.vocab_size)\n",
        "print(\"max_length:\", train_dataset.max_length)\n",
        "print(\"Input dim:\", input_dim)\n",
        "\n",
        "vae = VAE(input_dim, latent_dim)\n",
        "vae.to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-5)\n",
        "\n",
        "# # Learning rate scheduler\n",
        "# scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "# Training and Evaluation loop\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    vae.train()  # Set model to training mode\n",
        "    train_loss = 0\n",
        "    for data in train_loader:  # Iterate over training data\n",
        "        optimizer.zero_grad()\n",
        "        # Flatten the input here before passing to the model\n",
        "        data = data.view(-1, input_dim).to(device)\n",
        "        recon_batch, mu, logvar = vae(data)\n",
        "        #print(f\"Reconstructed output: {recon_batch[:5]}\") # testing\n",
        "        #break\n",
        "        loss = vae_loss(recon_batch, data, mu, logvar)\n",
        "        #print(f\"batch loss: {loss.item()}\") # for testing\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/len(train_loader)}')\n",
        "\n",
        "    # Evaluation on test set\n",
        "    vae.eval()  # Set model to evaluation mode\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():  # No need to calculate gradients during evaluation\n",
        "        for data in test_loader:  # Iterate over test data\n",
        "            # Flatten the input here as well\n",
        "            data = data.view(-1, input_dim).to(device)\n",
        "            recon_batch, mu, logvar = vae(data)\n",
        "            loss = vae_loss(recon_batch, data, mu, logvar)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Test Loss: {test_loss/len(test_loader)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dd695f2",
      "metadata": {
        "id": "2dd695f2"
      },
      "outputs": [],
      "source": [
        "# Generate a new molecule from VAE by sampling from the latent space\n",
        "def generate_smiles(model, latent_dim=64, idx_to_char=None, temperature=1.0):\n",
        "    z = torch.randn(1, latent_dim).to(model.fc1.weight.device)  # Ensure z is on the same device as the model\n",
        "    with torch.no_grad():\n",
        "        generated = model.decode(z)  # Use decode instead of decoder\n",
        "    # Add postprocessing to convert to SMILES\n",
        "    # generated_tokens_indices = torch.argmax(generated, dim=-1).cpu().numpy().flatten()\n",
        "    probs = F.softmax(generated / temperature, dim=-1)\n",
        "\n",
        "    # Sample the next character from the probability distribution\n",
        "    generated_tokens_indices = torch.multinomial(probs, 1).cpu().numpy().flatten()\n",
        "\n",
        "    # Print generated tokens and indices for debugging\n",
        "    print(\"Generated tokens indices:\", generated_tokens_indices)\n",
        "    print(\"Generated tokens:\", [idx_to_char.get(i, \"<UNK>\") for i in generated_tokens_indices])\n",
        "\n",
        "    # Iterate through indices to build the SMILES string\n",
        "    generated_smiles = \"\".join([idx_to_char.get(i, \"\") for i in generated_tokens_indices])\n",
        "\n",
        "    return generated_smiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc567905",
      "metadata": {
        "id": "bc567905",
        "outputId": "5cf8081b-b923-41da-9222-01c94fa9f672"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated tokens indices: [2329]\n",
            "Generated tokens: ['<UNK>']\n",
            "Generated SMILES: \n"
          ]
        }
      ],
      "source": [
        "# Generate a new molecule from VAE by sampling from the latent space\n",
        "generated_smiles = generate_smiles(vae, latent_dim, train_dataset.idx_to_char)  # pass idx_to_char\n",
        "\n",
        "print(f\"Generated SMILES: {generated_smiles}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}