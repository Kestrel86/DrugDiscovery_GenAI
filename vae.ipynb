{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e53c5634",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "e53c5634",
        "outputId": "61bc9203-bde6-4269-978f-c60c57cb1128"
      },
      "outputs": [],
      "source": [
        "import matplotlib as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from rdkit import Chem\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1c227beb",
      "metadata": {
        "id": "1c227beb"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def load_smiles_from_csv(path, split_type='train'):\n",
        "    '''\n",
        "    Loads SMILES strings from a CSV file.\n",
        "\n",
        "    Args:\n",
        "        path (str): Path to the CSV file\n",
        "        split_type (str): Split type ('train' or 'test')\n",
        "\n",
        "    Returns:\n",
        "        list: List of SMILES strings\n",
        "    '''\n",
        "    smiles = []\n",
        "    with open(path, 'r') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            if row['SPLIT'].strip().lower() == split_type:\n",
        "                smiles.append(row['SMILES'].strip())\n",
        "    return smiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "QFYVVczcKLoV",
      "metadata": {
        "id": "QFYVVczcKLoV"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Functions are from the RNN model we have but not entirely sure where they would fit in the VAE\n",
        "Currently working on implementation, the process_smiles function may help in creating valid molecules\n",
        "'''\n",
        "\n",
        "# Function to add start and end tokens\n",
        "def process_smiles(smiles_list):\n",
        "    return [\"^\" + s + \"$\" for s in smiles_list]\n",
        "\n",
        "# Create character dictionaries including special tokens\n",
        "def create_vocab(smiles_list):\n",
        "    all_chars = sorted(list(set(''.join(smiles_list))))\n",
        "    char2idx = {ch: i + 1 for i, ch in enumerate(all_chars)}\n",
        "    char2idx[''] = 0  # Padding token\n",
        "    idx2char = {i: ch for ch, i in char2idx.items()}\n",
        "    return char2idx, idx2char, len(char2idx)\n",
        "\n",
        "# Enhanced tokenization\n",
        "def tokenize(smiles, char2idx):\n",
        "    return [char2idx.get(c, 0) for c in smiles]  # Default to 0 if unknown\n",
        "\n",
        "def detokenize(tokens, idx2char):\n",
        "    return ''.join([idx2char.get(t, '') for t in tokens if t != 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2bd0c0a6",
      "metadata": {
        "id": "2bd0c0a6"
      },
      "outputs": [],
      "source": [
        "def extract_unique_chars(smiles_list):\n",
        "    '''\n",
        "    Extracts unique characters from a list of SMILES strings.\n",
        "\n",
        "    Args:\n",
        "        smiles_list (list): List of SMILES strings\n",
        "\n",
        "    Returns:\n",
        "        list: List of unique characters\n",
        "    '''\n",
        "    unique_chars = set()\n",
        "    for smiles in smiles_list:\n",
        "        unique_chars.update(smiles.strip())\n",
        "    return sorted(unique_chars)\n",
        "\n",
        "def clean_smiles(smiles):\n",
        "    '''\n",
        "    Cleans a SMILES string by removing unwanted characters.\n",
        "\n",
        "    Args:\n",
        "        smiles (str): SMILES string\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned SMILES string\n",
        "    '''\n",
        "    # Remove unwanted metadata and special characters\n",
        "    cleaned = smiles.split(',')[0].strip()\n",
        "    cleaned = cleaned.replace('#', '')  # Remove '#' characters\n",
        "    cleaned = cleaned.replace('$', '')  # Remove '$' characters\n",
        "    cleaned = cleaned.replace('^', '')  # Remove '^' characters if present\n",
        "    return cleaned\n",
        "\n",
        "def decode_smiles(one_hot_tensor, idx_to_char):\n",
        "    '''\n",
        "    Decodes a one-hot encoded tensor back to SMILES.\n",
        "\n",
        "    Args:\n",
        "        one_hot_tensor (torch.Tensor): One-hot encoded tensor\n",
        "    '''\n",
        "    smiles = ''\n",
        "    one_hot_tensor = one_hot_tensor.view(-1, len(idx_to_char))\n",
        "    for row in one_hot_tensor:\n",
        "        idx = row.argmax().item()\n",
        "        char = idx_to_char[idx]\n",
        "        if char == '$' and len(smiles) > 0:  # Stop at first $ after content\n",
        "            break\n",
        "        smiles += char\n",
        "    return smiles.strip()\n",
        "\n",
        "def verify_smiles(smiles):\n",
        "  '''\n",
        "  Verifies the validity of a SMILES string using RDKit.\n",
        "\n",
        "  Args:\n",
        "      smiles (str): SMILES string to verify\n",
        "\n",
        "  Returns:\n",
        "      bool: True if valid, False otherwise\n",
        "  '''\n",
        "  mol = Chem.MolFromSmiles(smiles)\n",
        "  return mol is not None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2dd695f2",
      "metadata": {
        "id": "2dd695f2"
      },
      "outputs": [],
      "source": [
        "# Generate a new molecule from VAE by sampling from the latent space\n",
        "def generate_smiles(model, latent_dim, idx_to_char, temperature=1.0):\n",
        "    '''\n",
        "    Generates a new SMILES string by sampling from the VAE's latent space.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): VAE model\n",
        "        latent_dim (int): Dimension of the latent space\n",
        "    '''\n",
        "    z = torch.randn(1, latent_dim).to(model.fc1.weight.device)  # Ensure z is on the same device as the model\n",
        "    with torch.no_grad():\n",
        "        generated = model.decode(z)\n",
        "\n",
        "    # Add postprocessing to convert to SMILES\n",
        "    probs = F.softmax(generated.view(max_length, vocab_size) / temperature, dim=-1)\n",
        "\n",
        "    # Sample the next character from the probability distribution\n",
        "    generated_tokens_indices = torch.multinomial(probs, 1).cpu().numpy().flatten()\n",
        "\n",
        "    # Iterate through indices to build the SMILES string\n",
        "    generated_smiles = \"\".join([idx_to_char.get(i, \"\") for i in generated_tokens_indices])\n",
        "    generated_smiles = generated_smiles.replace('^', '').replace('$', '')\n",
        "\n",
        "    # Verification using rdkit\n",
        "    is_valid = verify_smiles(generated_smiles)\n",
        "    if is_valid:\n",
        "      return generated_smiles\n",
        "    else:\n",
        "      return \"INVALID\"\n",
        "\n",
        "    # return generated_smiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "062ff3ba",
      "metadata": {
        "id": "062ff3ba"
      },
      "outputs": [],
      "source": [
        "# Dataset class for SMILES strings\n",
        "\n",
        "# Contemplate Protein To Vector Encoding\n",
        "class SMILESDataset(Dataset):\n",
        "    def __init__(self, smiles_list, max_length=150, char_to_idx=None):\n",
        "        '''\n",
        "        Initializes the SMILESDataset with a list of SMILES strings.\n",
        "\n",
        "        Args:\n",
        "            smiles_list (list): List of SMILES strings\n",
        "            max_length (int): Maximum length of the SMILES strings\n",
        "            char_to_idx (dict): Character-to-index mapping\n",
        "\n",
        "        The dataset will one-hot encode each character in a SMILES string to a fixed-size tensor of shape (max_length * vocab_size).\n",
        "        If a SMILES string is shorter than max_length, it will be padded with zeros. If longer, it will be truncated.\n",
        "        '''\n",
        "        self.smiles_list = smiles_list\n",
        "        self.max_length = max_length\n",
        "\n",
        "        if char_to_idx is None:\n",
        "            raise ValueError(\"Please provide a fixed character-to-index mapping\")\n",
        "            # self.char_to_idx, self.idx_to_char = build_vocabulary(smiles_list)\n",
        "        else:\n",
        "            self.char_to_idx = char_to_idx\n",
        "            self.idx_to_char = {v: k for k, v in char_to_idx.items()}\n",
        "\n",
        "        self.vocab_size = len(self.char_to_idx)\n",
        "\n",
        "        original_count = len(smiles_list)\n",
        "        filtered = []\n",
        "        invalid_count = 0\n",
        "\n",
        "        for s in smiles_list:\n",
        "            s = s.strip()\n",
        "            if all(c in self.char_to_idx for c in s):\n",
        "                filtered.append(s)\n",
        "            else:\n",
        "                invalid_count += 1\n",
        "        print(f\"Total: {original_count}, Valid: {len(filtered)}, Invalid: {invalid_count}\")\n",
        "        self.smiles_list = filtered\n",
        "\n",
        "    def __len__(self):\n",
        "        '''\n",
        "        Returns:\n",
        "            int: Number of valid SMILES strings in the dataset\n",
        "        '''\n",
        "\n",
        "        return len(self.smiles_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        '''\n",
        "        Fetches the encoded version of a SMILES string at a given index.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the SMILES string to retrieve\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: One-hot encoded tensor of the SMILES string of shape (max_length * vocab_size)\n",
        "        '''\n",
        "\n",
        "        smiles = self.smiles_list[idx]\n",
        "\n",
        "        # One-hot encode the SMILES string\n",
        "        encoded = torch.zeros(self.max_length, self.vocab_size)\n",
        "        for i, char in enumerate(smiles[:self.max_length]):\n",
        "            encoded[i, self.char_to_idx[char]] = 1.0\n",
        "\n",
        "        return encoded.view(-1) #Flatten into 1D tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "oN5phZo8tZC1",
      "metadata": {
        "id": "oN5phZo8tZC1"
      },
      "outputs": [],
      "source": [
        "# Rework flow, completely all over the place with its math and needs to be changed with how Jacobian is used\n",
        "\n",
        "\n",
        "class Flow(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim, num_flows):\n",
        "        super(Flow, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_flows = num_flows\n",
        "\n",
        "\n",
        "        self.flows = nn.ModuleList([\n",
        "            AffineCouplingLayer(input_dim, latent_dim) for _ in range(num_flows)\n",
        "        ])\n",
        "    def forward(self, x):\n",
        "        log_det_jacobian = 0\n",
        "        for flow in self.flows:\n",
        "            x, ld = flow(x)\n",
        "            log_det_jacobian += ld\n",
        "        return x, log_det_jacobian\n",
        "\n",
        "\n",
        "# Invertibility (REALNVP)\n",
        "class AffineCouplingLayer(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim):\n",
        "        super(AffineCouplingLayer, self).__init__()\n",
        "        # Implement the affine coupling layer\n",
        "        self.input_dim = input_dim # added line\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim // 2, latent_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(latent_dim, input_dim // 2)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Split the input into two parts\n",
        "        x1, x2 = x[:, :self.input_dim // 2], x[:, self.input_dim // 2:]\n",
        "\n",
        "        # Apply the transformation to one part using the other part\n",
        "        log_s = self.net(x1)\n",
        "        t = self.net(x1)\n",
        "\n",
        "        # Calculate the transformed output\n",
        "        x2_transformed = x2 * torch.exp(log_s) + t\n",
        "\n",
        "        # Combine the transformed parts\n",
        "        transformed_x = torch.cat([x1, x2_transformed], dim=1)\n",
        "\n",
        "        # Calculate the log determinant of the Jacobian\n",
        "        log_det_jacobian = torch.sum(log_s, dim=1)\n",
        "\n",
        "        return transformed_x, log_det_jacobian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3HyjMRPVDcTs",
      "metadata": {
        "id": "3HyjMRPVDcTs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim, vocab_size):\n",
        "        super(VAE, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.fc1 = nn.Linear(input_dim, 256)\n",
        "        self.fc_mu = nn.Linear(256, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(256, latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.fc3 = nn.Linear(latent_dim, 256)\n",
        "        self.fc4 = nn.Linear(256, input_dim)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return self.fc_mu(h1), self.fc_logvar(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, self.input_dim))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "\n",
        "        # Pass z through flow model\n",
        "        #z, log_det_jacobian = flow_model(z)\n",
        "\n",
        "        recon_x = self.decode(z)\n",
        "        return recon_x, mu, logvar, #log_det_jacobian # jacobian is added"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "NrM1-GTTQ63G",
      "metadata": {
        "id": "NrM1-GTTQ63G"
      },
      "outputs": [],
      "source": [
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    \"\"\"\n",
        "    Compute VAE loss with proper reshaping.\n",
        "    \n",
        "    Args:\n",
        "        recon_x (torch.Tensor): Reconstructed input from VAE\n",
        "        x (torch.Tensor): Original input data\n",
        "        mu (torch.Tensor): Mean from encoder\n",
        "        logvar (torch.Tensor): Log variance from encoder\n",
        "        \n",
        "    Returns:\n",
        "        torch.Tensor: Combined loss (BCE + KLD)\n",
        "    \"\"\"\n",
        "    batch_size = x.size(0)\n",
        "    \n",
        "    # Keep the original shapes for BCE calculation\n",
        "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "    \n",
        "    # KL divergence\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    \n",
        "    # Weight factors (can be adjusted)\n",
        "    beta = 0.1  # Weight for KLD term\n",
        "    \n",
        "    return BCE + beta * KLD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "bb2c250f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb2c250f",
        "outputId": "14a29115-105f-4579-bbfb-d7a2758dcfe6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total unique characters: 31\n",
            "Unique characters in dataset:\n",
            "['$', '(', ')', '-', '1', '2', '3', '4', '5', '6', '=', 'B', 'C', 'E', 'F', 'H', 'I', 'L', 'M', 'N', 'O', 'S', '[', ']', '^', 'c', 'l', 'n', 'o', 'r', 's']\n",
            "Total: 100000, Valid: 100000, Invalid: 0\n",
            "Total: 100000, Valid: 100000, Invalid: 0\n",
            "Training Vocabulary Size: 31\n",
            "Test Vocabulary Size: 31\n",
            "# Train SMILES after filtering: 100000\n",
            "# Test SMILES after filtering: 100000\n",
            "Number of batches in train_loader: 12500\n",
            "Number of batches in test_loader: 12500\n"
          ]
        }
      ],
      "source": [
        "# Load SMILES strings\n",
        "with open('dataset/train.txt', 'r') as f:\n",
        "    smiles_train = [line.strip() for line in f][1:]\n",
        "\n",
        "with open('dataset/test.txt', 'r') as f:\n",
        "    smiles_test = [line.strip() for line in f]\n",
        "\n",
        "# Apply cleaning to your SMILES\n",
        "smiles_train = [clean_smiles(smiles) for smiles in smiles_train]\n",
        "smiles_test = [clean_smiles(smiles) for smiles in smiles_test]\n",
        "smiles_train = process_smiles(smiles_train)\n",
        "smiles_test = process_smiles(smiles_test)\n",
        "smiles_train = smiles_train[:100000]\n",
        "smiles_test = smiles_test[:100000]\n",
        "\n",
        "# smiles_train = load_smiles_from_csv('dataset/train.txt', split_type='train')\n",
        "# smiles_test = load_smiles_from_csv('dataset/test.txt', split_type='test')  # if test rows are in same file\n",
        "\n",
        "# print(f\"Raw SMILES loaded: train={len(smiles_train)}, test={len(smiles_test)}\") # output for testing purposes\n",
        "all_smiles = smiles_train + smiles_test\n",
        "unique_chars = extract_unique_chars(all_smiles)\n",
        "\n",
        "print(f\"Total unique characters: {len(unique_chars)}\")\n",
        "print(\"Unique characters in dataset:\")\n",
        "print(unique_chars)\n",
        "\n",
        "# Use extracted unique characters to rebuild vocabulary\n",
        "VALID_CHARS = unique_chars\n",
        "char_to_idx = {c: i for i, c in enumerate(VALID_CHARS)}\n",
        "idx_to_char = {i: c for c, i in char_to_idx.items()}\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SMILESDataset(smiles_train, max_length=150, char_to_idx=char_to_idx)\n",
        "test_dataset = SMILESDataset(smiles_test, max_length=150, char_to_idx=char_to_idx)\n",
        "print(\"Training Vocabulary Size:\", train_dataset.vocab_size)\n",
        "print(\"Test Vocabulary Size:\", test_dataset.vocab_size) # Should be the same\n",
        "\n",
        "\n",
        "print(f\"# Train SMILES after filtering: {len(train_dataset)}\")\n",
        "print(f\"# Test SMILES after filtering: {len(test_dataset)}\")\n",
        "# train_dataset = SMILESDataset(smiles_train)\n",
        "# test_dataset = SMILESDataset(smiles_test, char_to_idx=train_dataset.char_to_idx)  # Share vocabulary\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)  # No need to shuffle test data\n",
        "\n",
        "print(f\"Number of batches in train_loader: {len(train_loader)}\")\n",
        "print(f\"Number of batches in test_loader: {len(test_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a8590f4b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8590f4b",
        "outputId": "94bb0e95-a6c9-4808-ce3c-b270c9bb30e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "\n",
            "Sample SMILES visualizations:\n",
            "\n",
            "Sample 1\n",
            "Original : ^CCCS(=O)c1ccc2[nH]c(=NC(=O)OC)[nH]c2c1$\n",
            "Decoded  : ^CCCS(=O)c1ccc2[nH]c(=NC(=O)OC)[nH]c2c1\n",
            "Shape    : torch.Size([4650])\n",
            "\n",
            "Sample 2\n",
            "Original : ^CC(C)(C)C(=O)C(Oc1ccc(Cl)cc1)n1ccnc1$\n",
            "Decoded  : ^CC(C)(C)C(=O)C(Oc1ccc(Cl)cc1)n1ccnc1\n",
            "Shape    : torch.Size([4650])\n",
            "\n",
            "Sample 3\n",
            "Original : ^Cc1c(Cl)cccc1Nc1ncccc1C(=O)OCC(O)CO$\n",
            "Decoded  : ^Cc1c(Cl)cccc1Nc1ncccc1C(=O)OCC(O)CO\n",
            "Shape    : torch.Size([4650])\n"
          ]
        }
      ],
      "source": [
        "# Check a batch of data\n",
        "for i, data in enumerate(train_loader):\n",
        "    if i == 0:  # Just visualize the first batch\n",
        "        print(data)\n",
        "        break\n",
        "\n",
        "# Visualize 3 samples\n",
        "print(\"\\nSample SMILES visualizations:\")\n",
        "for i in range(3):\n",
        "    encoded = train_dataset[i]\n",
        "    original = train_dataset.smiles_list[i]\n",
        "    decoded = decode_smiles(encoded, train_dataset.idx_to_char)\n",
        "\n",
        "    print(f\"\\nSample {i+1}\")\n",
        "    print(f\"Original : {original}\")\n",
        "    print(f\"Decoded  : {decoded}\")\n",
        "    print(f\"Shape    : {encoded.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c91ba139",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "c91ba139",
        "outputId": "99e1556a-5688-4187-a223-eb9d4059c23c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Vocab size: 31\n",
            "max_length: 150\n",
            "Input dim: 4650\n",
            "Epoch [1/100], Train Loss: 1003.3841191113281\n",
            "Epoch [1/100], Test Loss: 648.727353071289\n",
            "Epoch [2/100], Train Loss: 512.0341269873047\n",
            "Epoch [2/100], Test Loss: 469.3863980725098\n",
            "Epoch [3/100], Train Loss: 390.4384926074219\n",
            "Epoch [3/100], Test Loss: 390.87121095336914\n",
            "Epoch [4/100], Train Loss: 330.06779743530274\n",
            "Epoch [4/100], Test Loss: 345.54019759521486\n",
            "Epoch [5/100], Train Loss: 293.3521489550781\n",
            "Epoch [5/100], Test Loss: 314.0687670703125\n",
            "Epoch [6/100], Train Loss: 266.9794173046875\n",
            "Epoch [6/100], Test Loss: 290.7090727893066\n",
            "Epoch [7/100], Train Loss: 247.60863052246094\n",
            "Epoch [7/100], Test Loss: 273.35651283325194\n",
            "Epoch [8/100], Train Loss: 232.37719386352538\n",
            "Epoch [8/100], Test Loss: 259.82488083007814\n",
            "Epoch [9/100], Train Loss: 220.2210998840332\n",
            "Epoch [9/100], Test Loss: 247.97930316833495\n",
            "Epoch [10/100], Train Loss: 210.05184823242186\n",
            "Epoch [10/100], Test Loss: 237.38002477233886\n",
            "Epoch [11/100], Train Loss: 200.92890161499022\n",
            "Epoch [11/100], Test Loss: 229.4120879779053\n",
            "Epoch [12/100], Train Loss: 193.19167901916504\n",
            "Epoch [12/100], Test Loss: 220.70722692871092\n",
            "Epoch [13/100], Train Loss: 186.37637881896973\n",
            "Epoch [13/100], Test Loss: 213.32184170959474\n",
            "Epoch [14/100], Train Loss: 180.2876052557373\n",
            "Epoch [14/100], Test Loss: 207.2167444824219\n",
            "Epoch [15/100], Train Loss: 175.21285503356933\n",
            "Epoch [15/100], Test Loss: 203.71350973388672\n",
            "Epoch [16/100], Train Loss: 170.6894811468506\n",
            "Epoch [16/100], Test Loss: 198.0743923260498\n",
            "Epoch [17/100], Train Loss: 166.80534777770995\n",
            "Epoch [17/100], Test Loss: 193.3878400274658\n",
            "Epoch [18/100], Train Loss: 163.09834369812012\n",
            "Epoch [18/100], Test Loss: 190.1306417614746\n",
            "Epoch [19/100], Train Loss: 159.73477131103516\n",
            "Epoch [19/100], Test Loss: 187.2909027105713\n",
            "Epoch [20/100], Train Loss: 156.85342801391602\n",
            "Epoch [20/100], Test Loss: 183.71794547729493\n",
            "Epoch [21/100], Train Loss: 154.2579712689209\n",
            "Epoch [21/100], Test Loss: 181.8212703942871\n",
            "Epoch [22/100], Train Loss: 151.92570427612304\n",
            "Epoch [22/100], Test Loss: 178.1193977520752\n",
            "Epoch [23/100], Train Loss: 149.87359454406737\n",
            "Epoch [23/100], Test Loss: 175.9375014324951\n",
            "Epoch [24/100], Train Loss: 147.85310158630372\n",
            "Epoch [24/100], Test Loss: 173.82354231384278\n",
            "Epoch [25/100], Train Loss: 146.16506716369628\n",
            "Epoch [25/100], Test Loss: 171.57690786132812\n",
            "Epoch [26/100], Train Loss: 144.6126068713379\n",
            "Epoch [26/100], Test Loss: 170.59252514709473\n",
            "Epoch [27/100], Train Loss: 143.1541790509033\n",
            "Epoch [27/100], Test Loss: 168.89631066894532\n",
            "Epoch [28/100], Train Loss: 141.80808583190918\n",
            "Epoch [28/100], Test Loss: 168.38895743896484\n",
            "Epoch [29/100], Train Loss: 140.69527604431153\n",
            "Epoch [29/100], Test Loss: 166.32163719848631\n",
            "Epoch [30/100], Train Loss: 139.4820070227051\n",
            "Epoch [30/100], Test Loss: 164.71462066894532\n",
            "Epoch [31/100], Train Loss: 138.51441361938475\n",
            "Epoch [31/100], Test Loss: 163.8174334625244\n",
            "Epoch [32/100], Train Loss: 137.65507492614745\n",
            "Epoch [32/100], Test Loss: 162.85593689697265\n",
            "Epoch [33/100], Train Loss: 136.7271137310791\n",
            "Epoch [33/100], Test Loss: 162.8910424316406\n",
            "Epoch [34/100], Train Loss: 135.91172715393066\n",
            "Epoch [34/100], Test Loss: 162.01604487060547\n",
            "Epoch [35/100], Train Loss: 135.25697134155274\n",
            "Epoch [35/100], Test Loss: 161.36129938049316\n",
            "Epoch [36/100], Train Loss: 134.52857167785643\n",
            "Epoch [36/100], Test Loss: 159.94536058288574\n",
            "Epoch [37/100], Train Loss: 133.90988631958007\n",
            "Epoch [37/100], Test Loss: 159.43877488647462\n",
            "Epoch [38/100], Train Loss: 133.17778109558105\n",
            "Epoch [38/100], Test Loss: 157.83699305236817\n",
            "Epoch [39/100], Train Loss: 132.70173283508302\n",
            "Epoch [39/100], Test Loss: 158.10544959594728\n",
            "Epoch [40/100], Train Loss: 132.2112472570801\n",
            "Epoch [40/100], Test Loss: 156.84032908935546\n",
            "Epoch [41/100], Train Loss: 131.61740324035645\n",
            "Epoch [41/100], Test Loss: 157.1804600177002\n",
            "Epoch [42/100], Train Loss: 131.05234803100586\n",
            "Epoch [42/100], Test Loss: 156.33787925598145\n",
            "Epoch [43/100], Train Loss: 130.57596368286133\n",
            "Epoch [43/100], Test Loss: 156.07169830932617\n",
            "Epoch [44/100], Train Loss: 130.17262363464354\n",
            "Epoch [44/100], Test Loss: 155.14631605957032\n",
            "Epoch [45/100], Train Loss: 129.68654411560058\n",
            "Epoch [45/100], Test Loss: 155.07092205993652\n",
            "Epoch [46/100], Train Loss: 129.31231978515626\n",
            "Epoch [46/100], Test Loss: 155.05856232299806\n",
            "Epoch [47/100], Train Loss: 128.91850949645996\n",
            "Epoch [47/100], Test Loss: 154.1354396032715\n",
            "Epoch [48/100], Train Loss: 128.5515741522217\n",
            "Epoch [48/100], Test Loss: 153.68478280029296\n",
            "Epoch [49/100], Train Loss: 128.176218984375\n",
            "Epoch [49/100], Test Loss: 153.79687497436524\n",
            "Epoch [50/100], Train Loss: 127.73919860412597\n",
            "Epoch [50/100], Test Loss: 153.0217504260254\n",
            "Epoch [51/100], Train Loss: 127.52565465576171\n",
            "Epoch [51/100], Test Loss: 153.88498147766114\n",
            "Epoch [52/100], Train Loss: 127.23054579284668\n",
            "Epoch [52/100], Test Loss: 152.5417392852783\n",
            "Epoch [53/100], Train Loss: 126.97204343017579\n",
            "Epoch [53/100], Test Loss: 152.47219485778808\n",
            "Epoch [54/100], Train Loss: 126.50031921142578\n",
            "Epoch [54/100], Test Loss: 152.36053823852538\n",
            "Epoch [55/100], Train Loss: 126.30419368835449\n",
            "Epoch [55/100], Test Loss: 151.0957214477539\n",
            "Epoch [56/100], Train Loss: 125.96484052734375\n",
            "Epoch [56/100], Test Loss: 150.96976371032716\n",
            "Epoch [57/100], Train Loss: 125.76303913696289\n",
            "Epoch [57/100], Test Loss: 150.3882334423828\n",
            "Epoch [58/100], Train Loss: 125.5282694116211\n",
            "Epoch [58/100], Test Loss: 150.10694328430176\n",
            "Epoch [59/100], Train Loss: 125.26448968811034\n",
            "Epoch [59/100], Test Loss: 150.95500661193847\n",
            "Epoch [60/100], Train Loss: 124.98935015136719\n",
            "Epoch [60/100], Test Loss: 149.53063062561034\n",
            "Epoch [61/100], Train Loss: 124.66165501220704\n",
            "Epoch [61/100], Test Loss: 151.10516291809083\n",
            "Epoch [62/100], Train Loss: 124.47072775878907\n",
            "Epoch [62/100], Test Loss: 149.86172405944825\n",
            "Epoch [63/100], Train Loss: 124.24121915100098\n",
            "Epoch [63/100], Test Loss: 149.71887302429198\n",
            "Epoch [64/100], Train Loss: 124.04148143310547\n",
            "Epoch [64/100], Test Loss: 149.35668689819335\n",
            "Epoch [65/100], Train Loss: 123.8259842010498\n",
            "Epoch [65/100], Test Loss: 149.28107408508302\n",
            "Epoch [66/100], Train Loss: 123.58436123291015\n",
            "Epoch [66/100], Test Loss: 149.52584602294922\n",
            "Epoch [67/100], Train Loss: 123.40949592590331\n",
            "Epoch [67/100], Test Loss: 149.5607352178955\n",
            "Epoch [68/100], Train Loss: 123.17378038696289\n",
            "Epoch [68/100], Test Loss: 147.90392787963867\n",
            "Epoch [69/100], Train Loss: 122.96593239135743\n",
            "Epoch [69/100], Test Loss: 148.431554553833\n",
            "Epoch [70/100], Train Loss: 122.79640719482421\n",
            "Epoch [70/100], Test Loss: 147.98844948059082\n",
            "Epoch [71/100], Train Loss: 122.60824477844238\n",
            "Epoch [71/100], Test Loss: 148.0104990185547\n",
            "Epoch [72/100], Train Loss: 122.52401121337891\n",
            "Epoch [72/100], Test Loss: 148.11421161743164\n",
            "Epoch [73/100], Train Loss: 122.28083448852539\n",
            "Epoch [73/100], Test Loss: 148.4639070489502\n",
            "Epoch [74/100], Train Loss: 122.00647806884766\n",
            "Epoch [74/100], Test Loss: 147.4391342828369\n",
            "Epoch [75/100], Train Loss: 121.92949641052246\n",
            "Epoch [75/100], Test Loss: 147.45830956359865\n",
            "Epoch [76/100], Train Loss: 121.75565282714844\n",
            "Epoch [76/100], Test Loss: 146.7753112664795\n",
            "Epoch [77/100], Train Loss: 121.52869932312012\n",
            "Epoch [77/100], Test Loss: 147.30472432556152\n",
            "Epoch [78/100], Train Loss: 121.48285220092774\n",
            "Epoch [78/100], Test Loss: 146.8663579309082\n",
            "Epoch [79/100], Train Loss: 121.2572461566162\n",
            "Epoch [79/100], Test Loss: 146.96154702941894\n",
            "Epoch [80/100], Train Loss: 121.14382949829101\n",
            "Epoch [80/100], Test Loss: 146.2493385308838\n",
            "Epoch [81/100], Train Loss: 120.7966816430664\n",
            "Epoch [81/100], Test Loss: 146.87267768249512\n",
            "Epoch [82/100], Train Loss: 120.78237252990722\n",
            "Epoch [82/100], Test Loss: 146.87453495422363\n",
            "Epoch [83/100], Train Loss: 120.53956215332032\n",
            "Epoch [83/100], Test Loss: 145.61087828918457\n",
            "Epoch [84/100], Train Loss: 120.46000704772949\n",
            "Epoch [84/100], Test Loss: 145.67499628295897\n",
            "Epoch [85/100], Train Loss: 120.40533458618164\n",
            "Epoch [85/100], Test Loss: 146.08878145202635\n",
            "Epoch [86/100], Train Loss: 120.17850062072753\n",
            "Epoch [86/100], Test Loss: 144.95762352111817\n",
            "Epoch [87/100], Train Loss: 120.1594695678711\n",
            "Epoch [87/100], Test Loss: 145.7817553149414\n",
            "Epoch [88/100], Train Loss: 120.0148550366211\n",
            "Epoch [88/100], Test Loss: 146.07962398864746\n",
            "Epoch [89/100], Train Loss: 119.9049294128418\n",
            "Epoch [89/100], Test Loss: 144.93257501098634\n",
            "Epoch [90/100], Train Loss: 119.67368606872559\n",
            "Epoch [90/100], Test Loss: 144.38134360717774\n",
            "Epoch [91/100], Train Loss: 119.54004928100586\n",
            "Epoch [91/100], Test Loss: 145.205415123291\n",
            "Epoch [92/100], Train Loss: 119.35943243774415\n",
            "Epoch [92/100], Test Loss: 144.2666713647461\n",
            "Epoch [93/100], Train Loss: 119.30628226867675\n",
            "Epoch [93/100], Test Loss: 145.00084026306152\n",
            "Epoch [94/100], Train Loss: 119.15143604431152\n",
            "Epoch [94/100], Test Loss: 144.08122094848633\n",
            "Epoch [95/100], Train Loss: 119.14351171691895\n",
            "Epoch [95/100], Test Loss: 144.2201250213623\n",
            "Epoch [96/100], Train Loss: 119.06171799194335\n",
            "Epoch [96/100], Test Loss: 144.25110827148438\n",
            "Epoch [97/100], Train Loss: 118.77016877441406\n",
            "Epoch [97/100], Test Loss: 145.41366494628906\n",
            "Epoch [98/100], Train Loss: 118.71741254211426\n",
            "Epoch [98/100], Test Loss: 144.16358907653807\n",
            "Epoch [99/100], Train Loss: 118.47981728637696\n",
            "Epoch [99/100], Test Loss: 143.64537020507814\n",
            "Epoch [100/100], Train Loss: 118.41314854797363\n",
            "Epoch [100/100], Test Loss: 143.55949493591308\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "import torch\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device = torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Instantiate the VAE model\n",
        "input_dim = train_dataset.vocab_size * train_dataset.max_length  # Flatten the input (max_length x vocab_size)\n",
        "latent_dim = 128\n",
        "\n",
        "vocab_size = train_dataset.vocab_size\n",
        "max_length = train_dataset.max_length\n",
        "\n",
        "print(\"Vocab size:\", train_dataset.vocab_size)\n",
        "print(\"max_length:\", train_dataset.max_length)\n",
        "print(\"Input dim:\", input_dim)\n",
        "\n",
        "vae = VAE(input_dim, latent_dim, len(idx_to_char))\n",
        "vae.to(device)\n",
        "\n",
        "# Must define flow and affline coupling layer\n",
        "# input_dim = vae.latent_dim\n",
        "# num_flows = 4 # Number of flow layers\n",
        "# flow_model = Flow(input_dim, num_flows)\n",
        "# flow_model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(vae.parameters(), lr=0.0001)\n",
        "#optimizer = torch.optim.Adam(list(vae.parameters()) + list(flow_model.parameters()), lr=0.0001)\n",
        "\n",
        "\n",
        "# Training and Evaluation loop\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    vae.train()  # Set model to training mode\n",
        "    train_loss = 0\n",
        "    for data in train_loader:  # Iterate over training data\n",
        "        optimizer.zero_grad()\n",
        "        # Flatten the input here before passing to the model\n",
        "        #data = data.view(-1, input_dim).to(device)\n",
        "        data = data.to(device)\n",
        "        recon_batch, mu, logvar = vae(data)\n",
        "        #recon_batch, mu, logvar, log_det_jacobian = vae(data)\n",
        "\n",
        "        #print(f\"Reconstructed output: {recon_batch[:5]}\") # testing\n",
        "        #break\n",
        "\n",
        "        loss = vae_loss(recon_batch, data, mu, logvar)\n",
        "        #loss = vae_loss(recon_batch, data, mu, logvar, log_det_jacobian)\n",
        "\n",
        "        #print(f\"batch loss: {loss.item()}\") # for testing\n",
        "\n",
        "        loss.backward() # back prop\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        optimizer.step() # optimization\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/len(train_loader)}')\n",
        "\n",
        "    # Evaluation on test set\n",
        "    vae.eval()  # Set model to evaluation mode\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():  # No need to calculate gradients during evaluation\n",
        "        for data in test_loader:  # Iterate over test data\n",
        "            # Flatten the input here as well\n",
        "            #data = data.view(-1, input_dim).to(device)\n",
        "            data = data.to(device)\n",
        "            recon_batch, mu, logvar = vae(data)\n",
        "            #recon_batch, mu, logvar, log_det_jacobian = vae(data)\n",
        "            #loss = vae_loss(recon_batch, data, mu, logvar, log_det_jacobian)\n",
        "            loss = vae_loss(recon_batch, data, mu, logvar)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Test Loss: {test_loss/len(test_loader)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MenC6MTLaiIg",
      "metadata": {
        "id": "MenC6MTLaiIg"
      },
      "source": [
        "Flow add, it created a significant drop in loss but then negative loss and break model. Possibly try to make the model stop before that happens of adjust the parameters with flow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "bc567905",
      "metadata": {
        "id": "bc567905"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated SMILES: INVALID\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[21:53:12] SMILES Parse Error: syntax error while parsing: 5CH1r)n]s]B]ocFl6)C(oELMFH=LLB=cn]C1Cc3B1(oCs23r6csNrE=3(C]Lsl-31s2lM([Hso)MI1Oro)INI]SNB3BCr=E(3BI3613c3)sIN1(2(c(2H)n=Cllc1)B45-=1HFO3B-=E)s6l\n",
            "[21:53:12] SMILES Parse Error: check for mistakes around position 1:\n",
            "[21:53:12] 5CH1r)n]s]B]ocFl6)C(oELMFH=LLB=cn]C1Cc3B1\n",
            "[21:53:12] ^\n",
            "[21:53:12] SMILES Parse Error: Failed parsing SMILES '5CH1r)n]s]B]ocFl6)C(oELMFH=LLB=cn]C1Cc3B1(oCs23r6csNrE=3(C]Lsl-31s2lM([Hso)MI1Oro)INI]SNB3BCr=E(3BI3613c3)sIN1(2(c(2H)n=Cllc1)B45-=1HFO3B-=E)s6l' for input: '5CH1r)n]s]B]ocFl6)C(oELMFH=LLB=cn]C1Cc3B1(oCs23r6csNrE=3(C]Lsl-31s2lM([Hso)MI1Oro)INI]SNB3BCr=E(3BI3613c3)sIN1(2(c(2H)n=Cllc1)B45-=1HFO3B-=E)s6l'\n"
          ]
        }
      ],
      "source": [
        "# Generate a new molecule from VAE by sampling from the latent space\n",
        "generated_smiles = generate_smiles(vae, latent_dim, train_dataset.idx_to_char)  # pass idx_to_char\n",
        "\n",
        "print(f\"Generated SMILES: {generated_smiles}\")\n",
        "\n",
        "# # Mock setup for quick testing\n",
        "# vae.eval()  # Set model to eval mode (disables dropout, etc.)\n",
        "\n",
        "# # Generate SMILES from random latent vector\n",
        "# try:\n",
        "#     result = generate_smiles(vae, latent_dim, idx_to_char=train_dataset.idx_to_char, temperature=1.0)\n",
        "#     print(\"Test SMILES output:\", result) # will print invalid if not valid\n",
        "# except Exception as e:\n",
        "#     print(\"Error while generating SMILES:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h-nqTr8BddZi",
      "metadata": {
        "id": "h-nqTr8BddZi"
      },
      "source": [
        "Thoughts:\n",
        "\n",
        "Potetnial invalidity due to:\n",
        "Insufficient training\n",
        "More data needed\n",
        "Expansion of latent space"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gtDExC7pI0tw",
      "metadata": {
        "id": "gtDExC7pI0tw"
      },
      "source": [
        "Problems recorded:\n",
        "\n",
        "Enountered a problem where the model is taking a terabyte of data at once and breaking.\n",
        "  For now adjusted the number of strings in the dataset\n",
        "  Learned that the model is not properly breaking up the strings and just taking them whole.\n",
        "\n",
        "Encountered trouble with model only printing carbon and negative learning value\n",
        "  Learned that vocab was not correctly understood by the model\n",
        "  Adjusted how the data was recorded and one-hot encoding.\n",
        "\n",
        "VAE model original version had problems in learning and structure may be off.\n",
        "  To keep it simple for now we used a simple designed VAE version but taken from GPT as a template.\n",
        "\n",
        "Problem with FLOW use and negative Loss\n",
        "  Jacobian is improperly recorded and used. Need to rework Flow Model\n",
        "  Look into Kosaraju GLOW model as a proper reference"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
